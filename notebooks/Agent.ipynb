{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "from tensorflow.contrib import seq2seq\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "CHARACTERS = list(string.ascii_lowercase + string.digits + string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR:tensorflow:==================================\n",
      "Object was never used (type <class 'tensorflow.python.ops.tensor_array_ops.TensorArray'>):\n",
      "<tensorflow.python.ops.tensor_array_ops.TensorArray object at 0x109f49470>\n",
      "If you want to mark it as used call its \"mark_used()\" method.\n",
      "It was originally created here:\n",
      "['File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\\n    \"__main__\", mod_spec)', 'File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 85, in _run_code\\n    exec(code, run_globals)', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\\n    app.launch_new_instance()', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\\n    app.start()', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 474, in start\\n    ioloop.IOLoop.instance().start()', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\\n    super(ZMQIOLoop, self).start()', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tornado/ioloop.py\", line 887, in start\\n    handler_func(fd_obj, events)', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\\n    return fn(*args, **kwargs)', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\\n    self._handle_recv()', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\\n    self._run_callback(callback, msg)', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\\n    callback(*args, **kwargs)', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\\n    return fn(*args, **kwargs)', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\\n    return self.dispatch_shell(stream, msg)', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\\n    handler(stream, idents, msg)', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\\n    user_expressions, allow_stdin)', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\\n    res = shell.run_cell(code, store_history=store_history, silent=silent)', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\\n    interactivity=interactivity, compiler=compiler, result=result)', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2821, in run_ast_nodes\\n    if self.run_code(code, result):', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\\n    exec(code_obj, self.user_global_ns, self.user_ns)', 'File \"<ipython-input-124-d65b60d3b0d2>\", line 65, in <module>\\n    dtype=tf.float32', 'File \"<ipython-input-124-d65b60d3b0d2>\", line 46, in temporal_rnn\\n    swap_memory = swap_memory', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2775, in while_loop\\n    result = context.BuildLoop(cond, body, loop_vars, shape_invariants)', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2604, in BuildLoop\\n    pred, body, original_loop_vars, loop_vars, shape_invariants)', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/ops/control_flow_ops.py\", line 2554, in _BuildLoop\\n    body_result = body(*packed_vars_for_body)', 'File \"<ipython-input-124-d65b60d3b0d2>\", line 34, in _time_step\\n    output_ta.write(time, output)', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 175, in wrapped\\n    return _add_should_use_warning(fn(*args, **kwargs))', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 144, in _add_should_use_warning\\n    wrapped = TFShouldUseWarningWrapper(x)', 'File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/util/tf_should_use.py\", line 101, in __init__\\n    stack = [s.strip() for s in traceback.format_stack()]']\n",
      "==================================\n",
      "Tensor(\"TensorArrayStack/TensorArrayGatherV3:0\", shape=(?, 12, 300), dtype=float32) (<tf.Tensor 'while/Exit_2:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'while/Exit_3:0' shape=(?, 300) dtype=float32>, <tf.Tensor 'while/Exit_4:0' shape=(?, 300) dtype=float32>)\n"
     ]
    }
   ],
   "source": [
    "def temporal_rnn(cell, inputs, rates, sequence_length=None, initial_state=None, dtype=None, time_major=False, parallel_iterations=32, swap_memory=False):\n",
    "    if not time_major:\n",
    "        inputs = tf.transpose(inputs, [1,0,2])\n",
    "    \n",
    "    _, _, input_size = inputs.get_shape().as_list()\n",
    "    max_time, batch_size, _ = tf.unstack(tf.shape(inputs))\n",
    "    \n",
    "    if sequence_length is None:\n",
    "        sequence_length = tf.ones([batch_size], dtype=tf.int32) * max_time\n",
    "    \n",
    "    inputs_ta = tf.TensorArray(tf.float32, max_time).unstack(inputs)\n",
    "    outputs_ta = tf.TensorArray(tf.float32, max_time)\n",
    "    \n",
    "    zero_output = tf.zeros([max_time, batch_size, cell.output_size], dtype=dtype)\n",
    "    \n",
    "    def _get_initial_state():\n",
    "        if initial_state is None:\n",
    "            assert dtype is not None\n",
    "            return cell.zero_state(batch_size, dtype)\n",
    "        return initial_state\n",
    "    \n",
    "    def _time_step(time, output_ta, state):\n",
    "        input_t = inputs_ta.read(time)\n",
    "        \n",
    "        call_cell = lambda: cell(input_t, state)\n",
    "        \n",
    "        if sequence_length is None:\n",
    "            output, new_state = call_cell()\n",
    "        else:\n",
    "            output, new_state = call_cell()\n",
    "        \n",
    "        output_ta.write(time, output)\n",
    "\n",
    "        return time+1, output_ta, new_state\n",
    "    \n",
    "    state = _get_initial_state()\n",
    "    time = tf.constant(0, dtype=tf.int32, name='time')\n",
    "    \n",
    "    _, outputs_ta, final_state = tf.while_loop(\n",
    "        cond = lambda time, *_: time < max_time,\n",
    "        body = _time_step,\n",
    "        loop_vars = (time, outputs_ta, state),\n",
    "        parallel_iterations = parallel_iterations,\n",
    "        swap_memory = swap_memory\n",
    "    )\n",
    "    \n",
    "    return outputs_ta.stack(), final_state\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    inputs_ph = tf.placeholder(tf.float32, [64, 12, 300])\n",
    "    \n",
    "    cell = rnn.MultiRNNCell([\n",
    "        rnn.GRUCell(300),\n",
    "        rnn.GRUCell(300),\n",
    "        rnn.GRUCell(300),\n",
    "    ])\n",
    "    \n",
    "    outputs, state = temporal_rnn(\n",
    "        cell,\n",
    "        inputs_ph,\n",
    "        [4, 2, 1],\n",
    "        time_major=True,\n",
    "        dtype=tf.float32\n",
    "    )\n",
    "    \n",
    "    print(outputs, state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def add_pad_eos(indices, sequence_length, eos_token = 1, pre_pad = True):\n",
    "    batch_size, max_length = tf.unstack(tf.shape(indices))\n",
    "    \n",
    "    pad = tf.zeros([batch_size, 1], dtype=tf.int32)\n",
    "    if pre_pad:\n",
    "        eos = tf.one_hot(sequence_length+1, max_length+2, dtype=tf.int32) * eos_token\n",
    "        return tf.concat([pad, indices, pad], 1) + eos\n",
    "    else:\n",
    "        eos = tf.one_hot(sequence_length, max_length+2, dtype=tf.int32) * eos_token\n",
    "        return tf.concat([indices, pad, pad], 1) + eos\n",
    "\n",
    "class Agent(object):\n",
    "    \n",
    "    def __init__(self, previous_state, inputs, inputs_length, previous_output, previous_output_length, query_result_state, query_result_slots, query_result_values, query_result_slots_count, query_result_values_length, word_embeddings_shape, n_slots, n_actions, decoder_targets=None, decoder_targets_length=None, decoder_sampling_p=0.0, trainable_embeddings=True, hidden_size=300, dropout=0.0):\n",
    "        super(Agent, self).__init__()\n",
    "        \n",
    "        # Inputs\n",
    "        self._previous_state = previous_state\n",
    "        self._inputs = inputs\n",
    "        self._inputs_length = inputs_length\n",
    "        self._previous_output = previous_output\n",
    "        self._previous_output_length = previous_output_length\n",
    "        \n",
    "        # Query result\n",
    "        self._query_result_state = query_result_state\n",
    "        self._query_result_slots = query_result_slots\n",
    "        self._query_result_values = query_result_values\n",
    "        self._query_result_slots_count = query_result_slots_count\n",
    "        self._query_result_values_length = query_result_values_length\n",
    "            \n",
    "        # Decoder targets (only for training)\n",
    "        self._decoder_targets = decoder_targets\n",
    "        self._decoder_targets_length = decoder_targets_length\n",
    "        self._decoder_sampling_p = decoder_sampling_p\n",
    "        \n",
    "        # Conf\n",
    "        self._hidden_size = int(hidden_size)\n",
    "        self._word_embeddings_shape = list(word_embeddings_shape)\n",
    "        self._n_slots = int(n_slots)\n",
    "        self._n_actions = int(n_actions)\n",
    "        self._n_query_states = 3\n",
    "        self._trainable_embeddings = bool(trainable_embeddings)\n",
    "        self._dropout = dropout\n",
    "        tf.summary.scalar('dropout', self._dropout)\n",
    "\n",
    "        print('Agent(hidden_size={0}, n_slots={1}, n_actions={2})'.format(self._hidden_size, self._n_slots, self._n_actions))\n",
    "        \n",
    "        # Build\n",
    "        self._embeddings_module()\n",
    "        self._input_encoder()\n",
    "        self._slot_parser()\n",
    "        self._query_result_encoder()\n",
    "        self._policy()\n",
    "        self._response_generator()\n",
    "        \n",
    "        self._saver_ops()\n",
    "        \n",
    "    @property\n",
    "    def policy_state_size(self):\n",
    "        return 2*self._hidden_size\n",
    "        \n",
    "    def _saver_ops(self):\n",
    "        self.saver = tf.train.Saver(max_to_keep=None)\n",
    "        \n",
    "    def _embeddings_module(self):\n",
    "        with tf.name_scope('embeddings'):\n",
    "            self._word_embeddings = tf.Variable(tf.zeros(self._word_embeddings_shape), trainable=self._trainable_embeddings, name='word_embeddings')\n",
    "            self._slot_embeddings = tf.Variable(tf.zeros([self._n_slots, self._word_embeddings_shape[1]]), trainable=True, name='slot_embeddings')\n",
    "            \n",
    "            self._inputs_embedded = tf.nn.embedding_lookup(self._word_embeddings, self._inputs)\n",
    "            self._previous_output_embedded = tf.nn.embedding_lookup(self._word_embeddings, self._previous_output)\n",
    "            \n",
    "            self._query_result_slots_embedded = tf.nn.embedding_lookup(self._slot_embeddings, self._query_result_slots)\n",
    "            self._query_result_values_embedded = tf.nn.embedding_lookup(self._word_embeddings, self._query_result_values)\n",
    "            \n",
    "    def _rnn_cell(self, size=None, activation=None, dropout=None, residual=False):\n",
    "        cell = rnn.GRUCell((size or self._hidden_size), activation=activation)\n",
    "\n",
    "        if residual:\n",
    "            cell = rnn.ResidualWrapper(cell)\n",
    "\n",
    "        if dropout is not None:\n",
    "            cell = rnn.DropoutWrapper(cell, input_keep_prob=(1.0 - dropout))\n",
    "\n",
    "        return cell\n",
    "        \n",
    "    def _text_encoder(self, inputs, inputs_length, scope='text_encoder', reuse=False):\n",
    "        with tf.variable_scope(scope, reuse=reuse):\n",
    "            _outputs, _state = tf.nn.bidirectional_dynamic_rnn(\n",
    "                cell_fw = self._rnn_cell(activation=tf.nn.tanh),\n",
    "                cell_bw = self._rnn_cell(activation=tf.nn.tanh),\n",
    "                inputs = inputs,\n",
    "                sequence_length = inputs_length,\n",
    "                dtype = tf.float32\n",
    "            )\n",
    "\n",
    "        return (\n",
    "            tf.concat(_outputs, -1),\n",
    "            tf.concat(_state, -1)\n",
    "        )\n",
    " \n",
    "    def _input_encoder(self):\n",
    "        with tf.name_scope('inputs_encoder'):            \n",
    "            (self._inputs_encoder_outputs,\n",
    "             self._inputs_encoder_state) = self._text_encoder(self._inputs_embedded, self._inputs_length)\n",
    "            (self._previous_output_encoder_outputs,\n",
    "             self._previous_output_encoder_state) = self._text_encoder(self._previous_output_embedded, self._previous_output_length, reuse=True)\n",
    "            \n",
    "    def _slot_parser(self):\n",
    "        with tf.variable_scope('slot_parser'):\n",
    "            # Project text encoder state\n",
    "            e_inputs = tf.layers.dense(\n",
    "                self._inputs_encoder_outputs,\n",
    "                self._hidden_size,\n",
    "                activation = tf.nn.tanh,\n",
    "                name = 'text_encoder_outputs_projection'\n",
    "            )\n",
    "            e_previous_output = tf.layers.dense(\n",
    "                self._previous_output_encoder_outputs,\n",
    "                self._hidden_size,\n",
    "                activation = tf.nn.tanh,\n",
    "                name = 'text_encoder_outputs_projection',\n",
    "                reuse = True\n",
    "            )\n",
    "            \n",
    "            # Compare inputs with agent's previous output\n",
    "            e = tf.matmul(e_inputs, e_previous_output, transpose_b=True, name='e')\n",
    "            beta = tf.matmul(tf.nn.softmax(e), self._previous_output_encoder_outputs)\n",
    "            \n",
    "            inputs_compared = tf.layers.dense(\n",
    "                tf.concat([self._inputs_encoder_outputs, beta], 2),\n",
    "                self._hidden_size,\n",
    "                activation = tf.nn.tanh\n",
    "            )\n",
    "            \n",
    "            # Final slot logits/probabilities\n",
    "            self._slot_logits = tf.layers.dense(\n",
    "                tf.layers.dropout(inputs_compared, rate=self._dropout),\n",
    "                self._n_slots\n",
    "            )\n",
    "            self.slot_probabilities = tf.nn.softmax(self._slot_logits)\n",
    "            self.slot_ids = tf.argmax(self._slot_logits, -1)\n",
    "\n",
    "            # Slot (any)\n",
    "            state_proj = tf.layers.dense(\n",
    "                tf.concat([self._inputs_encoder_state, self._previous_output_encoder_state], -1),\n",
    "                self._hidden_size,\n",
    "                activation = tf.nn.tanh\n",
    "            )\n",
    "            \n",
    "            self._slot_any_logits = tf.layers.dense(\n",
    "                tf.layers.dropout(state_proj, rate=self._dropout),\n",
    "                self._n_slots\n",
    "            )\n",
    "            self.slot_any_probabilites = tf.sigmoid(self._slot_any_logits)\n",
    "            self.slot_any = tf.greater(self.slot_any_probabilites, .5)\n",
    "    \n",
    "    def _query_result_encoder(self):\n",
    "        with tf.name_scope('query_result_encoder'):\n",
    "            batch_size, n_slots, n_tokens = tf.unstack(tf.shape(self._query_result_values))\n",
    "    \n",
    "            _, _value_encoder_state = self._text_encoder(\n",
    "                inputs = tf.reshape(self._query_result_values_embedded, [-1, n_tokens, self._word_embeddings_shape[1]]),\n",
    "                inputs_length = tf.reshape(self._query_result_values_length, [-1]),\n",
    "                reuse = True\n",
    "            )\n",
    "        \n",
    "            query_result_slot_value = tf.concat([\n",
    "                self._query_result_slots_embedded,\n",
    "                tf.reshape(_value_encoder_state, [batch_size, n_slots, 2*self._hidden_size])\n",
    "            ], -1)\n",
    "            \n",
    "            _, self._query_result_encoder_state = tf.nn.dynamic_rnn(\n",
    "                self._rnn_cell(activation=tf.nn.tanh, dropout=self._dropout),\n",
    "                inputs = query_result_slot_value,\n",
    "                sequence_length = self._query_result_slots_count,\n",
    "                dtype = tf.float32\n",
    "            )\n",
    "        \n",
    "    def _policy(self):\n",
    "        with tf.name_scope('policy'):\n",
    "            policy_context = tf.layers.dense(\n",
    "                tf.concat([\n",
    "                    self._inputs_encoder_state,\n",
    "                    self._query_result_encoder_state,\n",
    "                    tf.one_hot(self._query_result_state, self._n_query_states, dtype=tf.float32),\n",
    "                ], -1),\n",
    "                2*self.policy_state_size\n",
    "            )\n",
    "\n",
    "            policy_cell = rnn.MultiRNNCell([\n",
    "                self._rnn_cell(self.policy_state_size, activation=tf.nn.tanh, dropout=self._dropout),\n",
    "                self._rnn_cell(self.policy_state_size, activation=tf.nn.tanh, dropout=self._dropout),\n",
    "                self._rnn_cell(self.policy_state_size, activation=tf.nn.tanh, dropout=self._dropout)\n",
    "            ])\n",
    "\n",
    "            self._policy_output, self.policy_state = policy_cell(\n",
    "                policy_context,\n",
    "                tuple([self._previous_state[i,:,:] for i in range(3)])\n",
    "            )\n",
    "\n",
    "            # Value\n",
    "            value_l1 = tf.layers.dense(\n",
    "                tf.layers.dropout(self._policy_output, rate=self._dropout),\n",
    "                self._hidden_size,\n",
    "                activation = tf.nn.tanh\n",
    "            )\n",
    "            self.value = tf.layers.dense(\n",
    "                tf.layers.dropout(value_l1, rate=self._dropout),\n",
    "                1\n",
    "            )\n",
    "            self.value = tf.squeeze(self.value, -1)\n",
    "\n",
    "            # Action\n",
    "            action_l1 = tf.layers.dense(\n",
    "                tf.layers.dropout(self._policy_output, rate=self._dropout),\n",
    "                self._hidden_size,\n",
    "                activation = tf.nn.tanh\n",
    "            )\n",
    "            self._action_logits = tf.layers.dense(\n",
    "                tf.layers.dropout(action_l1, rate=self._dropout),\n",
    "                self._n_actions\n",
    "            )\n",
    "            self.action_probabilities = tf.nn.softmax(self._action_logits)\n",
    "            self.action_ids = tf.argmax(self._action_logits, -1, output_type=tf.int32)\n",
    "        \n",
    "    def _response_generator(self):\n",
    "        with tf.name_scope('response_generator'):\n",
    "            batch_size, _ = tf.unstack(tf.shape(self._inputs))\n",
    "            self._decoder_output_layer = Dense(self._word_embeddings_shape[0])\n",
    "            \n",
    "            if self._decoder_targets is not None:\n",
    "                print('Training decoder helper.')\n",
    "                \n",
    "                decoder_targets_embedded = tf.nn.embedding_lookup(\n",
    "                    self._word_embeddings,\n",
    "                    add_pad_eos(self._decoder_targets, self._decoder_targets_length)\n",
    "                )\n",
    "                helper = seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "                    inputs = decoder_targets_embedded,\n",
    "                    sequence_length = (self._decoder_targets_length + 2),\n",
    "                    embedding = self._word_embeddings,\n",
    "                    sampling_probability = self._decoder_sampling_p\n",
    "                )\n",
    "                tf.summary.scalar('decoder_sampling_p', self._decoder_sampling_p)\n",
    "                \n",
    "            else:\n",
    "                print('Inference decoder helper.')\n",
    "                \n",
    "                helper = seq2seq.GreedyEmbeddingHelper(\n",
    "                    embedding = self._word_embeddings,\n",
    "                    start_tokens = tf.tile([0], [batch_size]),\n",
    "                    end_token = 1\n",
    "                )\n",
    "                \n",
    "            decoder_cell, decoder_initial_state = self._decoder_cell()\n",
    "            decoder = seq2seq.BasicDecoder(\n",
    "                decoder_cell,\n",
    "                helper = helper,\n",
    "                initial_state = decoder_initial_state,\n",
    "                output_layer = self._decoder_output_layer\n",
    "            )\n",
    "            \n",
    "            decoder_outputs, _, _ = seq2seq.dynamic_decode(\n",
    "                decoder = decoder,\n",
    "                impute_finished = True\n",
    "            )\n",
    "            \n",
    "            self._decoder_logits = decoder_outputs.rnn_output\n",
    "            self.decoder_token_ids = tf.argmax(self._decoder_logits, -1, output_type=tf.int32)\n",
    "            \n",
    "    def _decoder_cell(self):\n",
    "        batch_size, _ = tf.unstack(tf.shape(self._policy_output))\n",
    "\n",
    "        attention = seq2seq.BahdanauAttention(\n",
    "            num_units = 2*self._hidden_size,\n",
    "            memory = self._inputs_encoder_outputs,\n",
    "            memory_sequence_length = self._inputs_length\n",
    "        )\n",
    "        \n",
    "        attentive_cell = seq2seq.AttentionWrapper(\n",
    "            cell = self._rnn_cell(self.policy_state_size, activation=tf.nn.tanh),\n",
    "            attention_mechanism = attention,\n",
    "            attention_layer_size = 2*self._hidden_size,\n",
    "            initial_cell_state = self._policy_output\n",
    "        )\n",
    "\n",
    "        cell = rnn.MultiRNNCell([\n",
    "            attentive_cell,\n",
    "            self._rnn_cell(self.policy_state_size, activation=tf.nn.tanh),\n",
    "        ])\n",
    "\n",
    "        initial_state = tuple([\n",
    "            attentive_cell.zero_state(batch_size, tf.float32),\n",
    "            self._policy_output\n",
    "        ])\n",
    "\n",
    "        return cell, initial_state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset states: (4, 3, 64, 600)\n",
      "Agent(hidden_size=300, n_slots=5, n_actions=4)\n",
      "Training decoder helper.\n",
      "Metrics path ./mdata/metrics/\n",
      "Dilatation rates: [4, 2, 1]\n"
     ]
    }
   ],
   "source": [
    "class Trainer(object):\n",
    "\n",
    "    DILATATION_RATES = [4, 2, 1]\n",
    "    \n",
    "    def __init__(self, n_slots, n_actions, word_embeddings_shape, save_path, hidden_size=300, graph=None, batch_size=64):\n",
    "        self._sess = tf.Session(graph=graph)\n",
    "        self._save_path = save_path\n",
    "        \n",
    "        self._n_slots = n_slots\n",
    "        self._n_actions = n_actions\n",
    "        self._word_embeddings_shape = word_embeddings_shape\n",
    "        self._hidden_size = hidden_size\n",
    "        \n",
    "        self._batch_size = batch_size\n",
    "        \n",
    "        self.reset()\n",
    "\n",
    "    @property\n",
    "    def states_memory_shape(self):\n",
    "        return (4, 3, self._batch_size, 2*self._hidden_size)\n",
    "    \n",
    "    def _metrics_writers(self):\n",
    "        print('Metrics path {0}/metrics/'.format(self._save_path))\n",
    "        \n",
    "        self._train_writer = tf.summary.FileWriter('{0}/metrics/train'.format(self._save_path), self._sess.graph)\n",
    "        self._test_writer = tf.summary.FileWriter('{0}/metrics/test'.format(self._save_path), self._sess.graph)\n",
    "        self._metrics_op = tf.summary.merge_all()\n",
    "        \n",
    "    def initialize_word_embeddings(self, embeddings):\n",
    "        embeddings_ph = tf.placeholder(tf.float32, self._word_embeddings_shape)\n",
    "        init_op = self.agent._word_embeddings.assign(embeddings_ph)\n",
    "        \n",
    "        return self._sess.run(init_op, feed_dict={embeddings_ph: embeddings})\n",
    "\n",
    "    def save_checkpoint(self, step):\n",
    "        print('Write checkpoint:', self.agent.saver.save(self._sess, '{0}/checkpoints/model.ckpt'.format(self._save_path), global_step=step))\n",
    "    \n",
    "    def reset(self):\n",
    "        self._states_index = np.zeros(self._batch_size, dtype=np.int32)\n",
    "        self._states_memory = np.zeros(self.states_memory_shape, dtype=np.float32)\n",
    "\n",
    "        assert self._states_memory.shape == self.states_memory_shape\n",
    "        print('Reset states:', self._states_memory.shape)\n",
    "\n",
    "    def _reset_states(self, predicate):\n",
    "        self._states_index[predicate] = 0\n",
    "        self._states_memory[:,:,predicate] = np.zeros(self.states_memory_shape[3], dtype=np.float32)            \n",
    "\n",
    "        assert self._states_memory.shape == self.states_memory_shape\n",
    "\n",
    "    def _get_states(self):\n",
    "        cell_states = []\n",
    "        for cell_id, dilatation in enumerate(self.DILATATION_RATES):\n",
    "            batch_states = []\n",
    "            for batch_sample_id, location_get_id in enumerate(self._states_index % dilatation):\n",
    "                batch_states.append(self._states_memory[location_get_id,cell_id,batch_sample_id,:])\n",
    "            cell_states.append(batch_states)\n",
    "        return np.asarray(cell_states)\n",
    "        \n",
    "    def _update_states(self, states):\n",
    "        states = np.array(states, copy=True)\n",
    "\n",
    "        for cell_id, dilatation in enumerate(self.DILATATION_RATES):\n",
    "            location_update_id = (dilatation - 1) - self._states_index % dilatation\n",
    "            self._states_memory[location_update_id,cell_id,:,:] = states[cell_id,:,:]\n",
    "\n",
    "        self._states_index += 1\n",
    "        assert self._states_memory.shape == self.states_memory_shape\n",
    "\n",
    "class SupervisedTrainer(Trainer):\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(SupervisedTrainer, self).__init__(**kwargs)\n",
    "        \n",
    "        self.state = tf.placeholder(tf.float32, self.states_memory_shape[1:])\n",
    "        self.inputs = tf.placeholder(tf.int32, [None, None])\n",
    "        self.inputs_length = tf.placeholder(tf.int32, [None])\n",
    "        self.previous_output = tf.placeholder(tf.int32, [None, None])\n",
    "        self.previous_output_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "        self.query_result_state = tf.placeholder(tf.int32, [None])\n",
    "        self.query_result_slots = tf.placeholder(tf.int32, [None, None])\n",
    "        self.query_result_values = tf.placeholder(tf.int32, [None, None, None])\n",
    "        self.query_result_slots_count = tf.placeholder(tf.int32, [None])\n",
    "        self.query_result_values_length = tf.placeholder(tf.int32, [None, None])\n",
    "        \n",
    "        self.slot_targets = tf.placeholder(tf.int32, [None, None])\n",
    "        self.slot_any_targets = tf.placeholder(tf.int32, [None, None])\n",
    "        self.action_targets = tf.placeholder(tf.int32, [None])\n",
    "        self.value_targets = tf.placeholder(tf.float32, [None])\n",
    "        self.targets = tf.placeholder(tf.int32, [None, None])\n",
    "        self.targets_length = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        self._decoder_sampling_p = tf.placeholder(tf.float32, [])\n",
    "        self._loss_mixture_weights = tf.placeholder(tf.float32, [None])\n",
    "        self._dropout = tf.placeholder(tf.float32, [])\n",
    "        \n",
    "        self.agent = Agent(\n",
    "            self.state, self.inputs, self.inputs_length, self.previous_output, self.previous_output_length, self.query_result_state,\n",
    "            self.query_result_slots, self.query_result_values, self.query_result_slots_count, self.query_result_values_length,\n",
    "            word_embeddings_shape = self._word_embeddings_shape,\n",
    "            n_slots = self._n_slots, n_actions = self._n_actions,\n",
    "            decoder_targets = self.targets, decoder_targets_length = self.targets_length, decoder_sampling_p = self._decoder_sampling_p,\n",
    "            hidden_size = self._hidden_size, dropout=self._dropout\n",
    "        )\n",
    "        \n",
    "        self._loss()\n",
    "        self._optimizer()\n",
    "        self._metrics_writers()\n",
    "        print('Dilatation rates:', self.DILATATION_RATES)\n",
    "        \n",
    "    def _loss(self):\n",
    "        is_speak_sample = tf.cast(tf.equal(self.action_targets, 1), tf.float32)\n",
    "        padded_targets = add_pad_eos(self.targets, self.targets_length, pre_pad=False)\n",
    "        \n",
    "        # Decoder\n",
    "        stepwise_ce = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits = self.agent._decoder_logits,\n",
    "            labels = tf.one_hot(padded_targets, self._word_embeddings_shape[0])\n",
    "        )\n",
    "        stepwise_ce *= tf.sequence_mask(self.targets_length+2, dtype=tf.float32)\n",
    "        \n",
    "        self.decoder_loss = tf.reduce_sum((tf.reduce_sum(stepwise_ce, -1) / tf.cast(self.targets_length+2, tf.float32)) * is_speak_sample) / tf.reduce_sum(is_speak_sample)\n",
    "        self.decoder_accuracy = tf.reduce_mean(tf.cast(tf.equal(padded_targets, self.agent.decoder_token_ids), tf.float32))\n",
    "        \n",
    "        # Slot parser\n",
    "        slotwise_ce = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits = self.agent._slot_logits,\n",
    "            labels = tf.one_hot(self.slot_targets, self._n_slots)\n",
    "        )\n",
    "        slotwise_ce *= tf.sequence_mask(self.inputs_length, dtype=tf.float32)\n",
    "        \n",
    "        self.slots_loss = tf.reduce_sum((tf.reduce_sum(slotwise_ce, -1) / tf.cast(self.inputs_length, tf.float32)) * is_speak_sample) / tf.reduce_sum(is_speak_sample)\n",
    "        self.slots_accuracy = tf.reduce_mean(tf.cast(tf.equal(self.slot_targets, tf.argmax(self.agent.slot_probabilities, -1, output_type=tf.int32)), tf.float32))\n",
    "\n",
    "        # Slot states\n",
    "        slot_any_onehot_targets = tf.reduce_sum(tf.one_hot(self.slot_any_targets, self._n_slots), 1)\n",
    "        slot_any_ce = tf.nn.sigmoid_cross_entropy_with_logits(\n",
    "            logits = self.agent._slot_any_logits,\n",
    "            labels = slot_any_onehot_targets\n",
    "        )\n",
    "        \n",
    "        self.slot_any_loss = tf.reduce_mean(slot_any_ce)\n",
    "        self.slot_any_accuracy = tf.reduce_mean(tf.cast(tf.equal(slot_any_onehot_targets, tf.cast(self.agent.slot_any, tf.float32)), tf.float32))\n",
    "        \n",
    "        # Policy\n",
    "        action_ce = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            logits = self.agent._action_logits,\n",
    "            labels = tf.one_hot(self.action_targets, self._n_actions)\n",
    "        )\n",
    "\n",
    "        self.action_loss = tf.reduce_mean(action_ce)\n",
    "        self.action_accuracy = tf.reduce_mean(tf.cast(tf.equal(self.action_targets, self.agent.action_ids), tf.float32))\n",
    "\n",
    "        # Value\n",
    "        self.value_loss = tf.losses.mean_squared_error(\n",
    "            labels = self.value_targets,\n",
    "            predictions = self.agent.value\n",
    "        )\n",
    "        \n",
    "        # Total Loss\n",
    "        self.loss = tf.reduce_sum(tf.multiply(\n",
    "            tf.stack([self.decoder_loss, self.slots_loss, self.slot_any_loss, self.action_loss, self.value_loss]),\n",
    "            self._loss_mixture_weights\n",
    "        ))\n",
    "        \n",
    "        # Metrics\n",
    "        tf.summary.scalar('decoder_loss', self.decoder_loss)\n",
    "        tf.summary.scalar('decoder_accuracy', self.decoder_accuracy)\n",
    "        tf.summary.scalar('slots_loss', self.slots_loss)\n",
    "        tf.summary.scalar('slots_accuracy', self.slots_accuracy)\n",
    "        tf.summary.scalar('slot_any_loss', self.slot_any_loss)\n",
    "        tf.summary.scalar('slot_any_accuracy', self.slot_any_accuracy)\n",
    "        tf.summary.scalar('action_loss', self.action_loss)\n",
    "        tf.summary.scalar('action_accuracy', self.action_accuracy)\n",
    "        tf.summary.scalar('value_loss', self.value_loss)\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "        \n",
    "    def _optimizer(self):\n",
    "        self.train_op = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "\n",
    "    def _compute_decoder_sampling_p(self, e, z=0.125, k=0.1):\n",
    "        return ((2*z) / (1 + np.exp(-k*e)) - z)\n",
    "        \n",
    "    def _feed_dict(self, e, batch, opts={}):\n",
    "        fd = {\n",
    "            self.state: self._get_states(),\n",
    "            self.inputs: batch['inputs'],\n",
    "            self.inputs_length: batch['inputs_length'],\n",
    "            self.previous_output: batch['previous_output'],\n",
    "            self.previous_output_length: batch['previous_output_length'],\n",
    "            self.targets: batch['targets'],\n",
    "            self.targets_length: batch['targets_length'],\n",
    "            self.slot_targets: batch['slot_targets'],\n",
    "            self.slot_any_targets: batch['slot_any_targets'],\n",
    "            self.action_targets: batch['action_targets'],\n",
    "            self.value_targets: batch['value_targets'],\n",
    "            self.query_result_state: batch['query_result_state'],\n",
    "            self.query_result_slots: batch['query_result']['slots'],\n",
    "            self.query_result_values: batch['query_result']['values'],\n",
    "            self.query_result_slots_count: batch['query_result']['slots_count'],\n",
    "            self.query_result_values_length: batch['query_result']['values_length'],\n",
    "            self._loss_mixture_weights: [1., 1., 1., 1., 1.],\n",
    "            self._decoder_sampling_p: self._compute_decoder_sampling_p(e)\n",
    "        }\n",
    "\n",
    "        for opt, val in opts.items():\n",
    "            fd[opt] = val\n",
    "\n",
    "        return fd\n",
    "        \n",
    "    def train_batch(self, e, i, batch):\n",
    "        self._reset_states(batch['reset_state'])\n",
    "        \n",
    "        _, new_states, metrics_val = self._sess.run(\n",
    "            [self.train_op, self.agent.policy_state, self._metrics_op],\n",
    "            feed_dict=self._feed_dict(e, batch, {self._dropout: 0.3})\n",
    "        )\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            self._train_writer.add_summary(metrics_val)\n",
    "        self._update_states(new_states)\n",
    "\n",
    "    def test_batch(self, e, i, batch):\n",
    "        self._reset_states(batch['reset_state'])\n",
    "        \n",
    "        new_states, metrics_val = self._sess.run(\n",
    "            [self.agent.policy_state, self._metrics_op],\n",
    "            feed_dict=self._feed_dict(e, batch, {self._dropout: 0.0})\n",
    "        )\n",
    "\n",
    "        if i % 20 == 0:\n",
    "            self._test_writer.add_summary(metrics_val)\n",
    "        self._update_states(new_states)\n",
    "        \n",
    "with tf.Graph().as_default():\n",
    "    SupervisedTrainer(n_slots=5, n_actions=4, word_embeddings_shape=[10000, 300], save_path='./mdata')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_dict = json.load(open('./data/frames_v2/word_dictionary.json', 'r'))\n",
    "embeddings = np.asarray(json.load(open('./data/frames_v2/embeddings.json', 'r')))\n",
    "slots_dict = json.load(open('./data/frames_v2/slots_dictionary.json', 'r'))\n",
    "actions_dict = json.load(open('./data/frames_v2/actions_dictionary.json', 'r'))\n",
    "action_frequencies = json.load(open('./data/frames_v2/action_frequencies.json'))\n",
    "\n",
    "samples = json.load(open('./data/frames_v2/embedded_frames.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.33606191,  0.7744456 ,  0.9384581 ,  0.95103438])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_weights = np.zeros(len(actions_dict))\n",
    "for action, freq in action_frequencies.items():\n",
    "    action_weights[actions_dict[action]] = 1. - freq\n",
    "action_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 1]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples[0][0]['slot_states']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [11  4]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [ 1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [ 2 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]\n",
      " [-1 -1]]\n"
     ]
    }
   ],
   "source": [
    "from jamesbot.utils.padding import pad_sequences\n",
    "import random\n",
    "\n",
    "def pad_complex(struct, max_keys_len, max_values_len, pad_value=0, shuffle=True):\n",
    "    '''\n",
    "    :param struct: Dictionary of arrays. \n",
    "    :returns: Vector of slot indices, matrix of value indices, number of slots and a vector of value lengths.\n",
    "    '''\n",
    "    if len(struct) == 0:\n",
    "        return (\n",
    "            np.zeros(shape=(max_keys_len), dtype=np.int32),\n",
    "            np.zeros(shape=(max_keys_len, max_values_len), dtype=np.int32)\n",
    "        )\n",
    "    \n",
    "    struct_items = list(struct.items())\n",
    "    if shuffle == True:\n",
    "        random.shuffle(struct_items)\n",
    "        \n",
    "    keys, values = [], []\n",
    "    for (key, value) in struct_items:\n",
    "        keys.append(key)\n",
    "        values.append(value)\n",
    "    if len(keys) < max_keys_len:\n",
    "        keys += [pad_value]*(max_keys_len-len(keys))\n",
    "        values += [[pad_value]]*(max_keys_len-len(values))\n",
    "    \n",
    "    return (\n",
    "        np.array(keys).astype(int),\n",
    "        pad_sequences(values, max_values_len)\n",
    "    )\n",
    "\n",
    "def pad_array_of_complex(structs):\n",
    "    '''\n",
    "    :param structs: An array of structs\n",
    "    :returns: Padded keys, values, struct sizes and value lengths\n",
    "    '''\n",
    "    key_counts = [len(struct) for struct in structs]\n",
    "    max_keys_count = np.clip(max(key_counts), 1, 15)\n",
    "    \n",
    "    value_lengths = [[len(value) for (_, value) in struct.items()] for struct in structs]\n",
    "    max_values_length = np.clip(max([(max(lens) if len(lens) > 0 else 0) for lens in value_lengths]), 1, 20)\n",
    "    \n",
    "    if len(structs) == 0:\n",
    "        return (\n",
    "            np.zeros(shape=(1,max_keys_count), dtype=np.int32),\n",
    "            np.zeros(shape=(1,max_keys_count,max_values_length), dtype=np.int32),\n",
    "            np.zeros(shape=(1,), dtype=np.int32),\n",
    "            np.zeros(shape=(1,max_keys_count), dtype=np.int32)\n",
    "        )\n",
    "    \n",
    "    keys_padded, values_padded = [], []\n",
    "    for struct in structs:\n",
    "        keys, values = pad_complex(struct, max_keys_count, max_values_length)\n",
    "        keys_padded.append(keys)\n",
    "        values_padded.append(values)\n",
    "    \n",
    "    keys_padded = np.array(keys_padded, dtype=np.int32)\n",
    "    values_padded = np.array(values_padded, dtype=np.int32)\n",
    "    \n",
    "    for i in range(len(structs)):\n",
    "        if len(value_lengths[i]) < max_keys_count:\n",
    "            value_lengths[i] += [0]*(max_keys_count-len(value_lengths[i]))\n",
    "    \n",
    "    return {\n",
    "        'slots': keys_padded,\n",
    "        'values': values_padded,\n",
    "        'slots_count': np.array(key_counts, dtype=np.int32),\n",
    "        'values_length': np.array(value_lengths, dtype=np.int32)\n",
    "    }\n",
    "\n",
    "\n",
    "class SamplesIterator(object):\n",
    "    \n",
    "    def __init__(self, samples, batch_size=64, max_sequence_len=50):\n",
    "        self._samples = samples\n",
    "        \n",
    "        self._batch_size = batch_size\n",
    "        self._max_sequence_len = max_sequence_len\n",
    "        \n",
    "    def _next_batch(self):\n",
    "        for i in range(self._batch_size):\n",
    "            dialog_id = self._dialog_indices[i]\n",
    "            turn_id = self._turn_indices[i]\n",
    "            \n",
    "            if len(self._samples[dialog_id]) > turn_id:\n",
    "                yield dict(self._samples[dialog_id][turn_id], reset_state=False)\n",
    "                \n",
    "                self._turn_indices[i] += 1\n",
    "            else:\n",
    "                if len(self._samples) == self._next_dialog_idx:\n",
    "                    continue\n",
    "                    \n",
    "                yield dict(self._samples[self._next_dialog_idx][0], reset_state=True)\n",
    "                \n",
    "                self._dialog_indices[i] = self._next_dialog_idx\n",
    "                self._turn_indices[i] = 0\n",
    "                self._next_dialog_idx += 1\n",
    "                \n",
    "    def _reset(self):\n",
    "        self._dialog_indices = list(range(self._batch_size))\n",
    "        self._next_dialog_idx = self._batch_size\n",
    "        self._turn_indices = [0]*self._batch_size\n",
    "                \n",
    "    def batches(self):\n",
    "        self._reset()\n",
    "\n",
    "        while True:\n",
    "            reset_state = []\n",
    "            inputs, inputs_length, slot_targets = [], [], []\n",
    "            previous_output, previous_output_length = [], []\n",
    "            targets, targets_length = [], []\n",
    "            action_targets, query_result_states, query_results = [], [], []\n",
    "            slot_any_targets, slot_any_target_lengths = [], []\n",
    "            values = []\n",
    "            \n",
    "            for batch_sample in self._next_batch():\n",
    "                reset_state.append(batch_sample['reset_state'])\n",
    "                inputs.append(batch_sample['token_ids'])\n",
    "                inputs_length.append(len(batch_sample['token_ids']))\n",
    "                slot_targets.append(batch_sample['token_slot_ids'])\n",
    "                previous_output.append(batch_sample['previous_response_delexicalized_token_ids'])\n",
    "                previous_output_length.append(len(batch_sample['previous_response_delexicalized_token_ids']))\n",
    "                targets.append(batch_sample['next_response_delexicalized_token_ids'])\n",
    "                targets_length.append(len(batch_sample['next_response_delexicalized_token_ids']))\n",
    "                action_targets.append(batch_sample['next_action']),\n",
    "                query_result_states.append(batch_sample['query_state'])\n",
    "                query_results.append(batch_sample['query_result'])\n",
    "                values.append(batch_sample['was_booked'])\n",
    "                slot_any_targets.append(batch_sample['slot_any'])\n",
    "                slot_any_target_lengths.append(len(batch_sample['slot_any']))\n",
    "            \n",
    "            if len(inputs) != self._batch_size:\n",
    "                return\n",
    "            \n",
    "            max_inputs_length = np.clip(max(inputs_length), 1, self._max_sequence_len)\n",
    "            max_previous_output_length = np.clip(max(previous_output_length), 1, self._max_sequence_len)\n",
    "            max_targets_length = np.clip(max(targets_length), 1, self._max_sequence_len)\n",
    "            max_slot_any_length = max(slot_any_target_lengths)\n",
    "            \n",
    "            yield {\n",
    "                'reset_state': reset_state,\n",
    "                'inputs': pad_sequences(inputs, max_inputs_length),\n",
    "                'inputs_length': np.clip(inputs_length, 1, self._max_sequence_len),\n",
    "                'slot_targets': pad_sequences(slot_targets, max_inputs_length),\n",
    "                'previous_output': pad_sequences(previous_output, max_previous_output_length),\n",
    "                'previous_output_length': np.clip(previous_output_length, 1, self._max_sequence_len),\n",
    "                'targets': pad_sequences(targets, max_targets_length),\n",
    "                'targets_length': np.clip(targets_length, 1, self._max_sequence_len),\n",
    "                'action_targets': action_targets,\n",
    "                'query_result_state': query_result_states,\n",
    "                'query_result': pad_array_of_complex(query_results),\n",
    "                'value_targets': values,\n",
    "                'slot_any_targets': pad_sequences(slot_any_targets, max_slot_any_length, -1)\n",
    "            }\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "iterator = SamplesIterator(samples)\n",
    "for i, batch in enumerate(iterator.batches()):\n",
    "    print(batch['slot_any_targets'])\n",
    "    break\n",
    "#     print(batch['targets'].shape)\n",
    "#     print(max(batch['targets_length']))\n",
    "    if max(batch['targets_length']) != batch['targets'].shape[1]:\n",
    "        raise ValueError('Not match.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reset states: (64, 600)\n",
      "Agent(hidden_size=300, n_slots=12, n_actions=10)\n",
      "Training decoder helper.\n",
      "Metrics path ./data/metrics/\n",
      "0 6.59309\n",
      "1 6.36613\n",
      "2 5.94242\n",
      "3 5.59887\n",
      "4 5.61605\n",
      "5 5.60507\n",
      "6 5.56894\n",
      "7 5.58158\n",
      "8 5.53801\n",
      "9 5.32534\n",
      "10 5.47998\n",
      "11 5.49389\n",
      "12 5.46485\n",
      "13 5.41271\n",
      "14 5.36853\n",
      "15 5.395\n",
      "16 5.48433\n",
      "17 5.38326\n",
      "18 5.27748\n",
      "19 5.29896\n",
      "20 5.24432\n",
      "21 5.41015\n",
      "22 5.32368\n",
      "23 5.13847\n",
      "24 5.13333\n",
      "25 5.24538\n",
      "26 5.12053\n",
      "27 5.14955\n",
      "28 5.11167\n",
      "29 5.10806\n",
      "30 5.06245\n",
      "31 5.01074\n",
      "32 4.95219\n",
      "33 5.04302\n",
      "34 5.21646\n",
      "35 5.24043\n",
      "36 5.12876\n",
      "37 5.15678\n",
      "38 5.10059\n",
      "39 5.21341\n",
      "40 4.97453\n",
      "41 5.15807\n",
      "42 5.2777\n",
      "43 5.23047\n",
      "44 5.24202\n",
      "45 5.15746\n",
      "46 5.14154\n",
      "47 5.10926\n",
      "48 5.1259\n",
      "49 5.15923\n",
      "50 4.98816\n",
      "51 4.96738\n",
      "52 4.81295\n",
      "53 4.80407\n",
      "54 4.84855\n",
      "55 4.90375\n",
      "56 5.07379\n",
      "57 5.21744\n",
      "58 5.21676\n",
      "59 5.10267\n",
      "60 5.25777\n",
      "61 5.21309\n",
      "62 5.05522\n",
      "63 5.03737\n",
      "64 5.00446\n",
      "65 5.0099\n",
      "66 5.00137\n",
      "67 4.9684\n",
      "68 4.94365\n",
      "69 5.0175\n",
      "70 4.76601\n",
      "71 5.07694\n",
      "72 5.20457\n",
      "73 5.16069\n",
      "74 5.01017\n",
      "75 4.89955\n",
      "76 5.2051\n",
      "77 5.1509\n",
      "78 4.99855\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-115-c086189442d2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-113-415bc77988a5>\u001b[0m in \u001b[0;36mtrain_batch\u001b[0;34m(self, i, batch)\u001b[0m\n\u001b[1;32m    157\u001b[0m         _, new_states, metrics_val, loss_val = self._sess.run(\n\u001b[1;32m    158\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_metrics_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 159\u001b[0;31m             \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_feed_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    160\u001b[0m         )\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    trainer = SupervisedTrainer(n_slots=len(slots_dict), n_actions=len(actions_dict), word_embeddings_shape=embeddings.shape, save_path='./data')\n",
    "    iterator = SamplesIterator(samples)\n",
    "    \n",
    "    trainer._sess.run(tf.global_variables_initializer())\n",
    "    trainer.initialize_word_embeddings(embeddings)\n",
    "    \n",
    "    for i, batch in enumerate(iterator.batches()):\n",
    "        print(i, trainer.train_batch(i, batch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 200.,  200.,  200.,  200.,  200.],\n",
       "        [ 200.,  200.,  200.,  200.,  200.],\n",
       "        [ 200.,  200.,  200.,  200.,  200.],\n",
       "        [ 200.,  200.,  200.,  200.,  200.],\n",
       "        [ 200.,  200.,  200.,  200.,  200.],\n",
       "        [ 200.,  200.,  200.,  200.,  200.],\n",
       "        [ 200.,  200.,  200.,  200.,  200.],\n",
       "        [ 200.,  200.,  200.,  200.,  200.],\n",
       "        [ 200.,  200.,  200.,  200.,  200.],\n",
       "        [ 200.,  200.,  200.,  200.,  200.],\n",
       "        [ 200.,  200.,  200.,  200.,  200.],\n",
       "        [ 200.,  200.,  200.,  200.,  200.]],\n",
       "\n",
       "       [[ 100.,  100.,  100.,  100.,  100.],\n",
       "        [ 100.,  100.,  100.,  100.,  100.],\n",
       "        [ 100.,  100.,  100.,  100.,  100.],\n",
       "        [ 100.,  100.,  100.,  100.,  100.],\n",
       "        [ 100.,  100.,  100.,  100.,  100.],\n",
       "        [ 100.,  100.,  100.,  100.,  100.],\n",
       "        [ 100.,  100.,  100.,  100.,  100.],\n",
       "        [ 100.,  100.,  100.,  100.,  100.],\n",
       "        [ 100.,  100.,  100.,  100.,  100.],\n",
       "        [ 100.,  100.,  100.,  100.,  100.],\n",
       "        [ 100.,  100.,  100.,  100.,  100.],\n",
       "        [ 100.,  100.,  100.,  100.,  100.]],\n",
       "\n",
       "       [[  50.,   50.,   50.,   50.,   50.],\n",
       "        [  50.,   50.,   50.,   50.,   50.],\n",
       "        [  50.,   50.,   50.,   50.,   50.],\n",
       "        [  50.,   50.,   50.,   50.,   50.],\n",
       "        [  50.,   50.,   50.,   50.,   50.],\n",
       "        [  50.,   50.,   50.,   50.,   50.],\n",
       "        [  50.,   50.,   50.,   50.,   50.],\n",
       "        [  50.,   50.,   50.,   50.,   50.],\n",
       "        [  50.,   50.,   50.,   50.,   50.],\n",
       "        [  50.,   50.,   50.,   50.,   50.],\n",
       "        [  50.,   50.,   50.,   50.,   50.],\n",
       "        [  50.,   50.,   50.,   50.,   50.]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros((3, 12, 5))\n",
    "z = np.ones((3, 12, 5)) * 5\n",
    "b = np.zeros(12)\n",
    "\n",
    "for _ in range(40):\n",
    "    for i, d in enumerate([1, 2, 4]):\n",
    "        pred = b % d == 0\n",
    "        a[i,pred,:] += z[i,pred,:]\n",
    "#         print(i, d, pred)\n",
    "    b += 1\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 0 Update: 3 Get: 0 a\n",
      "\n",
      "Step: 1 Update: 2 Get: 1 b\n",
      "\n",
      "Step: 2 Update: 1 Get: 2 cc\n",
      "\n",
      "Step: 3 Update: 0 Get: 3 dd\n",
      "\n",
      "Step: 4 Update: 3 Get: 0 aa\n",
      "\n",
      "Step: 5 Update: 2 Get: 1 bb\n",
      "\n",
      "Step: 6 Update: 1 Get: 2 ccc\n",
      "\n",
      "Step: 7 Update: 0 Get: 3 ddd\n",
      "\n",
      "Step: 8 Update: 3 Get: 0 aaa\n",
      "\n",
      "Step: 9 Update: 2 Get: 1 bbb\n",
      "\n",
      "Step: 10 Update: 1 Get: 2 cccc\n",
      "\n",
      "Step: 11 Update: 0 Get: 3 dddd\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['aaaa', 'bbbb', 'cccc', 'dddd']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dilatation = 4\n",
    "a = ['a', 'b', 'c', 'd']\n",
    "\n",
    "for d in range(12):\n",
    "    update_id = (dilatation-1) - d % dilatation\n",
    "    get_id = d % dilatation\n",
    "    a[update_id] += a[update_id][-1]\n",
    "    print('Step:', d, 'Update:', update_id, 'Get:', get_id, a[get_id])\n",
    "    print()\n",
    "\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
