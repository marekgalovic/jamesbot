{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def masked_softmax(values, lengths, time_major=False, mask_value=-np.inf):\n",
    "    with tf.name_scope('MaskedSoftmax'):\n",
    "        if time_major:\n",
    "            mask = tf.expand_dims(tf.transpose(tf.sequence_mask(lengths, tf.reduce_max(lengths), dtype=tf.float32)), -1)\n",
    "        else:\n",
    "            mask = tf.expand_dims(tf.sequence_mask(lengths, tf.reduce_max(lengths), dtype=tf.float32), -2)\n",
    "\n",
    "        inf_mask = (1 - mask) * mask_value\n",
    "        inf_mask = tf.where(tf.is_nan(inf_mask), tf.zeros_like(inf_mask), inf_mask)\n",
    "\n",
    "        return tf.nn.softmax(tf.multiply(values, mask) + inf_mask, 0 if time_major else -1)\n",
    "\n",
    "class InputPipeline(object):\n",
    "    \n",
    "    def __init__(self, filenames, batch_size=32, n_epochs=50, capacity=1e4):\n",
    "        self._filenames = filenames\n",
    "        self._batch_size = batch_size\n",
    "        self._n_epochs = n_epochs\n",
    "        self._capacity = capacity\n",
    "        \n",
    "        print('InputPipeline - batch_size:', self._batch_size, 'n_epochs:', self._n_epochs, 'capacity:', self._capacity)\n",
    "    \n",
    "\n",
    "    def _queue_reader(self):\n",
    "        with tf.device('/cpu:0'):\n",
    "            file_queue = tf.train.string_input_producer(self._filenames, num_epochs = self._n_epochs, capacity=self._capacity)\n",
    "\n",
    "            reader = tf.TFRecordReader()\n",
    "            _, serialized_example = reader.read(file_queue)\n",
    "\n",
    "            return tf.parse_single_sequence_example(\n",
    "                serialized_example,\n",
    "                context_features = {\n",
    "                    'input_seq': tf.VarLenFeature(tf.int64),\n",
    "                    'target_seq': tf.VarLenFeature(tf.int64),\n",
    "                    'input_seq_len': tf.FixedLenFeature([], tf.int64),\n",
    "                    'target_seq_len': tf.FixedLenFeature([], tf.int64),\n",
    "                    'history_size': tf.FixedLenFeature([], tf.int64),\n",
    "                    'history_seq_len': tf.VarLenFeature(tf.int64),\n",
    "                },\n",
    "                sequence_features = {\n",
    "                    'history': tf.VarLenFeature(tf.int64)\n",
    "                }\n",
    "            )\n",
    "      \n",
    "\n",
    "    def inputs(self):\n",
    "        with tf.device('/cpu:0'):\n",
    "            context_features, sequence_features = self._queue_reader()\n",
    "            \n",
    "            _, (input_seq, output_seq, input_seq_len, output_seq_len, history, history_size, history_seq_len) = tf.contrib.training.bucket_by_sequence_length(\n",
    "                tf.reduce_mean([tf.cast(context_features['input_seq_len'], tf.int32), tf.cast(context_features['target_seq_len'], tf.int32)]),\n",
    "                [\n",
    "                    context_features['input_seq'],\n",
    "                    context_features['target_seq'],\n",
    "                    context_features['input_seq_len'],\n",
    "                    context_features['target_seq_len'],\n",
    "                    sequence_features['history'],\n",
    "                    context_features['history_size'],\n",
    "                    context_features['history_seq_len']\n",
    "                ],\n",
    "                batch_size = self._batch_size,\n",
    "                bucket_boundaries=[5, 10, 15, 20, 30, 40, 50],\n",
    "                num_threads=8,\n",
    "                capacity = self._capacity,\n",
    "                allow_smaller_final_batch = True,\n",
    "                dynamic_pad=True\n",
    "            )\n",
    "            \n",
    "            return (\n",
    "                tf.transpose(tf.cast(tf.sparse_tensor_to_dense(input_seq), tf.int32)),\n",
    "                tf.transpose(tf.cast(tf.sparse_tensor_to_dense(output_seq), tf.int32)),\n",
    "                tf.transpose(tf.cast(tf.sparse_tensor_to_dense(history), tf.int32), [1,0,2]),\n",
    "                tf.cast(input_seq_len, tf.int32),\n",
    "                tf.cast(output_seq_len, tf.int32),\n",
    "                tf.cast(history_size, tf.int32),\n",
    "                tf.cast(tf.sparse_tensor_to_dense(history_seq_len), tf.int32),\n",
    "            )\n",
    "        \n",
    "class DenseLayer(object):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, activation=None, name='DenseLayer'):\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self._activation = activation\n",
    "        \n",
    "        with tf.name_scope(name):\n",
    "            self._W = tf.Variable(tf.truncated_normal([self.input_size, self.output_size], mean=0, stddev=0.1), dtype=tf.float32, name='W')\n",
    "            self._b = tf.Variable(tf.zeros([self.output_size]), dtype=tf.float32, name='b')\n",
    "            \n",
    "            tf.summary.histogram('weights', self._W)\n",
    "            tf.summary.histogram('bias', self._b)\n",
    "    \n",
    "    def __call__(self, inputs):\n",
    "        linear = tf.add(tf.matmul(inputs, self._W), self._b)\n",
    "        \n",
    "        if self._activation is not None:\n",
    "            return self._activation(linear)\n",
    "        \n",
    "        return linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Attention mechanisms\n",
    "\n",
    "class _Attention(object):\n",
    "    \n",
    "    def __init__(self, memory, memory_len=None):\n",
    "        '''\n",
    "        Abstract attention mechanism class.\n",
    "        \n",
    "        :param memory: Memory tensor to query. Shape: [num_steps x batch_size x embedding_size]\n",
    "        :param memory_len: Optional memory tensor length. Shape: [batch_size]\n",
    "        '''\n",
    "        \n",
    "        assert len(memory.get_shape()) == 3, 'Memory must have rank 3. [num_steps x batch_size x embedding_size]'\n",
    "        assert memory.get_shape()[-1] is not None, 'Last dimension of memory can not be None'\n",
    "        \n",
    "        self._memory = memory\n",
    "        self._memory_len = memory_len\n",
    "        self._memory_width = int(self._memory.get_shape()[-1])\n",
    "        \n",
    "    def __call__(self, query):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def _align_query_with_memory(self, query):\n",
    "        num_steps, _, _ = tf.unstack(tf.shape(self._memory))\n",
    "        \n",
    "        return tf.concat([\n",
    "            tf.tile(tf.expand_dims(query, 0), [num_steps,1,1]),\n",
    "            self._memory\n",
    "        ], 2)\n",
    "\n",
    "        \n",
    "class BahdanauAttention(_Attention):\n",
    "    \n",
    "    def __init__(self, memory, mask_value=-np.inf, **kwargs):\n",
    "        super(BahdanauAttention, self).__init__(memory, **kwargs)\n",
    "        self._mask_value = mask_value\n",
    "        \n",
    "        with tf.name_scope('BahdanauAttention'):\n",
    "            self._attention_W = tf.Variable(tf.truncated_normal([self._memory_width*2, 1], mean=0, stddev=0.1), dtype=tf.float32, name='weights')\n",
    "            self._attention_b = tf.Variable(tf.zeros([1]), dtype=tf.float32, name='bias')\n",
    "            \n",
    "            tf.summary.histogram('attention_w', self._attention_W)\n",
    "            tf.summary.histogram('attention_b', self._attention_b)\n",
    "    \n",
    "    def _attention_weights(self, query):\n",
    "        num_steps, batch_size, _ = tf.unstack(tf.shape(self._memory))\n",
    "        \n",
    "        aligned = tf.reshape(self._align_query_with_memory(query), [-1, self._memory_width*2])\n",
    "        weights = tf.reshape(tf.add(tf.matmul(aligned, self._attention_W), self._attention_b), [num_steps, batch_size, 1])\n",
    "        \n",
    "        if self._memory_len is not None:\n",
    "            return masked_softmax(weights, self._memory_len, time_major=True, mask_value=self._mask_value)\n",
    "    \n",
    "        return tf.nn.softmax(weights, 0)\n",
    "            \n",
    "    def __call__(self, query):\n",
    "        assert query.get_shape()[-1] == self._memory_width, 'Last dimension of query must have size %d' % (self._memory_width)\n",
    "        \n",
    "        weights = self._attention_weights(query)\n",
    "        \n",
    "        return tf.reduce_sum(tf.multiply(weights, self._memory), 0)\n",
    "    \n",
    "\n",
    "class GatedAttention(_Attention):\n",
    "    \n",
    "    def __init__(self, memory, residual=False, **kwargs):\n",
    "        super(GatedAttention, self).__init__(memory, **kwargs)\n",
    "        self._residual = residual\n",
    "        \n",
    "        with tf.name_scope('GatedAttention'):\n",
    "            self._attention_W = tf.Variable(tf.truncated_normal([self._memory_width*2, self._memory_width], mean=0, stddev=0.1), dtype=tf.float32, name='weights')\n",
    "            self._attention_b = tf.Variable(tf.zeros([self._memory_width]), dtype=tf.float32, name='bias')\n",
    "            \n",
    "            tf.summary.histogram('attention_w', self._attention_W)\n",
    "            tf.summary.histogram('attention_b', self._attention_b)\n",
    "            \n",
    "    def _attention_weights(self, query):\n",
    "        num_steps, batch_size, _ = tf.unstack(tf.shape(self._memory))\n",
    "        \n",
    "        aligned = tf.reshape(self._align_query_with_memory(query), [-1, self._memory_width*2])\n",
    "        weights = tf.reshape(tf.add(tf.matmul(aligned, self._attention_W), self._attention_b), [num_steps, batch_size, self._memory_width])\n",
    "        \n",
    "        return tf.sigmoid(weights)\n",
    "        \n",
    "    \n",
    "    def __call__(self, query):\n",
    "        assert query.get_shape()[-1] == self._memory_width, 'Last dimension of query must have size %d' % (self._memory_width)\n",
    "        \n",
    "        weights = self._attention_weights(query)\n",
    "        c = tf.reduce_sum(tf.multiply(weights, self._memory), 0)\n",
    "        \n",
    "        if self._residual:\n",
    "            c = tf.add(query, c)\n",
    "        \n",
    "        return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class AttentionWrapper(tf.contrib.rnn.RNNCell):\n",
    "    \n",
    "    def __init__(self, cell, attention):\n",
    "        assert isinstance(attention, _Attention), 'Param :attention should be an _Attention class instance'\n",
    "        \n",
    "        with tf.name_scope('AttentionWrapper'):\n",
    "            self._cell = cell\n",
    "            self._attention = attention\n",
    "            self._candidate_state = DenseLayer(cell.state_size*3, cell.state_size, activation=tf.tanh, name='candidate_state')\n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._cell.state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._cell.output_size\n",
    "    \n",
    "    def zero_state(self, batch_size, dtype):\n",
    "        with tf.name_scope(type(self).__name__+'ZeroState', values=[batch_size]):\n",
    "            return self._cell.zero_state(batch_size, dtype)\n",
    "    \n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        new_output, new_state = self._cell(inputs, state, scope=scope)\n",
    "        \n",
    "        context = self._attention(new_state)\n",
    "        attended_state = self._candidate_state(tf.concat([new_state, new_output, context], 1))\n",
    "        \n",
    "        return (new_output, attended_state)\n",
    "    \n",
    "\n",
    "class MultiAttentionWrapper(tf.contrib.rnn.RNNCell):\n",
    "    \n",
    "    def __init__(self, cell, *attentions):\n",
    "        assert all([isinstance(attention, _Attention) for attention in attentions]), 'Param :attentions should be an _Attention class instance'\n",
    "        \n",
    "        with tf.name_scope('MultiAttentionWraper'):\n",
    "            self._cell = cell\n",
    "            self._attentions = attentions\n",
    "            self._candidate_state = DenseLayer(cell.state_size*(2+len(self._attentions)), cell.state_size, activation=tf.tanh, name='candidate_state')\n",
    "            \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return self._cell.state_size\n",
    "\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return self._cell.output_size\n",
    "    \n",
    "    def zero_state(self, batch_size, dtype):\n",
    "        with tf.name_scope(type(self).__name__+'ZeroState', values=[batch_size]):\n",
    "            return self._cell.zero_state(batch_size, dtype)\n",
    "        \n",
    "    def __call__(self, inputs, state, scope=None):\n",
    "        new_output, new_state = self._cell(inputs, state, scope=scope)\n",
    "        \n",
    "        contexts = tf.concat([attention(new_state) for attention in self._attentions], 1)\n",
    "        attended_state = self._candidate_state(tf.concat([new_state, new_output, contexts], 1))\n",
    "        \n",
    "        return (new_output, attended_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "class Decoder(object):\n",
    "    \n",
    "    def __init__(self, cell, initial_state, encoder_outputs, embeddings, targets=None, targets_length=None, feed_previous=False):\n",
    "        '''\n",
    "        :param cell: Tensorflow RNNCell instance\n",
    "        :param initial_state: Initial cell state (encoder final state)\n",
    "        :param decoder_targets: Decoder true outputs. If this param is passed true targets will be passed as\n",
    "        cell input.\n",
    "        '''\n",
    "        self._cell = cell\n",
    "        self._initial_state = initial_state\n",
    "        self._encoder_outputs = encoder_outputs\n",
    "        self._embeddings = embeddings\n",
    "        self._targets = targets\n",
    "        self._targets_length = targets_length\n",
    "        self._feed_previous = feed_previous\n",
    "    \n",
    "    def __call__(self, encoder_state, encoder_outputs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq - stacked:\n",
      "State size: 500\n",
      "Embeddings shape: [7800, 300]\n",
      "Feed previous: False\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "class Seq2Seq(object):\n",
    "    \n",
    "    def __init__(self, state_size, embeddings_shape, feed_previous=False):\n",
    "        self._state_size = state_size\n",
    "        self._embeddings_shape = embeddings_shape\n",
    "        self._feed_previous = feed_previous\n",
    "        \n",
    "        print('Seq2Seq - stacked:')\n",
    "        print('State size:', self._state_size)\n",
    "        print('Embeddings shape:', self._embeddings_shape)\n",
    "        print('Feed previous:', self._feed_previous)\n",
    "        \n",
    "    def graph(self, input_seq, target_seq, input_seq_len, target_seq_len, history, history_size, history_seq_len):\n",
    "        '''\n",
    "        :param input_seq: Input sequence tensor\n",
    "        :param target_seq: Target sequence tensor\n",
    "        :param input_seq_len: Input sequence length tensor\n",
    "        :param target_seq_len: Target sequence length tensor\n",
    "        :param history: Sequence history tensor\n",
    "        :param history_size: Tensor representing number of sequences in history.\n",
    "        :param history_seq_len: History sequences length\n",
    "        '''\n",
    "        self._input_seq = input_seq\n",
    "        self._target_seq = target_seq\n",
    "        self._input_seq_len = input_seq_len\n",
    "        self._target_seq_len = target_seq_len\n",
    "        # History\n",
    "        self._history = history\n",
    "        self._history_size = history_size\n",
    "        self._history_seq_len = history_seq_len\n",
    "        \n",
    "        # Build\n",
    "        self._initialize_embeddings()\n",
    "        self._context_encoder()\n",
    "        self._encoder()\n",
    "        self._context_attention()\n",
    "        self._decoder()\n",
    "        \n",
    "        return self._loss()\n",
    "    \n",
    "    def _initialize_embeddings(self):\n",
    "        self._embeddings = tf.Variable(tf.random_uniform(self._embeddings_shape, -0.5, 0.5), dtype=tf.float32, name='embeddings')\n",
    "        \n",
    "        with tf.device('/cpu:0'):\n",
    "            _, batch_size = tf.unstack(tf.shape(self._input_seq))\n",
    "            \n",
    "            self._input_seq_embedded = tf.nn.embedding_lookup(self._embeddings, self._input_seq)\n",
    "            self._history_embbedded = tf.nn.embedding_lookup(self._embeddings, self._history)\n",
    "            self._pad = tf.nn.embedding_lookup(self._embeddings, tf.zeros([batch_size], dtype=tf.int32))\n",
    "            self._eos = tf.nn.embedding_lookup(self._embeddings, tf.ones([batch_size], dtype=tf.int32))\n",
    "    \n",
    "    def _context_encoder(self):\n",
    "        # _context shape: [num_sentences x batch_size x embedding_size]\n",
    "        with tf.variable_scope('context_encoder'):\n",
    "            n_messages, batch_size, num_steps, _ = tf.unstack(tf.shape(self._history_embbedded))\n",
    "            history_flattened = tf.transpose(tf.reshape(self._history_embbedded, [-1, num_steps, self._embeddings_shape[1]]), [1,0,2])\n",
    "            \n",
    "            fw_cell = tf.contrib.rnn.GRUCell(self._state_size)\n",
    "            bw_cell = tf.contrib.rnn.GRUCell(self._state_size)\n",
    "            \n",
    "            _, encoder_state = tf.nn.bidirectional_dynamic_rnn(\n",
    "                fw_cell, bw_cell,\n",
    "                history_flattened,\n",
    "                sequence_length=tf.reshape(self._history_seq_len, [-1]),\n",
    "                dtype=tf.float32,\n",
    "                time_major=True\n",
    "            )\n",
    "            \n",
    "            # Project [2*state_size] -> [state_size]\n",
    "            projected_state = tf.layers.dense(tf.concat(encoder_state, 1), self._state_size, activation=tf.tanh, name='context_encoder_state_projection')\n",
    "            \n",
    "            self._context = tf.reshape(projected_state, [n_messages, batch_size, self._state_size])\n",
    "            tf.summary.histogram('context', self._context)\n",
    "        \n",
    "    def _encoder(self):\n",
    "        with tf.variable_scope('encoder'):\n",
    "            with tf.name_scope('bi-directional'):\n",
    "                fw_cell = tf.contrib.rnn.GRUCell(self._state_size)\n",
    "                bw_cell = tf.contrib.rnn.GRUCell(self._state_size)\n",
    "\n",
    "                bi_encoder_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                    fw_cell, bw_cell,\n",
    "                    self._input_seq_embedded,\n",
    "                    sequence_length = self._input_seq_len,\n",
    "                    dtype=tf.float32,\n",
    "                    time_major=True\n",
    "                )\n",
    "                \n",
    "                bi_encoder_outputs = tf.concat(bi_encoder_outputs, 2)\n",
    "            \n",
    "            with tf.name_scope('uni-directional'):\n",
    "                cell = tf.contrib.rnn.MultiRNNCell([\n",
    "                    tf.contrib.rnn.GRUCell(self._state_size),\n",
    "                    tf.contrib.rnn.GRUCell(self._state_size),\n",
    "                ], state_is_tuple=True)\n",
    "                \n",
    "                uni_encoder_outputs, uni_encoder_state = tf.nn.dynamic_rnn(\n",
    "                    cell,\n",
    "                    bi_encoder_outputs,\n",
    "                    sequence_length = self._input_seq_len,\n",
    "                    dtype = tf.float32,\n",
    "                    time_major = True\n",
    "                )\n",
    "                \n",
    "                self._encoder_outputs = uni_encoder_outputs\n",
    "                self._encoder_state = uni_encoder_state[-1]\n",
    "            \n",
    "            tf.summary.histogram('encoder_outputs', self._encoder_outputs)\n",
    "            tf.summary.histogram('encoder_state', self._encoder_state)\n",
    "    \n",
    "    def _context_attention(self):\n",
    "        with tf.name_scope('context_attention'):\n",
    "            context_attention = BahdanauAttention(self._context, memory_len=self._history_size, mask_value=1e-18)\n",
    "\n",
    "            self._encoder_state_with_context = context_attention(self._encoder_state)\n",
    "        \n",
    "    def _initialize_decoder_params(self):\n",
    "        self._output_projection_layer = DenseLayer(self._state_size, self._embeddings_shape[0], name='output_projection')\n",
    "        \n",
    "        # Prepare targets tensor array if feed_previous=False\n",
    "        if not self._feed_previous:\n",
    "            with tf.device('/cpu:0'):\n",
    "                _target_seq_embedded = tf.nn.embedding_lookup(self._embeddings, self._target_seq)\n",
    "        \n",
    "            _targets_ta = tf.TensorArray(dtype=tf.float32, size=tf.reduce_max(self._target_seq_len))\n",
    "            self._targets_ta = _targets_ta.unstack(_target_seq_embedded)\n",
    "    \n",
    "    def _decoder(self):  \n",
    "        with tf.variable_scope('decoder'):\n",
    "            self._initialize_decoder_params()\n",
    "            \n",
    "            attention_cell = MultiAttentionWrapper(\n",
    "                tf.contrib.rnn.GRUCell(self._state_size),\n",
    "                BahdanauAttention(self._encoder_outputs, memory_len=self._input_seq_len),\n",
    "                BahdanauAttention(self._context, memory_len=self._history_size, mask_value=1e-18)\n",
    "            )\n",
    "\n",
    "            cell = tf.contrib.rnn.MultiRNNCell([\n",
    "                # Only first cell has attention\n",
    "                attention_cell,\n",
    "                # Other cells\n",
    "                tf.contrib.rnn.GRUCell(self._state_size),\n",
    "                tf.contrib.rnn.GRUCell(self._state_size)\n",
    "            ], state_is_tuple=True)\n",
    "\n",
    "            decoder_outputs_ta, _, _ = tf.nn.raw_rnn(cell, self._decoder_loop_fn)\n",
    "            decoder_outputs = decoder_outputs_ta.stack()\n",
    "            \n",
    "            tf.summary.histogram('decoder_outputs', decoder_outputs)\n",
    "\n",
    "            num_steps, batch_size, decoder_output_size = tf.unstack(tf.shape(decoder_outputs))\n",
    "            self._decoder_logits = tf.reshape(\n",
    "                self._output_projection_layer(tf.reshape(decoder_outputs, [-1, decoder_output_size])),\n",
    "                [num_steps, batch_size, self._embeddings_shape[0]]\n",
    "            )\n",
    "\n",
    "            self.decoder_embedding_ids = tf.cast(tf.argmax(self._decoder_logits, 2), tf.int32)\n",
    "    \n",
    "    def _decoder_loop_fn(self, time, previous_output, previous_state, previous_loop_state):\n",
    "        is_finished = tf.greater_equal(time, self._target_seq_len)\n",
    "            \n",
    "        if previous_state is None:\n",
    "            # Initial state\n",
    "            return (is_finished, self._eos, tuple([self._encoder_state_with_context]*3), None, None)\n",
    "        \n",
    "        def _next_input():\n",
    "            if not self._feed_previous:\n",
    "                return self._targets_ta.read(time - 1)\n",
    "            \n",
    "            embedding_ids = tf.argmax(self._output_projection_layer(previous_output), 1)\n",
    "            with tf.device('/cpu:0'):\n",
    "                return tf.nn.embedding_lookup(self._embeddings, embedding_ids)\n",
    "        \n",
    "        next_input = tf.cond(tf.reduce_all(is_finished), lambda: self._pad, _next_input)\n",
    "        \n",
    "        return (is_finished, next_input, previous_state, previous_output, None)\n",
    "    \n",
    "    def _loss(self):\n",
    "        stepwise_ce = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels = tf.one_hot(self._target_seq, self._embeddings_shape[0]),\n",
    "            logits = self._decoder_logits\n",
    "        )\n",
    "        \n",
    "        self.loss = tf.reduce_mean(stepwise_ce)\n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "        \n",
    "        self.accuracy = tf.reduce_mean(tf.cast(tf.equal(self._target_seq, self.decoder_embedding_ids), tf.float32))\n",
    "        tf.summary.scalar('accuracy', self.accuracy)\n",
    "        \n",
    "        return self.loss\n",
    "\n",
    "\n",
    "with tf.Graph().as_default() as graph:\n",
    "    input_seq = tf.placeholder(tf.int32, [None, None])\n",
    "    target_seq = tf.placeholder(tf.int32, [50, 32])\n",
    "    input_seq_len = tf.placeholder(tf.int32, [None])\n",
    "    target_seq_len = tf.placeholder(tf.int32, [32])\n",
    "    history = tf.placeholder(tf.int32, [None, None, None])\n",
    "    history_size = tf.placeholder(tf.int32, [None])\n",
    "    history_seq_len = tf.placeholder(tf.int32, [None, None])\n",
    "\n",
    "    model = Seq2Seq(500, [7800,300])\n",
    "    model.graph(input_seq, target_seq, input_seq_len, target_seq_len, history, history_size, history_seq_len)\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq - stacked:\n",
      "State size: 150\n",
      "Embeddings shape: [15, 50]\n",
      "Feed previous: False\n",
      "InputPipeline - batch_size: 64 n_epochs: 50 capacity: 10000.0\n",
      "Loss: 2.71297 Accuracy: 0.178125 Step: 1\n",
      "Loss: 2.59408 Accuracy: 0.15625 Step: 2\n",
      "Loss: 2.48051 Accuracy: 0.2375 Step: 3\n",
      "Loss: 2.42493 Accuracy: 0.174107 Step: 4\n",
      "Loss: 2.3006 Accuracy: 0.240625 Step: 5\n",
      "Loss: 2.35138 Accuracy: 0.174107 Step: 6\n",
      "Loss: 2.15192 Accuracy: 0.271875 Step: 7\n",
      "Loss: 2.20017 Accuracy: 0.200893 Step: 8\n",
      "Loss: 2.15671 Accuracy: 0.221875 Step: 9\n",
      "Loss: 2.11477 Accuracy: 0.229911 Step: 10\n",
      "Loss: 2.14174 Accuracy: 0.203125 Step: 11\n",
      "Loss: 2.08963 Accuracy: 0.272321 Step: 12\n",
      "Loss: 2.06514 Accuracy: 0.359375 Step: 13\n",
      "Loss: 2.12444 Accuracy: 0.28125 Step: 14\n",
      "Loss: 2.02451 Accuracy: 0.359375 Step: 15\n",
      "Loss: 2.10033 Accuracy: 0.294643 Step: 16\n",
      "Loss: 2.0054 Accuracy: 0.384375 Step: 17\n",
      "Loss: 2.0999 Accuracy: 0.287946 Step: 18\n",
      "Loss: 2.04236 Accuracy: 0.3625 Step: 19\n",
      "Loss: 2.06664 Accuracy: 0.274554 Step: 20\n",
      "Loss: 2.06098 Accuracy: 0.35 Step: 21\n",
      "Loss: 2.04698 Accuracy: 0.285714 Step: 22\n",
      "Loss: 2.04085 Accuracy: 0.371875 Step: 23\n",
      "Loss: 2.04083 Accuracy: 0.276786 Step: 24\n",
      "Loss: 1.98998 Accuracy: 0.33125 Step: 25\n",
      "Loss: 2.03331 Accuracy: 0.274554 Step: 26\n",
      "Loss: 2.04216 Accuracy: 0.261161 Step: 27\n",
      "Loss: 2.0261 Accuracy: 0.409375 Step: 28\n",
      "Loss: 2.01455 Accuracy: 0.301339 Step: 29\n",
      "Loss: 1.95044 Accuracy: 0.384375 Step: 30\n",
      "Loss: 1.90062 Accuracy: 0.415625 Step: 31\n",
      "Loss: 2.11968 Accuracy: 0.276786 Step: 32\n",
      "Loss: 1.88325 Accuracy: 0.396875 Step: 33\n",
      "Loss: 1.98025 Accuracy: 0.285714 Step: 34\n",
      "Loss: 1.87008 Accuracy: 0.39375 Step: 35\n",
      "Loss: 1.98389 Accuracy: 0.330357 Step: 36\n",
      "Loss: 1.8509 Accuracy: 0.359375 Step: 37\n",
      "Loss: 2.01614 Accuracy: 0.267857 Step: 38\n",
      "Loss: 1.80912 Accuracy: 0.365625 Step: 39\n",
      "Loss: 1.9232 Accuracy: 0.34375 Step: 40\n",
      "Loss: 1.81827 Accuracy: 0.4125 Step: 41\n",
      "Loss: 1.88963 Accuracy: 0.301339 Step: 42\n",
      "Loss: 1.78187 Accuracy: 0.4 Step: 43\n",
      "Loss: 1.86386 Accuracy: 0.357143 Step: 44\n",
      "Loss: 1.75769 Accuracy: 0.446875 Step: 45\n",
      "Loss: 1.75423 Accuracy: 0.440625 Step: 46\n",
      "Loss: 1.96123 Accuracy: 0.3125 Step: 47\n",
      "Loss: 1.86922 Accuracy: 0.345982 Step: 48\n",
      "Loss: 1.84041 Accuracy: 0.4 Step: 49\n",
      "Loss: 1.86935 Accuracy: 0.372768 Step: 50\n",
      "Loss: 1.758 Accuracy: 0.46875 Step: 51\n",
      "Loss: 1.84304 Accuracy: 0.377232 Step: 52\n",
      "Loss: 1.73046 Accuracy: 0.465625 Step: 53\n",
      "Loss: 1.83144 Accuracy: 0.388393 Step: 54\n",
      "Loss: 1.66559 Accuracy: 0.44375 Step: 55\n",
      "Loss: 1.84869 Accuracy: 0.334821 Step: 56\n",
      "Loss: 1.67688 Accuracy: 0.490625 Step: 57\n",
      "Loss: 1.81108 Accuracy: 0.381696 Step: 58\n",
      "Loss: 1.64241 Accuracy: 0.49375 Step: 59\n",
      "Loss: 1.78213 Accuracy: 0.354911 Step: 60\n",
      "Loss: 1.63765 Accuracy: 0.4875 Step: 61\n",
      "Loss: 1.75996 Accuracy: 0.368304 Step: 62\n",
      "Loss: 1.60047 Accuracy: 0.428125 Step: 63\n",
      "Loss: 1.80314 Accuracy: 0.408482 Step: 64\n",
      "Loss: 1.58901 Accuracy: 0.4875 Step: 65\n",
      "Loss: 1.78551 Accuracy: 0.377232 Step: 66\n",
      "Loss: 1.59991 Accuracy: 0.484375 Step: 67\n",
      "Loss: 1.74776 Accuracy: 0.386161 Step: 68\n",
      "Loss: 1.58559 Accuracy: 0.490625 Step: 69\n",
      "Loss: 1.70393 Accuracy: 0.404018 Step: 70\n",
      "Loss: 1.54637 Accuracy: 0.5 Step: 71\n",
      "Loss: 1.72637 Accuracy: 0.368304 Step: 72\n",
      "Loss: 1.59586 Accuracy: 0.490625 Step: 73\n",
      "Loss: 1.77585 Accuracy: 0.345982 Step: 74\n",
      "Loss: 1.58033 Accuracy: 0.45 Step: 75\n",
      "Loss: 1.84699 Accuracy: 0.386161 Step: 76\n",
      "Loss: 1.55006 Accuracy: 0.53125 Step: 77\n",
      "Loss: 1.79616 Accuracy: 0.341518 Step: 78\n",
      "Loss: 1.56516 Accuracy: 0.475 Step: 79\n",
      "Loss: 1.75018 Accuracy: 0.350446 Step: 80\n",
      "Loss: 1.5599 Accuracy: 0.484375 Step: 81\n",
      "Loss: 1.70971 Accuracy: 0.370536 Step: 82\n",
      "Loss: 1.53422 Accuracy: 0.5125 Step: 83\n",
      "Loss: 1.70278 Accuracy: 0.415179 Step: 84\n",
      "Loss: 1.55292 Accuracy: 0.490625 Step: 85\n",
      "Loss: 1.67625 Accuracy: 0.368304 Step: 86\n",
      "Loss: 1.55183 Accuracy: 0.475 Step: 87\n",
      "Loss: 1.65764 Accuracy: 0.401786 Step: 88\n",
      "Loss: 1.54312 Accuracy: 0.521875 Step: 89\n",
      "Loss: 1.68732 Accuracy: 0.377232 Step: 90\n",
      "Loss: 1.50811 Accuracy: 0.496875 Step: 91\n",
      "Loss: 1.67223 Accuracy: 0.381696 Step: 92\n",
      "Loss: 1.53783 Accuracy: 0.515625 Step: 93\n",
      "Loss: 1.63312 Accuracy: 0.433036 Step: 94\n",
      "Loss: 1.49214 Accuracy: 0.525 Step: 95\n",
      "Loss: 1.66457 Accuracy: 0.415179 Step: 96\n",
      "Loss: 1.61115 Accuracy: 0.428571 Step: 97\n",
      "Loss: 1.48739 Accuracy: 0.521875 Step: 98\n",
      "Loss: 1.65631 Accuracy: 0.368304 Step: 99\n",
      "Loss: 1.43237 Accuracy: 0.546875 Step: 100\n",
      "GT: [8 7 6 8 1]\n",
      "Predicted: [5 3 8 8 1]\n",
      "------\n",
      "GT: [4 3 9 9 1]\n",
      "Predicted: [5 6 6 9 1]\n",
      "------\n",
      "GT: [8 4 2 8 1]\n",
      "Predicted: [5 3 8 8 1]\n",
      "------\n",
      "Loss: 1.63014 Accuracy: 0.40625 Step: 101\n",
      "Loss: 1.44737 Accuracy: 0.515625 Step: 102\n",
      "Loss: 1.68832 Accuracy: 0.379464 Step: 103\n",
      "Loss: 1.46176 Accuracy: 0.4875 Step: 104\n",
      "Loss: 1.64056 Accuracy: 0.446429 Step: 105\n",
      "Loss: 1.43844 Accuracy: 0.5125 Step: 106\n",
      "Loss: 1.64376 Accuracy: 0.388393 Step: 107\n",
      "Loss: 1.40069 Accuracy: 0.5125 Step: 108\n",
      "Loss: 1.5998 Accuracy: 0.473214 Step: 109\n",
      "Loss: 1.45302 Accuracy: 0.54375 Step: 110\n",
      "Loss: 1.37162 Accuracy: 0.540625 Step: 111\n",
      "Loss: 1.61387 Accuracy: 0.444196 Step: 112\n",
      "Loss: 1.4039 Accuracy: 0.5125 Step: 113\n",
      "Loss: 1.58248 Accuracy: 0.410714 Step: 114\n",
      "Loss: 1.38628 Accuracy: 0.553125 Step: 115\n",
      "Loss: 1.59336 Accuracy: 0.466518 Step: 116\n",
      "Loss: 1.36905 Accuracy: 0.4875 Step: 117\n",
      "Loss: 1.56252 Accuracy: 0.46875 Step: 118\n",
      "Loss: 1.35408 Accuracy: 0.55 Step: 119\n",
      "Loss: 1.5633 Accuracy: 0.448661 Step: 120\n",
      "Loss: 1.37045 Accuracy: 0.521875 Step: 121\n",
      "Loss: 1.52958 Accuracy: 0.448661 Step: 122\n",
      "Loss: 1.34821 Accuracy: 0.559375 Step: 123\n",
      "Loss: 1.54858 Accuracy: 0.448661 Step: 124\n",
      "Loss: 1.39987 Accuracy: 0.5375 Step: 125\n",
      "Loss: 1.53184 Accuracy: 0.462054 Step: 126\n",
      "Loss: 1.35227 Accuracy: 0.559375 Step: 127\n",
      "Loss: 1.4894 Accuracy: 0.470982 Step: 128\n",
      "Loss: 1.33365 Accuracy: 0.553125 Step: 129\n",
      "Loss: 1.50192 Accuracy: 0.479911 Step: 130\n",
      "Loss: 1.3286 Accuracy: 0.56875 Step: 131\n",
      "Loss: 1.48556 Accuracy: 0.484375 Step: 132\n",
      "Loss: 1.32063 Accuracy: 0.565625 Step: 133\n",
      "Loss: 1.48033 Accuracy: 0.470982 Step: 134\n",
      "Loss: 1.30523 Accuracy: 0.60625 Step: 135\n",
      "Loss: 1.4878 Accuracy: 0.462054 Step: 136\n",
      "Loss: 1.33746 Accuracy: 0.584375 Step: 137\n",
      "Loss: 1.48675 Accuracy: 0.459821 Step: 138\n",
      "Loss: 1.25226 Accuracy: 0.56875 Step: 139\n",
      "Loss: 1.49969 Accuracy: 0.466518 Step: 140\n",
      "Loss: 1.25969 Accuracy: 0.590625 Step: 141\n",
      "Loss: 1.44885 Accuracy: 0.488839 Step: 142\n",
      "Loss: 1.2596 Accuracy: 0.565625 Step: 143\n",
      "Loss: 1.47242 Accuracy: 0.486607 Step: 144\n",
      "Loss: 1.24702 Accuracy: 0.61875 Step: 145\n",
      "Loss: 1.45993 Accuracy: 0.482143 Step: 146\n",
      "Loss: 1.2276 Accuracy: 0.603125 Step: 147\n",
      "Loss: 1.25243 Accuracy: 0.625 Step: 148\n",
      "Loss: 1.38556 Accuracy: 0.511161 Step: 149\n",
      "Loss: 1.41231 Accuracy: 0.502232 Step: 150\n",
      "Loss: 1.2233 Accuracy: 0.628125 Step: 151\n",
      "Loss: 1.41419 Accuracy: 0.491071 Step: 152\n",
      "Loss: 1.24362 Accuracy: 0.590625 Step: 153\n",
      "Loss: 1.40333 Accuracy: 0.515625 Step: 154\n",
      "Loss: 1.21745 Accuracy: 0.653125 Step: 155\n",
      "Loss: 1.38795 Accuracy: 0.517857 Step: 156\n",
      "Loss: 1.19991 Accuracy: 0.634375 Step: 157\n",
      "Loss: 1.41495 Accuracy: 0.506696 Step: 158\n",
      "Loss: 1.1704 Accuracy: 0.621875 Step: 159\n",
      "Loss: 1.37859 Accuracy: 0.504464 Step: 160\n",
      "Loss: 1.17985 Accuracy: 0.675 Step: 161\n",
      "Loss: 1.38974 Accuracy: 0.508929 Step: 162\n",
      "Loss: 1.19364 Accuracy: 0.6125 Step: 163\n",
      "Loss: 1.37337 Accuracy: 0.5 Step: 164\n",
      "Loss: 1.14269 Accuracy: 0.6375 Step: 165\n",
      "Loss: 1.39294 Accuracy: 0.517857 Step: 166\n",
      "Loss: 1.18393 Accuracy: 0.596875 Step: 167\n",
      "Loss: 1.36795 Accuracy: 0.515625 Step: 168\n",
      "Loss: 1.13776 Accuracy: 0.6625 Step: 169\n",
      "Loss: 1.36301 Accuracy: 0.526786 Step: 170\n",
      "Loss: 1.15877 Accuracy: 0.571875 Step: 171\n",
      "Loss: 1.36059 Accuracy: 0.540179 Step: 172\n",
      "Loss: 1.13136 Accuracy: 0.66875 Step: 173\n",
      "Loss: 1.35436 Accuracy: 0.506696 Step: 174\n",
      "Loss: 1.14075 Accuracy: 0.65 Step: 175\n",
      "Loss: 1.14889 Accuracy: 0.61875 Step: 176\n",
      "Loss: 1.35574 Accuracy: 0.513393 Step: 177\n",
      "Loss: 1.36005 Accuracy: 0.488839 Step: 178\n",
      "Loss: 1.18483 Accuracy: 0.625 Step: 179\n",
      "Loss: 1.42033 Accuracy: 0.5 Step: 180\n",
      "Loss: 1.14356 Accuracy: 0.6 Step: 181\n",
      "Loss: 1.43891 Accuracy: 0.520089 Step: 182\n",
      "Loss: 1.14251 Accuracy: 0.65625 Step: 183\n",
      "Loss: 1.4963 Accuracy: 0.450893 Step: 184\n",
      "Loss: 1.14106 Accuracy: 0.6625 Step: 185\n",
      "Loss: 1.74798 Accuracy: 0.366071 Step: 186\n",
      "Loss: 1.12969 Accuracy: 0.671875 Step: 187\n",
      "Loss: 2.08092 Accuracy: 0.412946 Step: 188\n",
      "Loss: 1.26961 Accuracy: 0.56875 Step: 189\n",
      "Loss: 1.84146 Accuracy: 0.397321 Step: 190\n",
      "Loss: 1.19772 Accuracy: 0.615625 Step: 191\n",
      "Loss: 1.28044 Accuracy: 0.59375 Step: 192\n",
      "Loss: 1.99069 Accuracy: 0.40625 Step: 193\n",
      "Loss: 1.25702 Accuracy: 0.58125 Step: 194\n",
      "Loss: 1.44369 Accuracy: 0.475446 Step: 195\n",
      "Loss: 1.7036 Accuracy: 0.40625 Step: 196\n",
      "Loss: 1.44894 Accuracy: 0.495536 Step: 197\n",
      "Loss: 1.19294 Accuracy: 0.60625 Step: 198\n",
      "Loss: 1.70931 Accuracy: 0.455357 Step: 199\n",
      "Loss: 1.24082 Accuracy: 0.63125 Step: 200\n",
      "GT: [3 7 5 2 1]\n",
      "Predicted: [2 5 5 2 1]\n",
      "------\n",
      "GT: [8 2 5 1 0]\n",
      "Predicted: [2 2 5 1 0]\n",
      "------\n",
      "GT: [6 4 4 3 1]\n",
      "Predicted: [2 4 4 3 1]\n",
      "------\n",
      "Loss: 1.55208 Accuracy: 0.488839 Step: 201\n",
      "Loss: 1.20919 Accuracy: 0.6375 Step: 202\n",
      "Loss: 1.48656 Accuracy: 0.46875 Step: 203\n",
      "Loss: 1.17891 Accuracy: 0.640625 Step: 204\n",
      "Loss: 1.50206 Accuracy: 0.464286 Step: 205\n",
      "Loss: 1.15208 Accuracy: 0.66875 Step: 206\n",
      "Loss: 1.38367 Accuracy: 0.506696 Step: 207\n",
      "Loss: 1.14953 Accuracy: 0.65 Step: 208\n",
      "Loss: 1.37668 Accuracy: 0.508929 Step: 209\n",
      "Loss: 1.12467 Accuracy: 0.65 Step: 210\n",
      "Loss: 1.33684 Accuracy: 0.555804 Step: 211\n",
      "Loss: 1.14351 Accuracy: 0.64375 Step: 212\n",
      "Loss: 1.35487 Accuracy: 0.542411 Step: 213\n",
      "Loss: 1.12176 Accuracy: 0.659375 Step: 214\n",
      "Loss: 1.3326 Accuracy: 0.558036 Step: 215\n",
      "Loss: 1.11038 Accuracy: 0.653125 Step: 216\n",
      "Loss: 1.29603 Accuracy: 0.542411 Step: 217\n",
      "Loss: 1.06846 Accuracy: 0.646875 Step: 218\n",
      "Loss: 1.32289 Accuracy: 0.535714 Step: 219\n",
      "Loss: 1.04405 Accuracy: 0.671875 Step: 220\n",
      "Loss: 1.29108 Accuracy: 0.551339 Step: 221\n",
      "Loss: 1.09258 Accuracy: 0.675 Step: 222\n",
      "Loss: 1.29753 Accuracy: 0.555804 Step: 223\n",
      "Loss: 1.25707 Accuracy: 0.544643 Step: 224\n",
      "Loss: 1.05139 Accuracy: 0.646875 Step: 225\n",
      "Loss: 1.09302 Accuracy: 0.65625 Step: 226\n",
      "Loss: 1.24447 Accuracy: 0.555804 Step: 227\n",
      "Loss: 1.07775 Accuracy: 0.70625 Step: 228\n",
      "Loss: 1.26698 Accuracy: 0.546875 Step: 229\n",
      "Loss: 1.07129 Accuracy: 0.659375 Step: 230\n",
      "Loss: 1.2438 Accuracy: 0.569196 Step: 231\n",
      "Loss: 1.07591 Accuracy: 0.671875 Step: 232\n",
      "Loss: 1.24358 Accuracy: 0.584821 Step: 233\n",
      "Loss: 1.22267 Accuracy: 0.582589 Step: 234\n",
      "Loss: 1.03565 Accuracy: 0.678125 Step: 235\n",
      "Loss: 1.23975 Accuracy: 0.575893 Step: 236\n",
      "Loss: 1.0252 Accuracy: 0.64375 Step: 237\n",
      "Loss: 1.05793 Accuracy: 0.665625 Step: 238\n",
      "Loss: 1.23901 Accuracy: 0.5625 Step: 239\n",
      "Loss: 0.966894 Accuracy: 0.696875 Step: 240\n",
      "Loss: 1.25624 Accuracy: 0.573661 Step: 241\n",
      "Loss: 1.20277 Accuracy: 0.575893 Step: 242\n",
      "Loss: 1.01707 Accuracy: 0.665625 Step: 243\n",
      "Loss: 1.28265 Accuracy: 0.53125 Step: 244\n",
      "Loss: 1.06245 Accuracy: 0.653125 Step: 245\n",
      "Loss: 1.47977 Accuracy: 0.470982 Step: 246\n",
      "Loss: 0.9933 Accuracy: 0.684375 Step: 247\n",
      "Loss: 1.44581 Accuracy: 0.486607 Step: 248\n",
      "Loss: 1.07737 Accuracy: 0.684375 Step: 249\n",
      "Loss: 1.22065 Accuracy: 0.564732 Step: 250\n",
      "Loss: 1.02904 Accuracy: 0.69375 Step: 251\n",
      "Loss: 1.33997 Accuracy: 0.533482 Step: 252\n",
      "Loss: 1.0632 Accuracy: 0.66875 Step: 253\n",
      "Loss: 1.23096 Accuracy: 0.569196 Step: 254\n",
      "Loss: 1.02114 Accuracy: 0.68125 Step: 255\n",
      "Loss: 1.30666 Accuracy: 0.524554 Step: 256\n",
      "Loss: 1.00284 Accuracy: 0.634375 Step: 257\n",
      "Loss: 1.22065 Accuracy: 0.5625 Step: 258\n",
      "Loss: 1.18606 Accuracy: 0.600446 Step: 259\n",
      "Loss: 1.02679 Accuracy: 0.684375 Step: 260\n",
      "Loss: 1.00883 Accuracy: 0.6875 Step: 261\n",
      "Loss: 1.30077 Accuracy: 0.542411 Step: 262\n",
      "Loss: 1.18505 Accuracy: 0.595982 Step: 263\n",
      "Loss: 1.02809 Accuracy: 0.671875 Step: 264\n",
      "Loss: 1.06102 Accuracy: 0.671875 Step: 265\n",
      "Loss: 1.31725 Accuracy: 0.580357 Step: 266\n",
      "Loss: 0.99036 Accuracy: 0.65625 Step: 267\n",
      "Loss: 1.22134 Accuracy: 0.571429 Step: 268\n",
      "Loss: 0.942195 Accuracy: 0.728125 Step: 269\n",
      "Loss: 1.18486 Accuracy: 0.580357 Step: 270\n",
      "Loss: 1.02507 Accuracy: 0.684375 Step: 271\n",
      "Loss: 1.22283 Accuracy: 0.573661 Step: 272\n",
      "Loss: 1.00604 Accuracy: 0.696875 Step: 273\n",
      "Loss: 1.11419 Accuracy: 0.631696 Step: 274\n",
      "Loss: 0.95566 Accuracy: 0.69375 Step: 275\n",
      "Loss: 1.19896 Accuracy: 0.580357 Step: 276\n",
      "Loss: 0.971483 Accuracy: 0.696875 Step: 277\n",
      "Loss: 1.17911 Accuracy: 0.580357 Step: 278\n",
      "Loss: 0.924102 Accuracy: 0.70625 Step: 279\n",
      "Loss: 1.1561 Accuracy: 0.602679 Step: 280\n",
      "Loss: 0.956112 Accuracy: 0.678125 Step: 281\n",
      "Loss: 1.15402 Accuracy: 0.589286 Step: 282\n",
      "Loss: 0.974761 Accuracy: 0.709375 Step: 283\n",
      "Loss: 1.18455 Accuracy: 0.571429 Step: 284\n",
      "Loss: 0.929571 Accuracy: 0.71875 Step: 285\n",
      "Loss: 1.11199 Accuracy: 0.616071 Step: 286\n",
      "Loss: 0.951042 Accuracy: 0.69375 Step: 287\n",
      "Loss: 1.13445 Accuracy: 0.613839 Step: 288\n",
      "Loss: 0.915688 Accuracy: 0.721875 Step: 289\n",
      "Loss: 1.12041 Accuracy: 0.618304 Step: 290\n",
      "Loss: 0.951666 Accuracy: 0.725 Step: 291\n",
      "Loss: 1.1048 Accuracy: 0.631696 Step: 292\n",
      "Loss: 0.958369 Accuracy: 0.746875 Step: 293\n",
      "Loss: 0.927795 Accuracy: 0.721875 Step: 294\n",
      "Loss: 1.15023 Accuracy: 0.622768 Step: 295\n",
      "Loss: 1.09549 Accuracy: 0.640625 Step: 296\n",
      "Loss: 0.95493 Accuracy: 0.721875 Step: 297\n",
      "Loss: 1.17502 Accuracy: 0.598214 Step: 298\n",
      "Loss: 0.894208 Accuracy: 0.709375 Step: 299\n",
      "Loss: 0.932548 Accuracy: 0.715625 Step: 300\n",
      "GT: [6 9 3 7 1]\n",
      "Predicted: [9 3 3 7 1]\n",
      "------\n",
      "GT: [6 7 7 1 0]\n",
      "Predicted: [9 7 7 1 0]\n",
      "------\n",
      "GT: [9 3 2 1 0]\n",
      "Predicted: [9 3 2 1 0]\n",
      "------\n",
      "Loss: 1.40435 Accuracy: 0.553571 Step: 301\n",
      "Loss: 0.963422 Accuracy: 0.725 Step: 302\n",
      "Loss: 1.7387 Accuracy: 0.497768 Step: 303\n",
      "Loss: 1.09898 Accuracy: 0.65 Step: 304\n",
      "Loss: 1.63232 Accuracy: 0.520089 Step: 305\n",
      "Loss: 0.959252 Accuracy: 0.73125 Step: 306\n",
      "Loss: 1.25422 Accuracy: 0.569196 Step: 307\n",
      "Loss: 1.28915 Accuracy: 0.559375 Step: 308\n",
      "Loss: 1.25114 Accuracy: 0.580357 Step: 309\n",
      "Loss: 1.21211 Accuracy: 0.59375 Step: 310\n",
      "Loss: 1.07332 Accuracy: 0.675 Step: 311\n",
      "Loss: 0.962029 Accuracy: 0.71875 Step: 312\n",
      "Loss: 1.23096 Accuracy: 0.578125 Step: 313\n",
      "Loss: 0.957152 Accuracy: 0.70625 Step: 314\n",
      "Loss: 1.32956 Accuracy: 0.560268 Step: 315\n",
      "Loss: 0.968182 Accuracy: 0.725 Step: 316\n",
      "Loss: 1.26223 Accuracy: 0.582589 Step: 317\n",
      "Loss: 0.971912 Accuracy: 0.715625 Step: 318\n",
      "Loss: 1.19188 Accuracy: 0.595982 Step: 319\n",
      "Loss: 0.907041 Accuracy: 0.70625 Step: 320\n",
      "Loss: 1.27263 Accuracy: 0.566964 Step: 321\n",
      "Loss: 0.971918 Accuracy: 0.678125 Step: 322\n",
      "Loss: 1.1762 Accuracy: 0.587054 Step: 323\n",
      "Loss: 0.901517 Accuracy: 0.74375 Step: 324\n",
      "Loss: 1.20764 Accuracy: 0.616071 Step: 325\n",
      "Loss: 0.941746 Accuracy: 0.7125 Step: 326\n",
      "Loss: 1.09046 Accuracy: 0.627232 Step: 327\n",
      "Loss: 0.909886 Accuracy: 0.721875 Step: 328\n",
      "Loss: 1.14875 Accuracy: 0.616071 Step: 329\n",
      "Loss: 0.890461 Accuracy: 0.728125 Step: 330\n",
      "Loss: 1.08627 Accuracy: 0.622768 Step: 331\n",
      "Loss: 0.902234 Accuracy: 0.74375 Step: 332\n",
      "Loss: 1.15341 Accuracy: 0.616071 Step: 333\n",
      "Loss: 0.929227 Accuracy: 0.703125 Step: 334\n",
      "Loss: 1.08496 Accuracy: 0.616071 Step: 335\n",
      "Loss: 0.907763 Accuracy: 0.721875 Step: 336\n",
      "Loss: 1.09873 Accuracy: 0.613839 Step: 337\n",
      "Loss: 0.863262 Accuracy: 0.721875 Step: 338\n",
      "Loss: 1.10949 Accuracy: 0.627232 Step: 339\n",
      "Loss: 1.03748 Accuracy: 0.649554 Step: 340\n",
      "Loss: 0.937397 Accuracy: 0.75 Step: 341\n",
      "Loss: 1.10786 Accuracy: 0.622768 Step: 342\n",
      "Loss: 0.881042 Accuracy: 0.753125 Step: 343\n",
      "Loss: 0.885371 Accuracy: 0.74375 Step: 344\n",
      "Loss: 1.07944 Accuracy: 0.620536 Step: 345\n",
      "Loss: 0.878839 Accuracy: 0.73125 Step: 346\n",
      "Loss: 1.0282 Accuracy: 0.642857 Step: 347\n",
      "Loss: 0.893084 Accuracy: 0.75 Step: 348\n",
      "Loss: 1.02154 Accuracy: 0.654018 Step: 349\n",
      "Loss: 0.853482 Accuracy: 0.75 Step: 350\n",
      "Loss: 1.07634 Accuracy: 0.631696 Step: 351\n",
      "Loss: 0.815015 Accuracy: 0.753125 Step: 352\n",
      "Loss: 1.00228 Accuracy: 0.680804 Step: 353\n",
      "Loss: 0.818797 Accuracy: 0.76875 Step: 354\n",
      "Loss: 1.05151 Accuracy: 0.640625 Step: 355\n",
      "Loss: 0.883891 Accuracy: 0.74375 Step: 356\n",
      "Loss: 1.15566 Accuracy: 0.589286 Step: 357\n",
      "Loss: 0.849287 Accuracy: 0.771875 Step: 358\n",
      "Loss: 0.899596 Accuracy: 0.728125 Step: 359\n",
      "Loss: 1.38546 Accuracy: 0.502232 Step: 360\n",
      "Loss: 1.05727 Accuracy: 0.625 Step: 361\n",
      "Loss: 0.945432 Accuracy: 0.728125 Step: 362\n",
      "Loss: 1.33015 Accuracy: 0.544643 Step: 363\n",
      "Loss: 0.893053 Accuracy: 0.759375 Step: 364\n",
      "Loss: 1.14672 Accuracy: 0.600446 Step: 365\n",
      "Loss: 0.873574 Accuracy: 0.721875 Step: 366\n",
      "Loss: 1.35715 Accuracy: 0.549107 Step: 367\n",
      "Loss: 0.89413 Accuracy: 0.740625 Step: 368\n",
      "Loss: 1.09396 Accuracy: 0.631696 Step: 369\n",
      "Loss: 0.883404 Accuracy: 0.76875 Step: 370\n",
      "Loss: 1.12145 Accuracy: 0.595982 Step: 371\n",
      "Loss: 0.878456 Accuracy: 0.753125 Step: 372\n",
      "Loss: 1.13569 Accuracy: 0.625 Step: 373\n",
      "Loss: 0.860457 Accuracy: 0.721875 Step: 374\n",
      "Loss: 1.02949 Accuracy: 0.674107 Step: 375\n",
      "Loss: 0.824959 Accuracy: 0.75625 Step: 376\n",
      "Loss: 1.09058 Accuracy: 0.642857 Step: 377\n",
      "Loss: 0.855555 Accuracy: 0.778125 Step: 378\n",
      "Loss: 1.06572 Accuracy: 0.667411 Step: 379\n",
      "Loss: 0.850272 Accuracy: 0.740625 Step: 380\n",
      "Loss: 1.00713 Accuracy: 0.654018 Step: 381\n",
      "Loss: 0.860975 Accuracy: 0.78125 Step: 382\n",
      "Loss: 1.02012 Accuracy: 0.667411 Step: 383\n",
      "Loss: 0.860406 Accuracy: 0.771875 Step: 384\n",
      "Loss: 0.848227 Accuracy: 0.746875 Step: 385\n",
      "Loss: 1.08706 Accuracy: 0.633929 Step: 386\n",
      "Loss: 1.00616 Accuracy: 0.651786 Step: 387\n",
      "Loss: 0.859645 Accuracy: 0.75625 Step: 388\n",
      "Loss: 1.07596 Accuracy: 0.640625 Step: 389\n",
      "Loss: 0.865293 Accuracy: 0.759375 Step: 390\n",
      "Loss: 1.1152 Accuracy: 0.616071 Step: 391\n",
      "Loss: 0.80476 Accuracy: 0.725 Step: 392\n",
      "Loss: 0.942906 Accuracy: 0.694196 Step: 393\n",
      "Loss: 0.835281 Accuracy: 0.76875 Step: 394\n",
      "Loss: 1.05838 Accuracy: 0.669643 Step: 395\n",
      "Loss: 0.815223 Accuracy: 0.784375 Step: 396\n",
      "Loss: 1.14627 Accuracy: 0.609375 Step: 397\n",
      "Loss: 0.835669 Accuracy: 0.765625 Step: 398\n",
      "Loss: 0.94503 Accuracy: 0.671875 Step: 399\n",
      "Loss: 0.85427 Accuracy: 0.746875 Step: 400\n",
      "GT: [5 2 6 5 1]\n",
      "Predicted: [9 2 6 5 1]\n",
      "------\n",
      "GT: [2 2 5 3 1]\n",
      "Predicted: [9 2 5 3 1]\n",
      "------\n",
      "GT: [6 5 6 1 0]\n",
      "Predicted: [9 5 6 1 0]\n",
      "------\n",
      "Loss: 1.01667 Accuracy: 0.640625 Step: 401\n",
      "Loss: 0.799999 Accuracy: 0.775 Step: 402\n",
      "Loss: 1.04928 Accuracy: 0.633929 Step: 403\n",
      "Loss: 0.812555 Accuracy: 0.775 Step: 404\n",
      "Loss: 0.931216 Accuracy: 0.662946 Step: 405\n",
      "Loss: 0.83675 Accuracy: 0.796875 Step: 406\n",
      "Loss: 0.98155 Accuracy: 0.654018 Step: 407\n",
      "Loss: 0.828762 Accuracy: 0.775 Step: 408\n",
      "Loss: 0.94193 Accuracy: 0.696429 Step: 409\n",
      "Loss: 0.907829 Accuracy: 0.709821 Step: 410\n",
      "Loss: 0.788718 Accuracy: 0.775 Step: 411\n",
      "Loss: 0.972401 Accuracy: 0.654018 Step: 412\n",
      "Loss: 0.754163 Accuracy: 0.784375 Step: 413\n",
      "Loss: 0.957472 Accuracy: 0.647321 Step: 414\n",
      "Loss: 0.773574 Accuracy: 0.78125 Step: 415\n",
      "Loss: 0.927549 Accuracy: 0.689732 Step: 416\n",
      "Loss: 0.804394 Accuracy: 0.76875 Step: 417\n",
      "Loss: 0.915537 Accuracy: 0.700893 Step: 418\n",
      "Loss: 0.757027 Accuracy: 0.78125 Step: 419\n",
      "Loss: 0.909989 Accuracy: 0.676339 Step: 420\n",
      "Loss: 0.757121 Accuracy: 0.80625 Step: 421\n",
      "Loss: 1.02546 Accuracy: 0.669643 Step: 422\n",
      "Loss: 0.745809 Accuracy: 0.784375 Step: 423\n",
      "Loss: 0.811133 Accuracy: 0.734375 Step: 424\n",
      "Loss: 1.23292 Accuracy: 0.598214 Step: 425\n",
      "Loss: 0.819427 Accuracy: 0.74375 Step: 426\n",
      "Loss: 1.45611 Accuracy: 0.566964 Step: 427\n",
      "Loss: 0.869261 Accuracy: 0.753125 Step: 428\n",
      "Loss: 0.964734 Accuracy: 0.683036 Step: 429\n",
      "Loss: 0.841255 Accuracy: 0.71875 Step: 430\n",
      "Loss: 1.0615 Accuracy: 0.651786 Step: 431\n",
      "Loss: 0.865108 Accuracy: 0.771875 Step: 432\n",
      "Loss: 0.986086 Accuracy: 0.671875 Step: 433\n",
      "Loss: 0.736847 Accuracy: 0.778125 Step: 434\n",
      "Loss: 1.15054 Accuracy: 0.618304 Step: 435\n",
      "Loss: 0.811835 Accuracy: 0.75 Step: 436\n",
      "Loss: 1.11964 Accuracy: 0.602679 Step: 437\n",
      "Loss: 0.817877 Accuracy: 0.76875 Step: 438\n",
      "Loss: 1.03538 Accuracy: 0.65625 Step: 439\n",
      "Loss: 0.802133 Accuracy: 0.76875 Step: 440\n",
      "Loss: 1.08295 Accuracy: 0.622768 Step: 441\n",
      "Loss: 0.788623 Accuracy: 0.76875 Step: 442\n",
      "Loss: 0.931648 Accuracy: 0.683036 Step: 443\n",
      "Loss: 0.775407 Accuracy: 0.78125 Step: 444\n",
      "Loss: 1.03121 Accuracy: 0.638393 Step: 445\n",
      "Loss: 0.776768 Accuracy: 0.778125 Step: 446\n",
      "Loss: 1.03459 Accuracy: 0.649554 Step: 447\n",
      "Loss: 0.810968 Accuracy: 0.80625 Step: 448\n",
      "Loss: 0.957644 Accuracy: 0.674107 Step: 449\n",
      "Loss: 0.850412 Accuracy: 0.7625 Step: 450\n",
      "Loss: 1.00092 Accuracy: 0.698661 Step: 451\n",
      "Loss: 0.779626 Accuracy: 0.771875 Step: 452\n",
      "Loss: 1.15662 Accuracy: 0.642857 Step: 453\n",
      "Loss: 0.761971 Accuracy: 0.778125 Step: 454\n",
      "Loss: 0.956406 Accuracy: 0.680804 Step: 455\n",
      "Loss: 0.76076 Accuracy: 0.784375 Step: 456\n",
      "Loss: 0.991372 Accuracy: 0.678571 Step: 457\n",
      "Loss: 0.819631 Accuracy: 0.78125 Step: 458\n",
      "Loss: 0.929041 Accuracy: 0.685268 Step: 459\n",
      "Loss: 0.754945 Accuracy: 0.790625 Step: 460\n",
      "Loss: 0.79545 Accuracy: 0.784375 Step: 461\n",
      "Loss: 1.08077 Accuracy: 0.633929 Step: 462\n",
      "Loss: 0.953049 Accuracy: 0.691964 Step: 463\n",
      "Loss: 0.786474 Accuracy: 0.79375 Step: 464\n",
      "Loss: 0.912523 Accuracy: 0.712054 Step: 465\n",
      "Loss: 0.832445 Accuracy: 0.78125 Step: 466\n",
      "Loss: 0.904384 Accuracy: 0.71875 Step: 467\n",
      "Loss: 0.778784 Accuracy: 0.790625 Step: 468\n",
      "Loss: 0.972674 Accuracy: 0.654018 Step: 469\n",
      "Loss: 0.729308 Accuracy: 0.834375 Step: 470\n",
      "Loss: 0.848354 Accuracy: 0.743304 Step: 471\n",
      "Loss: 0.744736 Accuracy: 0.784375 Step: 472\n",
      "Loss: 0.864814 Accuracy: 0.729911 Step: 473\n",
      "Loss: 0.771051 Accuracy: 0.796875 Step: 474\n",
      "Loss: 0.981647 Accuracy: 0.680804 Step: 475\n",
      "Loss: 0.721268 Accuracy: 0.809375 Step: 476\n",
      "Loss: 1.11372 Accuracy: 0.613839 Step: 477\n",
      "Loss: 0.736607 Accuracy: 0.765625 Step: 478\n",
      "Loss: 1.0011 Accuracy: 0.665179 Step: 479\n",
      "Loss: 0.789619 Accuracy: 0.76875 Step: 480\n",
      "Loss: 0.982647 Accuracy: 0.678571 Step: 481\n",
      "Loss: 0.701502 Accuracy: 0.8 Step: 482\n",
      "Loss: 0.995859 Accuracy: 0.65625 Step: 483\n",
      "Loss: 0.7375 Accuracy: 0.809375 Step: 484\n",
      "Loss: 0.874531 Accuracy: 0.747768 Step: 485\n",
      "Loss: 0.722936 Accuracy: 0.79375 Step: 486\n",
      "Loss: 0.947315 Accuracy: 0.689732 Step: 487\n",
      "Loss: 0.752107 Accuracy: 0.784375 Step: 488\n",
      "Loss: 0.734256 Accuracy: 0.775 Step: 489\n",
      "Loss: 1.02355 Accuracy: 0.640625 Step: 490\n",
      "Loss: 0.847148 Accuracy: 0.709821 Step: 491\n",
      "Loss: 0.765356 Accuracy: 0.76875 Step: 492\n",
      "Loss: 0.972157 Accuracy: 0.647321 Step: 493\n",
      "Loss: 0.718207 Accuracy: 0.7875 Step: 494\n",
      "Loss: 1.02322 Accuracy: 0.669643 Step: 495\n",
      "Loss: 0.805141 Accuracy: 0.784375 Step: 496\n",
      "Loss: 0.951329 Accuracy: 0.674107 Step: 497\n",
      "Loss: 0.747376 Accuracy: 0.803125 Step: 498\n",
      "Loss: 1.16589 Accuracy: 0.59375 Step: 499\n",
      "Loss: 0.76759 Accuracy: 0.7875 Step: 500\n",
      "GT: [6 5 3 1 0]\n",
      "Predicted: [5 5 3 1 0]\n",
      "------\n",
      "GT: [6 2 8 5 1]\n",
      "Predicted: [5 2 8 5 1]\n",
      "------\n",
      "GT: [7 4 5 1 0]\n",
      "Predicted: [5 4 5 1 0]\n",
      "------\n",
      "Loss: 0.915067 Accuracy: 0.6875 Step: 501\n",
      "Loss: 0.752594 Accuracy: 0.76875 Step: 502\n",
      "Loss: 0.955003 Accuracy: 0.667411 Step: 503\n",
      "Loss: 0.764615 Accuracy: 0.76875 Step: 504\n",
      "Loss: 0.745777 Accuracy: 0.79375 Step: 505\n",
      "Loss: 1.03315 Accuracy: 0.627232 Step: 506\n",
      "Loss: 0.726774 Accuracy: 0.765625 Step: 507\n",
      "Loss: 0.880908 Accuracy: 0.689732 Step: 508\n",
      "Loss: 0.7151 Accuracy: 0.784375 Step: 509\n",
      "Loss: 0.978526 Accuracy: 0.669643 Step: 510\n",
      "Loss: 0.717522 Accuracy: 0.778125 Step: 511\n",
      "Loss: 0.963347 Accuracy: 0.676339 Step: 512\n",
      "Loss: 0.763666 Accuracy: 0.80625 Step: 513\n",
      "Loss: 0.873345 Accuracy: 0.725446 Step: 514\n",
      "Loss: 0.736144 Accuracy: 0.809375 Step: 515\n",
      "Loss: 0.98982 Accuracy: 0.6875 Step: 516\n",
      "Loss: 0.735384 Accuracy: 0.8 Step: 517\n",
      "Loss: 0.852011 Accuracy: 0.738839 Step: 518\n",
      "Loss: 0.738359 Accuracy: 0.8 Step: 519\n",
      "Loss: 0.878368 Accuracy: 0.662946 Step: 520\n",
      "Loss: 0.711089 Accuracy: 0.79375 Step: 521\n",
      "Loss: 0.890614 Accuracy: 0.698661 Step: 522\n",
      "Loss: 0.713529 Accuracy: 0.8 Step: 523\n",
      "Loss: 0.868465 Accuracy: 0.709821 Step: 524\n",
      "Loss: 0.720168 Accuracy: 0.784375 Step: 525\n",
      "Loss: 0.805759 Accuracy: 0.736607 Step: 526\n",
      "Loss: 0.710908 Accuracy: 0.821875 Step: 527\n",
      "Loss: 0.844524 Accuracy: 0.714286 Step: 528\n",
      "Loss: 0.722062 Accuracy: 0.79375 Step: 529\n",
      "Loss: 0.917042 Accuracy: 0.680804 Step: 530\n",
      "Loss: 0.661602 Accuracy: 0.8 Step: 531\n",
      "Loss: 0.795771 Accuracy: 0.747768 Step: 532\n",
      "Loss: 0.710031 Accuracy: 0.790625 Step: 533\n",
      "Loss: 0.865761 Accuracy: 0.712054 Step: 534\n",
      "Loss: 0.704992 Accuracy: 0.83125 Step: 535\n",
      "Loss: 0.770839 Accuracy: 0.745536 Step: 536\n",
      "Loss: 0.720041 Accuracy: 0.825 Step: 537\n",
      "Loss: 0.783261 Accuracy: 0.734375 Step: 538\n",
      "Loss: 0.701967 Accuracy: 0.8 Step: 539\n",
      "Loss: 0.834778 Accuracy: 0.75 Step: 540\n",
      "Loss: 0.71722 Accuracy: 0.840625 Step: 541\n",
      "Loss: 0.863505 Accuracy: 0.707589 Step: 542\n",
      "Loss: 0.687148 Accuracy: 0.815625 Step: 543\n",
      "Loss: 0.782925 Accuracy: 0.763393 Step: 544\n",
      "Loss: 0.752366 Accuracy: 0.803125 Step: 545\n",
      "Loss: 0.757769 Accuracy: 0.767857 Step: 546\n",
      "Loss: 0.730796 Accuracy: 0.761161 Step: 547\n",
      "Loss: 0.658679 Accuracy: 0.803125 Step: 548\n",
      "Loss: 0.836109 Accuracy: 0.71875 Step: 549\n",
      "Loss: 0.685259 Accuracy: 0.8 Step: 550\n",
      "Loss: 0.698118 Accuracy: 0.80625 Step: 551\n",
      "Loss: 1.15102 Accuracy: 0.660714 Step: 552\n",
      "Loss: 0.879876 Accuracy: 0.705357 Step: 553\n",
      "Loss: 0.731172 Accuracy: 0.79375 Step: 554\n",
      "Loss: 1.24791 Accuracy: 0.622768 Step: 555\n",
      "Loss: 0.684819 Accuracy: 0.803125 Step: 556\n",
      "Loss: 0.902908 Accuracy: 0.707589 Step: 557\n",
      "Loss: 0.78868 Accuracy: 0.79375 Step: 558\n",
      "Loss: 1.09083 Accuracy: 0.625 Step: 559\n",
      "Loss: 0.716097 Accuracy: 0.796875 Step: 560\n",
      "Loss: 0.952764 Accuracy: 0.667411 Step: 561\n",
      "Loss: 0.754332 Accuracy: 0.796875 Step: 562\n",
      "Loss: 0.922455 Accuracy: 0.698661 Step: 563\n",
      "Loss: 0.755345 Accuracy: 0.8 Step: 564\n",
      "Loss: 0.843973 Accuracy: 0.71875 Step: 565\n",
      "Loss: 0.737918 Accuracy: 0.79375 Step: 566\n",
      "Loss: 0.845548 Accuracy: 0.698661 Step: 567\n",
      "Loss: 0.692702 Accuracy: 0.79375 Step: 568\n",
      "Loss: 0.806585 Accuracy: 0.720982 Step: 569\n",
      "Loss: 0.869727 Accuracy: 0.6875 Step: 570\n",
      "Loss: 0.699597 Accuracy: 0.8 Step: 571\n",
      "Loss: 0.693025 Accuracy: 0.8 Step: 572\n",
      "Loss: 0.770152 Accuracy: 0.747768 Step: 573\n",
      "Loss: 0.678719 Accuracy: 0.821875 Step: 574\n",
      "Loss: 0.79154 Accuracy: 0.738839 Step: 575\n",
      "Loss: 0.67447 Accuracy: 0.809375 Step: 576\n",
      "Loss: 0.774206 Accuracy: 0.752232 Step: 577\n",
      "Loss: 0.666535 Accuracy: 0.825 Step: 578\n",
      "Loss: 0.753117 Accuracy: 0.779018 Step: 579\n",
      "Loss: 0.650686 Accuracy: 0.815625 Step: 580\n",
      "Loss: 0.760299 Accuracy: 0.756696 Step: 581\n",
      "Loss: 0.646946 Accuracy: 0.790625 Step: 582\n",
      "Loss: 0.758542 Accuracy: 0.756696 Step: 583\n",
      "Loss: 0.681624 Accuracy: 0.809375 Step: 584\n",
      "Loss: 0.715524 Accuracy: 0.779018 Step: 585\n",
      "Loss: 0.636961 Accuracy: 0.803125 Step: 586\n",
      "Loss: 0.723333 Accuracy: 0.785714 Step: 587\n",
      "Loss: 0.668619 Accuracy: 0.80625 Step: 588\n",
      "Loss: 0.725773 Accuracy: 0.772321 Step: 589\n",
      "Loss: 0.654622 Accuracy: 0.815625 Step: 590\n",
      "Loss: 0.73019 Accuracy: 0.738839 Step: 591\n",
      "Loss: 0.645544 Accuracy: 0.80625 Step: 592\n",
      "Loss: 0.728868 Accuracy: 0.765625 Step: 593\n",
      "Loss: 0.659259 Accuracy: 0.828125 Step: 594\n",
      "Loss: 0.657105 Accuracy: 0.815625 Step: 595\n",
      "Loss: 0.750462 Accuracy: 0.783482 Step: 596\n",
      "Loss: 0.682576 Accuracy: 0.774554 Step: 597\n",
      "Loss: 0.643638 Accuracy: 0.815625 Step: 598\n",
      "Loss: 0.725745 Accuracy: 0.765625 Step: 599\n",
      "Loss: 0.648681 Accuracy: 0.809375 Step: 600\n",
      "GT: [5 3 2 1 0]\n",
      "Predicted: [8 3 2 1 0]\n",
      "------\n",
      "GT: [4 3 4 7 1]\n",
      "Predicted: [8 3 4 7 1]\n",
      "------\n",
      "GT: [2 6 8 5 1]\n",
      "Predicted: [8 6 8 5 1]\n",
      "------\n",
      "Loss: 0.669292 Accuracy: 0.787946 Step: 601\n",
      "Loss: 0.652229 Accuracy: 0.80625 Step: 602\n",
      "Loss: 0.792752 Accuracy: 0.729911 Step: 603\n",
      "Loss: 0.677798 Accuracy: 0.81875 Step: 604\n",
      "Loss: 0.767513 Accuracy: 0.754464 Step: 605\n",
      "Loss: 0.696095 Accuracy: 0.840625 Step: 606\n",
      "Loss: 0.669734 Accuracy: 0.816964 Step: 607\n",
      "Loss: 0.639874 Accuracy: 0.809375 Step: 608\n",
      "Loss: 0.731531 Accuracy: 0.772321 Step: 609\n",
      "Loss: 0.607646 Accuracy: 0.821875 Step: 610\n",
      "Loss: 0.632845 Accuracy: 0.81875 Step: 611\n",
      "Loss: 0.770201 Accuracy: 0.745536 Step: 612\n",
      "Loss: 0.68823 Accuracy: 0.8125 Step: 613\n",
      "Loss: 0.898989 Accuracy: 0.734375 Step: 614\n",
      "Loss: 0.651991 Accuracy: 0.821875 Step: 615\n",
      "Loss: 0.832841 Accuracy: 0.741071 Step: 616\n",
      "Loss: 0.679835 Accuracy: 0.828125 Step: 617\n",
      "Loss: 0.910202 Accuracy: 0.714286 Step: 618\n",
      "Loss: 0.681183 Accuracy: 0.828125 Step: 619\n",
      "Loss: 0.999667 Accuracy: 0.680804 Step: 620\n",
      "Loss: 0.673492 Accuracy: 0.825 Step: 621\n",
      "Loss: 0.928028 Accuracy: 0.694196 Step: 622\n",
      "Loss: 0.669477 Accuracy: 0.78125 Step: 623\n",
      "Loss: 0.706378 Accuracy: 0.815625 Step: 624\n",
      "Loss: 0.723733 Accuracy: 0.809375 Step: 625\n",
      "Loss: 1.14384 Accuracy: 0.649554 Step: 626\n",
      "Loss: 0.837419 Accuracy: 0.738839 Step: 627\n",
      "Loss: 0.661131 Accuracy: 0.80625 Step: 628\n",
      "Loss: 0.750975 Accuracy: 0.775 Step: 629\n",
      "Loss: 1.17799 Accuracy: 0.667411 Step: 630\n",
      "Loss: 0.720329 Accuracy: 0.78125 Step: 631\n",
      "Loss: 0.890613 Accuracy: 0.714286 Step: 632\n",
      "Loss: 0.66349 Accuracy: 0.803125 Step: 633\n",
      "Loss: 0.795519 Accuracy: 0.732143 Step: 634\n",
      "Loss: 0.790709 Accuracy: 0.759375 Step: 635\n",
      "Loss: 0.84311 Accuracy: 0.732143 Step: 636\n",
      "Loss: 0.77021 Accuracy: 0.775 Step: 637\n",
      "Loss: 0.790985 Accuracy: 0.741071 Step: 638\n",
      "Loss: 0.662159 Accuracy: 0.81875 Step: 639\n",
      "Loss: 0.782591 Accuracy: 0.770089 Step: 640\n",
      "Loss: 0.67865 Accuracy: 0.8 Step: 641\n",
      "Loss: 0.862778 Accuracy: 0.736607 Step: 642\n",
      "Loss: 0.649323 Accuracy: 0.8 Step: 643\n",
      "Loss: 0.748224 Accuracy: 0.758929 Step: 644\n",
      "Loss: 0.677543 Accuracy: 0.809375 Step: 645\n",
      "Loss: 0.706132 Accuracy: 0.770089 Step: 646\n",
      "Loss: 0.664381 Accuracy: 0.79375 Step: 647\n",
      "Loss: 0.786462 Accuracy: 0.765625 Step: 648\n",
      "Loss: 0.654371 Accuracy: 0.809375 Step: 649\n",
      "Loss: 0.793718 Accuracy: 0.756696 Step: 650\n",
      "Loss: 0.654292 Accuracy: 0.815625 Step: 651\n",
      "Loss: 0.741054 Accuracy: 0.772321 Step: 652\n",
      "Loss: 0.699081 Accuracy: 0.774554 Step: 653\n",
      "Loss: 0.711081 Accuracy: 0.840625 Step: 654\n",
      "Loss: 0.712024 Accuracy: 0.79375 Step: 655\n",
      "Loss: 0.812863 Accuracy: 0.741071 Step: 656\n",
      "Loss: 0.646775 Accuracy: 0.8 Step: 657\n",
      "Loss: 0.739256 Accuracy: 0.752232 Step: 658\n",
      "Loss: 0.657406 Accuracy: 0.809375 Step: 659\n",
      "Loss: 0.78629 Accuracy: 0.723214 Step: 660\n",
      "Loss: 0.661437 Accuracy: 0.81875 Step: 661\n",
      "Loss: 0.782754 Accuracy: 0.738839 Step: 662\n",
      "Loss: 0.646598 Accuracy: 0.796875 Step: 663\n",
      "Loss: 0.718985 Accuracy: 0.776786 Step: 664\n",
      "Loss: 0.626341 Accuracy: 0.80625 Step: 665\n",
      "Loss: 0.687534 Accuracy: 0.787946 Step: 666\n",
      "Loss: 0.655206 Accuracy: 0.825 Step: 667\n",
      "Loss: 0.655631 Accuracy: 0.808036 Step: 668\n",
      "Loss: 0.663366 Accuracy: 0.81875 Step: 669\n",
      "Loss: 0.644173 Accuracy: 0.8125 Step: 670\n",
      "Loss: 0.708299 Accuracy: 0.774554 Step: 671\n",
      "Loss: 0.657546 Accuracy: 0.821875 Step: 672\n",
      "Loss: 0.672746 Accuracy: 0.779018 Step: 673\n",
      "Loss: 0.666868 Accuracy: 0.790179 Step: 674\n",
      "Loss: 0.664297 Accuracy: 0.821875 Step: 675\n",
      "Loss: 0.633394 Accuracy: 0.794643 Step: 676\n",
      "Loss: 0.668333 Accuracy: 0.8125 Step: 677\n",
      "Loss: 0.637367 Accuracy: 0.808036 Step: 678\n",
      "Loss: 0.655531 Accuracy: 0.8125 Step: 679\n",
      "Loss: 0.707645 Accuracy: 0.756696 Step: 680\n",
      "Loss: 0.640788 Accuracy: 0.81875 Step: 681\n",
      "Loss: 0.641482 Accuracy: 0.8125 Step: 682\n",
      "Loss: 0.63658 Accuracy: 0.8125 Step: 683\n",
      "Loss: 0.640416 Accuracy: 0.772321 Step: 684\n",
      "Loss: 0.630485 Accuracy: 0.83125 Step: 685\n",
      "Loss: 0.690935 Accuracy: 0.767857 Step: 686\n",
      "Loss: 0.615872 Accuracy: 0.8125 Step: 687\n",
      "Loss: 0.589104 Accuracy: 0.832589 Step: 688\n",
      "Loss: 0.609762 Accuracy: 0.815625 Step: 689\n",
      "Loss: 0.672017 Accuracy: 0.792411 Step: 690\n",
      "Loss: 0.642675 Accuracy: 0.840625 Step: 691\n",
      "Loss: 0.659889 Accuracy: 0.796875 Step: 692\n",
      "Loss: 0.615312 Accuracy: 0.828125 Step: 693\n",
      "Loss: 0.603327 Accuracy: 0.810268 Step: 694\n",
      "Loss: 0.672531 Accuracy: 0.821875 Step: 695\n",
      "Loss: 0.576072 Accuracy: 0.830357 Step: 696\n",
      "Loss: 0.665801 Accuracy: 0.8375 Step: 697\n",
      "Loss: 0.592112 Accuracy: 0.819196 Step: 698\n",
      "Loss: 0.628857 Accuracy: 0.834375 Step: 699\n",
      "Loss: 0.679965 Accuracy: 0.785714 Step: 700\n",
      "GT: [4 5 8 6 7 1 0]\n",
      "Predicted: [2 5 8 6 7 1 0]\n",
      "------\n",
      "GT: [3 5 3 4 5 4 1]\n",
      "Predicted: [2 5 3 4 5 3 1]\n",
      "------\n",
      "GT: [9 9 9 3 6 1 0]\n",
      "Predicted: [2 9 9 3 6 1 0]\n",
      "------\n",
      "Loss: 0.62714 Accuracy: 0.83125 Step: 701\n",
      "Loss: 0.623284 Accuracy: 0.805804 Step: 702\n",
      "Loss: 0.640189 Accuracy: 0.828125 Step: 703\n",
      "Loss: 0.606444 Accuracy: 0.801339 Step: 704\n",
      "Loss: 0.595054 Accuracy: 0.815625 Step: 705\n",
      "Loss: 0.598601 Accuracy: 0.823661 Step: 706\n",
      "Loss: 0.62428 Accuracy: 0.821875 Step: 707\n",
      "Loss: 0.600689 Accuracy: 0.825893 Step: 708\n",
      "Loss: 0.605057 Accuracy: 0.834375 Step: 709\n",
      "Loss: 0.650655 Accuracy: 0.808036 Step: 710\n",
      "Loss: 0.620117 Accuracy: 0.825 Step: 711\n",
      "Loss: 0.570076 Accuracy: 0.810268 Step: 712\n",
      "Loss: 0.631984 Accuracy: 0.81875 Step: 713\n",
      "Loss: 0.62492 Accuracy: 0.819196 Step: 714\n",
      "Loss: 0.630673 Accuracy: 0.809375 Step: 715\n",
      "Loss: 0.56392 Accuracy: 0.810268 Step: 716\n",
      "Loss: 0.637582 Accuracy: 0.821875 Step: 717\n",
      "Loss: 0.557742 Accuracy: 0.837054 Step: 718\n",
      "Loss: 0.649841 Accuracy: 0.825 Step: 719\n",
      "Loss: 0.550337 Accuracy: 0.84375 Step: 720\n",
      "Loss: 0.633986 Accuracy: 0.821875 Step: 721\n",
      "Loss: 0.543337 Accuracy: 0.832589 Step: 722\n",
      "Loss: 0.528513 Accuracy: 0.848214 Step: 723\n",
      "Loss: 0.61004 Accuracy: 0.825 Step: 724\n",
      "Loss: 0.552053 Accuracy: 0.832589 Step: 725\n",
      "Loss: 0.572503 Accuracy: 0.821875 Step: 726\n",
      "Loss: 0.54146 Accuracy: 0.84375 Step: 727\n",
      "Loss: 0.624115 Accuracy: 0.83125 Step: 728\n",
      "Loss: 0.577623 Accuracy: 0.8125 Step: 729\n",
      "Loss: 0.60666 Accuracy: 0.81875 Step: 730\n",
      "Loss: 0.542287 Accuracy: 0.837054 Step: 731\n",
      "Loss: 0.606735 Accuracy: 0.83125 Step: 732\n",
      "Loss: 0.560499 Accuracy: 0.84375 Step: 733\n",
      "Loss: 0.60746 Accuracy: 0.834375 Step: 734\n",
      "Loss: 0.589069 Accuracy: 0.84375 Step: 735\n",
      "Loss: 0.594845 Accuracy: 0.83125 Step: 736\n",
      "Loss: 0.575891 Accuracy: 0.815625 Step: 737\n",
      "Loss: 0.52663 Accuracy: 0.84375 Step: 738\n",
      "Loss: 0.607291 Accuracy: 0.815625 Step: 739\n",
      "Loss: 0.580658 Accuracy: 0.8125 Step: 740\n",
      "Loss: 0.61051 Accuracy: 0.821875 Step: 741\n",
      "Loss: 0.55826 Accuracy: 0.821429 Step: 742\n",
      "Loss: 0.578117 Accuracy: 0.8125 Step: 743\n",
      "Loss: 0.542986 Accuracy: 0.852679 Step: 744\n",
      "Loss: 0.620766 Accuracy: 0.825 Step: 745\n",
      "Loss: 0.567313 Accuracy: 0.825893 Step: 746\n",
      "Loss: 0.560659 Accuracy: 0.8375 Step: 747\n",
      "Loss: 0.578588 Accuracy: 0.821429 Step: 748\n",
      "Loss: 0.604886 Accuracy: 0.83125 Step: 749\n",
      "Loss: 0.558262 Accuracy: 0.834821 Step: 750\n",
      "Loss: 0.617888 Accuracy: 0.81875 Step: 751\n",
      "Loss: 0.611893 Accuracy: 0.828125 Step: 752\n",
      "Loss: 0.591045 Accuracy: 0.828125 Step: 753\n",
      "Loss: 0.528604 Accuracy: 0.850446 Step: 754\n",
      "Loss: 0.600681 Accuracy: 0.81875 Step: 755\n",
      "Loss: 0.573166 Accuracy: 0.821429 Step: 756\n",
      "Loss: 0.610193 Accuracy: 0.828125 Step: 757\n",
      "Loss: 0.529312 Accuracy: 0.837054 Step: 758\n",
      "Loss: 0.612169 Accuracy: 0.8125 Step: 759\n",
      "Loss: 0.52109 Accuracy: 0.841518 Step: 760\n",
      "Loss: 0.658193 Accuracy: 0.83125 Step: 761\n",
      "Loss: 0.602799 Accuracy: 0.830357 Step: 762\n",
      "Loss: 0.603593 Accuracy: 0.834375 Step: 763\n",
      "Loss: 0.555662 Accuracy: 0.857143 Step: 764\n",
      "Loss: 0.596332 Accuracy: 0.81875 Step: 765\n",
      "Loss: 0.636887 Accuracy: 0.816964 Step: 766\n",
      "Loss: 0.575099 Accuracy: 0.83125 Step: 767\n",
      "Loss: 0.560745 Accuracy: 0.821429 Step: 768\n",
      "Loss: 0.618606 Accuracy: 0.828125 Step: 769\n",
      "Loss: 0.559483 Accuracy: 0.850446 Step: 770\n",
      "Loss: 0.614415 Accuracy: 0.821875 Step: 771\n",
      "Loss: 0.51205 Accuracy: 0.859375 Step: 772\n",
      "Loss: 0.611464 Accuracy: 0.834375 Step: 773\n",
      "Loss: 0.526392 Accuracy: 0.84375 Step: 774\n",
      "Loss: 0.615501 Accuracy: 0.825 Step: 775\n",
      "Loss: 0.5534 Accuracy: 0.828125 Step: 776\n",
      "Loss: 0.625029 Accuracy: 0.834375 Step: 777\n",
      "Loss: 0.526663 Accuracy: 0.848214 Step: 778\n",
      "Loss: 0.634917 Accuracy: 0.809375 Step: 779\n",
      "Loss: 0.52024 Accuracy: 0.857143 Step: 780\n",
      "Loss: 0.629512 Accuracy: 0.8375 Step: 781\n",
      "Loss: 0.494214 Accuracy: 0.857143 Step: 782\n",
      "Loss: 0.614029 Accuracy: 0.821875 Step: 783\n",
      "Loss: 0.530196 Accuracy: 0.837054 Step: 784\n",
      "Loss: 0.615102 Accuracy: 0.81875 Step: 785\n",
      "Loss: 0.520538 Accuracy: 0.857143 Step: 786\n",
      "Loss: 0.618673 Accuracy: 0.81875 Step: 787\n",
      "Loss: 0.54971 Accuracy: 0.830357 Step: 788\n",
      "Loss: 0.571941 Accuracy: 0.83125 Step: 789\n",
      "Loss: 0.513913 Accuracy: 0.84375 Step: 790\n",
      "Loss: 0.581407 Accuracy: 0.828125 Step: 791\n",
      "Loss: 0.492453 Accuracy: 0.870536 Step: 792\n",
      "Loss: 0.581786 Accuracy: 0.81875 Step: 793\n",
      "Loss: 0.548334 Accuracy: 0.839286 Step: 794\n",
      "Loss: 0.576156 Accuracy: 0.81875 Step: 795\n",
      "Loss: 0.51647 Accuracy: 0.863839 Step: 796\n",
      "Loss: 0.601288 Accuracy: 0.809375 Step: 797\n",
      "Loss: 0.511161 Accuracy: 0.857143 Step: 798\n",
      "Loss: 0.582306 Accuracy: 0.81875 Step: 799\n",
      "Loss: 0.484286 Accuracy: 0.863839 Step: 800\n",
      "GT: [7 9 5 2 6 9 1]\n",
      "Predicted: [8 9 5 2 6 9 1]\n",
      "------\n",
      "GT: [4 7 2 6 9 2 1]\n",
      "Predicted: [8 7 2 6 9 2 1]\n",
      "------\n",
      "GT: [4 7 3 6 5 1 0]\n",
      "Predicted: [8 7 3 6 5 1 0]\n",
      "------\n",
      "Loss: 0.599736 Accuracy: 0.821875 Step: 801\n",
      "Loss: 0.610007 Accuracy: 0.821875 Step: 802\n",
      "Loss: 0.506857 Accuracy: 0.852679 Step: 803\n",
      "Loss: 0.492548 Accuracy: 0.868304 Step: 804\n",
      "Loss: 0.582249 Accuracy: 0.828125 Step: 805\n",
      "Loss: 0.495231 Accuracy: 0.848214 Step: 806\n",
      "Loss: 0.564941 Accuracy: 0.815625 Step: 807\n",
      "Loss: 0.490458 Accuracy: 0.845982 Step: 808\n",
      "Loss: 0.63135 Accuracy: 0.83125 Step: 809\n",
      "Loss: 0.487752 Accuracy: 0.857143 Step: 810\n",
      "Loss: 0.604453 Accuracy: 0.821875 Step: 811\n",
      "Loss: 0.510886 Accuracy: 0.84375 Step: 812\n",
      "Loss: 0.627005 Accuracy: 0.809375 Step: 813\n",
      "Loss: 0.499659 Accuracy: 0.852679 Step: 814\n",
      "Loss: 0.581844 Accuracy: 0.809375 Step: 815\n",
      "Loss: 0.471915 Accuracy: 0.84375 Step: 816\n",
      "Loss: 0.613659 Accuracy: 0.834375 Step: 817\n",
      "Loss: 0.590281 Accuracy: 0.81875 Step: 818\n",
      "Loss: 0.528782 Accuracy: 0.845982 Step: 819\n",
      "Loss: 0.56798 Accuracy: 0.825 Step: 820\n",
      "Loss: 0.470804 Accuracy: 0.854911 Step: 821\n",
      "Loss: 0.57897 Accuracy: 0.825 Step: 822\n",
      "Loss: 0.501397 Accuracy: 0.859375 Step: 823\n",
      "Loss: 0.594649 Accuracy: 0.821875 Step: 824\n",
      "Loss: 0.520678 Accuracy: 0.84375 Step: 825\n",
      "Loss: 0.621511 Accuracy: 0.840625 Step: 826\n",
      "Loss: 0.519228 Accuracy: 0.841518 Step: 827\n",
      "Loss: 0.588597 Accuracy: 0.828125 Step: 828\n",
      "Loss: 0.522368 Accuracy: 0.825893 Step: 829\n",
      "Loss: 0.627491 Accuracy: 0.825 Step: 830\n",
      "Loss: 0.538466 Accuracy: 0.837054 Step: 831\n",
      "Loss: 0.621212 Accuracy: 0.828125 Step: 832\n",
      "Loss: 0.52108 Accuracy: 0.821429 Step: 833\n",
      "Loss: 0.595308 Accuracy: 0.8125 Step: 834\n",
      "Loss: 0.636497 Accuracy: 0.801339 Step: 835\n",
      "Loss: 0.580767 Accuracy: 0.821875 Step: 836\n",
      "Loss: 0.576995 Accuracy: 0.834821 Step: 837\n",
      "Loss: 0.627157 Accuracy: 0.828125 Step: 838\n",
      "Loss: 0.564169 Accuracy: 0.837054 Step: 839\n",
      "Loss: 0.525154 Accuracy: 0.834821 Step: 840\n",
      "Loss: 0.590882 Accuracy: 0.834375 Step: 841\n",
      "Loss: 0.610717 Accuracy: 0.809375 Step: 842\n",
      "Loss: 0.581205 Accuracy: 0.8125 Step: 843\n",
      "Loss: 0.553588 Accuracy: 0.834375 Step: 844\n",
      "Loss: 0.524152 Accuracy: 0.84375 Step: 845\n",
      "Loss: 0.600975 Accuracy: 0.8125 Step: 846\n",
      "Loss: 0.541925 Accuracy: 0.821429 Step: 847\n",
      "Loss: 0.60088 Accuracy: 0.846875 Step: 848\n",
      "Loss: 0.536129 Accuracy: 0.830357 Step: 849\n",
      "Loss: 0.630512 Accuracy: 0.825 Step: 850\n",
      "Loss: 0.580647 Accuracy: 0.801339 Step: 851\n",
      "Loss: 0.624229 Accuracy: 0.828125 Step: 852\n",
      "Loss: 0.558432 Accuracy: 0.810268 Step: 853\n",
      "Loss: 0.615812 Accuracy: 0.8375 Step: 854\n",
      "Loss: 0.48832 Accuracy: 0.850446 Step: 855\n",
      "Loss: 0.586216 Accuracy: 0.834375 Step: 856\n",
      "Loss: 0.550569 Accuracy: 0.832589 Step: 857\n",
      "Loss: 0.504121 Accuracy: 0.841518 Step: 858\n",
      "Loss: 0.617442 Accuracy: 0.83125 Step: 859\n",
      "Loss: 0.576586 Accuracy: 0.801339 Step: 860\n",
      "Loss: 0.546074 Accuracy: 0.825 Step: 861\n",
      "Loss: 0.539623 Accuracy: 0.837054 Step: 862\n",
      "Loss: 0.605545 Accuracy: 0.821875 Step: 863\n",
      "Loss: 0.585858 Accuracy: 0.834375 Step: 864\n",
      "Loss: 0.556849 Accuracy: 0.834821 Step: 865\n",
      "Loss: 0.595045 Accuracy: 0.828125 Step: 866\n",
      "Loss: 0.570477 Accuracy: 0.819196 Step: 867\n",
      "Loss: 0.530477 Accuracy: 0.825893 Step: 868\n",
      "Loss: 0.597555 Accuracy: 0.809375 Step: 869\n",
      "Loss: 0.547932 Accuracy: 0.84375 Step: 870\n",
      "Loss: 0.598717 Accuracy: 0.81875 Step: 871\n",
      "Loss: 0.533534 Accuracy: 0.816964 Step: 872\n",
      "Loss: 0.601299 Accuracy: 0.815625 Step: 873\n",
      "Loss: 0.52742 Accuracy: 0.837054 Step: 874\n",
      "Loss: 0.626481 Accuracy: 0.821875 Step: 875\n",
      "Loss: 0.52165 Accuracy: 0.837054 Step: 876\n",
      "Loss: 0.616924 Accuracy: 0.8125 Step: 877\n",
      "Loss: 0.519488 Accuracy: 0.828125 Step: 878\n",
      "Loss: 0.576975 Accuracy: 0.828125 Step: 879\n",
      "Loss: 0.507999 Accuracy: 0.84375 Step: 880\n",
      "Loss: 0.579062 Accuracy: 0.825 Step: 881\n",
      "Loss: 0.503583 Accuracy: 0.839286 Step: 882\n",
      "Loss: 0.574511 Accuracy: 0.821875 Step: 883\n",
      "Loss: 0.485915 Accuracy: 0.837054 Step: 884\n",
      "Loss: 0.489373 Accuracy: 0.837054 Step: 885\n",
      "Loss: 0.592805 Accuracy: 0.825 Step: 886\n",
      "Loss: 0.579877 Accuracy: 0.834375 Step: 887\n",
      "Loss: 0.49147 Accuracy: 0.845982 Step: 888\n",
      "Loss: 0.58527 Accuracy: 0.825 Step: 889\n",
      "Loss: 0.493784 Accuracy: 0.841518 Step: 890\n",
      "Loss: 0.567754 Accuracy: 0.8375 Step: 891\n",
      "Loss: 0.523903 Accuracy: 0.854911 Step: 892\n",
      "Loss: 0.557917 Accuracy: 0.815625 Step: 893\n",
      "Loss: 0.528043 Accuracy: 0.828125 Step: 894\n",
      "Loss: 0.580461 Accuracy: 0.815625 Step: 895\n",
      "Loss: 0.482704 Accuracy: 0.845982 Step: 896\n",
      "Loss: 0.602217 Accuracy: 0.83125 Step: 897\n",
      "Loss: 0.53455 Accuracy: 0.825893 Step: 898\n",
      "Loss: 0.544692 Accuracy: 0.8125 Step: 899\n",
      "Loss: 0.486597 Accuracy: 0.863839 Step: 900\n",
      "GT: [4 5 5 3 5 1 0]\n",
      "Predicted: [2 5 5 3 5 1 0]\n",
      "------\n",
      "GT: [6 7 5 6 4 1 0]\n",
      "Predicted: [2 7 5 6 4 1 0]\n",
      "------\n",
      "GT: [6 3 3 3 9 1 0]\n",
      "Predicted: [2 3 3 3 9 1 0]\n",
      "------\n",
      "Loss: 0.607516 Accuracy: 0.821875 Step: 901\n",
      "Loss: 0.512937 Accuracy: 0.832589 Step: 902\n",
      "Loss: 0.546386 Accuracy: 0.8375 Step: 903\n",
      "Loss: 0.493155 Accuracy: 0.850446 Step: 904\n",
      "Loss: 0.557033 Accuracy: 0.834375 Step: 905\n",
      "Loss: 0.506206 Accuracy: 0.839286 Step: 906\n",
      "Loss: 0.593269 Accuracy: 0.825 Step: 907\n",
      "Loss: 0.50061 Accuracy: 0.863839 Step: 908\n",
      "Loss: 0.606403 Accuracy: 0.834375 Step: 909\n",
      "Loss: 0.614091 Accuracy: 0.792411 Step: 910\n",
      "Loss: 0.56699 Accuracy: 0.828125 Step: 911\n",
      "Loss: 0.589958 Accuracy: 0.814732 Step: 912\n",
      "Loss: 0.567932 Accuracy: 0.821875 Step: 913\n",
      "Loss: 0.538162 Accuracy: 0.837054 Step: 914\n",
      "Loss: 0.611053 Accuracy: 0.815625 Step: 915\n",
      "Loss: 0.541885 Accuracy: 0.816964 Step: 916\n",
      "Loss: 0.60481 Accuracy: 0.83125 Step: 917\n",
      "Loss: 0.539095 Accuracy: 0.863839 Step: 918\n",
      "Loss: 0.61755 Accuracy: 0.834375 Step: 919\n",
      "Loss: 0.499451 Accuracy: 0.857143 Step: 920\n",
      "Loss: 0.559929 Accuracy: 0.815625 Step: 921\n",
      "Loss: 0.521755 Accuracy: 0.848214 Step: 922\n",
      "Loss: 0.554135 Accuracy: 0.8375 Step: 923\n",
      "Loss: 0.578865 Accuracy: 0.821875 Step: 924\n",
      "Loss: 0.569733 Accuracy: 0.825893 Step: 925\n",
      "Loss: 0.615572 Accuracy: 0.815625 Step: 926\n",
      "Loss: 0.510302 Accuracy: 0.845982 Step: 927\n",
      "Loss: 0.581851 Accuracy: 0.828125 Step: 928\n",
      "Loss: 0.514978 Accuracy: 0.839286 Step: 929\n",
      "Loss: 0.621299 Accuracy: 0.8375 Step: 930\n",
      "Loss: 0.516303 Accuracy: 0.84375 Step: 931\n",
      "Loss: 0.595503 Accuracy: 0.828125 Step: 932\n",
      "Loss: 0.496589 Accuracy: 0.861607 Step: 933\n",
      "Loss: 0.479806 Accuracy: 0.857143 Step: 934\n",
      "Loss: 0.591838 Accuracy: 0.8125 Step: 935\n",
      "Loss: 0.611645 Accuracy: 0.8375 Step: 936\n",
      "Loss: 0.567258 Accuracy: 0.828125 Step: 937\n",
      "Loss: 0.47404 Accuracy: 0.859375 Step: 938\n",
      "Loss: 0.608369 Accuracy: 0.8125 Step: 939\n",
      "Loss: 0.592853 Accuracy: 0.825 Step: 940\n",
      "Loss: 1.01224 Accuracy: 0.683036 Step: 941\n",
      "Loss: 0.606984 Accuracy: 0.825 Step: 942\n",
      "Loss: 1.13279 Accuracy: 0.720982 Step: 943\n",
      "Loss: 1.00643 Accuracy: 0.741071 Step: 944\n",
      "Loss: 0.621839 Accuracy: 0.81875 Step: 945\n",
      "Loss: 0.562989 Accuracy: 0.83125 Step: 946\n",
      "Loss: 0.722793 Accuracy: 0.772321 Step: 947\n",
      "Loss: 0.693572 Accuracy: 0.771875 Step: 948\n",
      "Loss: 0.692673 Accuracy: 0.805804 Step: 949\n",
      "Loss: 0.579853 Accuracy: 0.825 Step: 950\n",
      "Loss: 0.625284 Accuracy: 0.799107 Step: 951\n",
      "Loss: 0.609317 Accuracy: 0.825893 Step: 952\n",
      "Loss: 0.596614 Accuracy: 0.809375 Step: 953\n",
      "Loss: 0.579907 Accuracy: 0.821875 Step: 954\n",
      "Loss: 0.742272 Accuracy: 0.756696 Step: 955\n",
      "Loss: 0.589313 Accuracy: 0.821875 Step: 956\n",
      "Loss: 0.614165 Accuracy: 0.785714 Step: 957\n",
      "Loss: 0.606774 Accuracy: 0.8125 Step: 958\n",
      "Loss: 0.61401 Accuracy: 0.801339 Step: 959\n",
      "Loss: 0.587374 Accuracy: 0.821875 Step: 960\n",
      "Loss: 0.629003 Accuracy: 0.794643 Step: 961\n",
      "Loss: 0.575742 Accuracy: 0.81875 Step: 962\n",
      "Loss: 0.562891 Accuracy: 0.810268 Step: 963\n",
      "Loss: 0.52104 Accuracy: 0.834821 Step: 964\n",
      "Loss: 0.600427 Accuracy: 0.828125 Step: 965\n",
      "Loss: 0.553588 Accuracy: 0.819196 Step: 966\n",
      "Loss: 0.607057 Accuracy: 0.825 Step: 967\n",
      "Loss: 0.610742 Accuracy: 0.8375 Step: 968\n",
      "Loss: 0.668455 Accuracy: 0.779018 Step: 969\n",
      "Loss: 0.58403 Accuracy: 0.821875 Step: 970\n",
      "Loss: 0.548529 Accuracy: 0.823661 Step: 971\n",
      "Loss: 0.587075 Accuracy: 0.825 Step: 972\n",
      "Loss: 0.598955 Accuracy: 0.790179 Step: 973\n",
      "Loss: 0.59601 Accuracy: 0.81875 Step: 974\n",
      "Loss: 0.555131 Accuracy: 0.814732 Step: 975\n",
      "Loss: 0.573213 Accuracy: 0.825 Step: 976\n",
      "Loss: 0.516197 Accuracy: 0.848214 Step: 977\n",
      "Loss: 0.545657 Accuracy: 0.828125 Step: 978\n",
      "Loss: 0.5456 Accuracy: 0.825893 Step: 979\n",
      "Loss: 0.582762 Accuracy: 0.825 Step: 980\n",
      "Loss: 0.537442 Accuracy: 0.834821 Step: 981\n",
      "Loss: 0.608908 Accuracy: 0.83125 Step: 982\n",
      "Loss: 0.58691 Accuracy: 0.828125 Step: 983\n",
      "Loss: 0.546238 Accuracy: 0.821429 Step: 984\n",
      "Loss: 0.595326 Accuracy: 0.825 Step: 985\n",
      "Loss: 0.527603 Accuracy: 0.823661 Step: 986\n",
      "Loss: 0.517254 Accuracy: 0.852679 Step: 987\n",
      "Loss: 0.607788 Accuracy: 0.821875 Step: 988\n",
      "Loss: 0.508871 Accuracy: 0.834821 Step: 989\n",
      "Loss: 0.587495 Accuracy: 0.825 Step: 990\n",
      "Loss: 0.497682 Accuracy: 0.854911 Step: 991\n",
      "Loss: 0.578144 Accuracy: 0.815625 Step: 992\n",
      "Loss: 0.521963 Accuracy: 0.848214 Step: 993\n",
      "Loss: 0.60206 Accuracy: 0.821875 Step: 994\n",
      "Loss: 0.505616 Accuracy: 0.84375 Step: 995\n",
      "Loss: 0.562883 Accuracy: 0.834375 Step: 996\n",
      "Loss: 0.519995 Accuracy: 0.821429 Step: 997\n",
      "Loss: 0.591336 Accuracy: 0.825 Step: 998\n",
      "Loss: 0.464702 Accuracy: 0.852679 Step: 999\n",
      "Loss: 0.543894 Accuracy: 0.81875 Step: 1000\n",
      "GT: [6 9 6 2 1]\n",
      "Predicted: [4 9 6 2 1]\n",
      "------\n",
      "GT: [2 8 4 3 1]\n",
      "Predicted: [4 8 4 3 1]\n",
      "------\n",
      "GT: [2 2 4 1 0]\n",
      "Predicted: [4 2 4 1 0]\n",
      "------\n",
      "Loss: 0.490309 Accuracy: 0.861607 Step: 1001\n",
      "Loss: 0.566159 Accuracy: 0.828125 Step: 1002\n",
      "Loss: 0.464636 Accuracy: 0.872768 Step: 1003\n",
      "Loss: 0.591531 Accuracy: 0.81875 Step: 1004\n",
      "Loss: 0.474686 Accuracy: 0.859375 Step: 1005\n",
      "Loss: 0.580839 Accuracy: 0.825 Step: 1006\n",
      "Loss: 0.488361 Accuracy: 0.837054 Step: 1007\n",
      "Loss: 0.605061 Accuracy: 0.8125 Step: 1008\n",
      "Loss: 0.462925 Accuracy: 0.866071 Step: 1009\n",
      "Loss: 0.601888 Accuracy: 0.84375 Step: 1010\n",
      "Loss: 0.495995 Accuracy: 0.845982 Step: 1011\n",
      "Loss: 0.577113 Accuracy: 0.834375 Step: 1012\n",
      "Loss: 0.472494 Accuracy: 0.850446 Step: 1013\n",
      "Loss: 0.58441 Accuracy: 0.8375 Step: 1014\n",
      "Loss: 0.457995 Accuracy: 0.850446 Step: 1015\n",
      "Loss: 0.553076 Accuracy: 0.821875 Step: 1016\n",
      "Loss: 0.466215 Accuracy: 0.857143 Step: 1017\n",
      "Loss: 0.558155 Accuracy: 0.825 Step: 1018\n",
      "Loss: 0.45648 Accuracy: 0.863839 Step: 1019\n",
      "Loss: 0.580266 Accuracy: 0.821875 Step: 1020\n",
      "Loss: 0.474456 Accuracy: 0.866071 Step: 1021\n",
      "Loss: 0.550177 Accuracy: 0.84375 Step: 1022\n",
      "Loss: 0.445048 Accuracy: 0.863839 Step: 1023\n",
      "Loss: 0.577302 Accuracy: 0.8125 Step: 1024\n",
      "Loss: 0.440151 Accuracy: 0.866071 Step: 1025\n",
      "Loss: 0.578719 Accuracy: 0.815625 Step: 1026\n",
      "Loss: 0.48157 Accuracy: 0.854911 Step: 1027\n",
      "Loss: 0.584081 Accuracy: 0.81875 Step: 1028\n",
      "Loss: 0.423632 Accuracy: 0.852679 Step: 1029\n",
      "Loss: 0.598148 Accuracy: 0.825 Step: 1030\n",
      "Loss: 0.423892 Accuracy: 0.877232 Step: 1031\n",
      "Loss: 0.596297 Accuracy: 0.825 Step: 1032\n",
      "Loss: 0.455562 Accuracy: 0.872768 Step: 1033\n",
      "Loss: 0.579732 Accuracy: 0.81875 Step: 1034\n",
      "Loss: 0.419415 Accuracy: 0.870536 Step: 1035\n",
      "Loss: 0.439834 Accuracy: 0.861607 Step: 1036\n",
      "Loss: 0.552277 Accuracy: 0.8375 Step: 1037\n",
      "Loss: 0.427794 Accuracy: 0.861607 Step: 1038\n",
      "Loss: 0.551376 Accuracy: 0.8125 Step: 1039\n",
      "Loss: 0.436413 Accuracy: 0.875 Step: 1040\n",
      "Loss: 0.578478 Accuracy: 0.81875 Step: 1041\n",
      "Loss: 0.449636 Accuracy: 0.854911 Step: 1042\n",
      "Loss: 0.565659 Accuracy: 0.821875 Step: 1043\n",
      "Loss: 0.442287 Accuracy: 0.854911 Step: 1044\n",
      "Loss: 0.568058 Accuracy: 0.83125 Step: 1045\n",
      "Loss: 0.429453 Accuracy: 0.881696 Step: 1046\n",
      "Loss: 0.553241 Accuracy: 0.846875 Step: 1047\n",
      "Loss: 0.548965 Accuracy: 0.815625 Step: 1048\n",
      "Loss: 0.482514 Accuracy: 0.866071 Step: 1049\n",
      "Loss: 0.540766 Accuracy: 0.8125 Step: 1050\n",
      "Loss: 0.409849 Accuracy: 0.879464 Step: 1051\n",
      "Loss: 0.563676 Accuracy: 0.828125 Step: 1052\n",
      "Loss: 0.44281 Accuracy: 0.861607 Step: 1053\n",
      "Loss: 0.562497 Accuracy: 0.815625 Step: 1054\n",
      "Loss: 0.438419 Accuracy: 0.866071 Step: 1055\n",
      "Loss: 0.552569 Accuracy: 0.81875 Step: 1056\n",
      "Loss: 0.421281 Accuracy: 0.866071 Step: 1057\n",
      "Loss: 0.565028 Accuracy: 0.825 Step: 1058\n",
      "Loss: 0.421637 Accuracy: 0.861607 Step: 1059\n",
      "Loss: 0.551741 Accuracy: 0.834375 Step: 1060\n",
      "Loss: 0.433692 Accuracy: 0.879464 Step: 1061\n",
      "Loss: 0.545414 Accuracy: 0.8375 Step: 1062\n",
      "Loss: 0.422379 Accuracy: 0.868304 Step: 1063\n",
      "Loss: 0.572984 Accuracy: 0.815625 Step: 1064\n",
      "Loss: 0.4346 Accuracy: 0.875 Step: 1065\n",
      "Loss: 0.54744 Accuracy: 0.825 Step: 1066\n",
      "Loss: 0.431086 Accuracy: 0.872768 Step: 1067\n",
      "Loss: 0.570266 Accuracy: 0.815625 Step: 1068\n",
      "Loss: 0.415275 Accuracy: 0.883929 Step: 1069\n",
      "Loss: 0.556578 Accuracy: 0.8375 Step: 1070\n",
      "Loss: 0.417104 Accuracy: 0.870536 Step: 1071\n",
      "Loss: 0.570664 Accuracy: 0.828125 Step: 1072\n",
      "Loss: 0.432675 Accuracy: 0.866071 Step: 1073\n",
      "Loss: 0.61654 Accuracy: 0.8375 Step: 1074\n",
      "Loss: 0.428752 Accuracy: 0.888393 Step: 1075\n",
      "Loss: 0.546321 Accuracy: 0.825 Step: 1076\n",
      "Loss: 0.436947 Accuracy: 0.879464 Step: 1077\n",
      "Loss: 0.545819 Accuracy: 0.825 Step: 1078\n",
      "Loss: 0.453091 Accuracy: 0.875 Step: 1079\n",
      "Loss: 0.541343 Accuracy: 0.828125 Step: 1080\n",
      "Loss: 0.434897 Accuracy: 0.870536 Step: 1081\n",
      "Loss: 0.589236 Accuracy: 0.815625 Step: 1082\n",
      "Loss: 0.438182 Accuracy: 0.877232 Step: 1083\n",
      "Loss: 0.567602 Accuracy: 0.828125 Step: 1084\n",
      "Loss: 0.443102 Accuracy: 0.863839 Step: 1085\n",
      "Loss: 0.572615 Accuracy: 0.83125 Step: 1086\n",
      "Loss: 0.444682 Accuracy: 0.863839 Step: 1087\n",
      "Loss: 0.583218 Accuracy: 0.834375 Step: 1088\n",
      "Loss: 0.430771 Accuracy: 0.866071 Step: 1089\n",
      "Loss: 0.550166 Accuracy: 0.834375 Step: 1090\n",
      "Loss: 0.435827 Accuracy: 0.866071 Step: 1091\n",
      "Loss: 0.597783 Accuracy: 0.81875 Step: 1092\n",
      "Loss: 0.414412 Accuracy: 0.875 Step: 1093\n",
      "Loss: 0.592995 Accuracy: 0.821875 Step: 1094\n",
      "Loss: 0.425582 Accuracy: 0.888393 Step: 1095\n",
      "Loss: 0.562286 Accuracy: 0.825 Step: 1096\n",
      "Loss: 0.428134 Accuracy: 0.870536 Step: 1097\n",
      "Loss: 0.580585 Accuracy: 0.834375 Step: 1098\n",
      "Loss: 0.438304 Accuracy: 0.879464 Step: 1099\n",
      "Loss: 0.565026 Accuracy: 0.828125 Step: 1100\n",
      "GT: [8 9 9 1 0]\n",
      "Predicted: [8 9 9 1 0]\n",
      "------\n",
      "GT: [4 2 5 1 0]\n",
      "Predicted: [8 2 5 1 0]\n",
      "------\n",
      "GT: [5 8 6 9 1]\n",
      "Predicted: [8 8 6 9 1]\n",
      "------\n",
      "Loss: 0.413673 Accuracy: 0.877232 Step: 1101\n",
      "Loss: 0.535524 Accuracy: 0.834375 Step: 1102\n",
      "Loss: 0.41448 Accuracy: 0.875 Step: 1103\n",
      "Loss: 0.562343 Accuracy: 0.815625 Step: 1104\n",
      "Loss: 0.420722 Accuracy: 0.879464 Step: 1105\n",
      "Loss: 0.538958 Accuracy: 0.825 Step: 1106\n",
      "Loss: 0.424659 Accuracy: 0.875 Step: 1107\n",
      "Loss: 0.550429 Accuracy: 0.81875 Step: 1108\n",
      "Loss: 0.448734 Accuracy: 0.870536 Step: 1109\n",
      "Loss: 0.565232 Accuracy: 0.809375 Step: 1110\n",
      "Loss: 0.413991 Accuracy: 0.868304 Step: 1111\n",
      "Loss: 0.543642 Accuracy: 0.815625 Step: 1112\n",
      "Loss: 0.56287 Accuracy: 0.825 Step: 1113\n",
      "Loss: 0.413395 Accuracy: 0.872768 Step: 1114\n",
      "Loss: 0.557245 Accuracy: 0.815625 Step: 1115\n",
      "Loss: 0.423074 Accuracy: 0.875 Step: 1116\n",
      "Loss: 0.435057 Accuracy: 0.866071 Step: 1117\n",
      "Loss: 0.549937 Accuracy: 0.825 Step: 1118\n",
      "Loss: 0.422979 Accuracy: 0.863839 Step: 1119\n",
      "Loss: 0.543351 Accuracy: 0.81875 Step: 1120\n",
      "Loss: 0.410545 Accuracy: 0.868304 Step: 1121\n",
      "Loss: 0.604424 Accuracy: 0.8375 Step: 1122\n",
      "Loss: 0.429891 Accuracy: 0.866071 Step: 1123\n",
      "Loss: 0.586043 Accuracy: 0.8125 Step: 1124\n",
      "Loss: 0.422634 Accuracy: 0.866071 Step: 1125\n",
      "Loss: 0.558397 Accuracy: 0.8125 Step: 1126\n",
      "Loss: 0.441374 Accuracy: 0.866071 Step: 1127\n",
      "Loss: 0.550483 Accuracy: 0.8125 Step: 1128\n",
      "Loss: 0.561326 Accuracy: 0.83125 Step: 1129\n",
      "Loss: 0.418555 Accuracy: 0.868304 Step: 1130\n",
      "Loss: 0.403908 Accuracy: 0.872768 Step: 1131\n",
      "Loss: 0.569126 Accuracy: 0.8125 Step: 1132\n",
      "Loss: 0.52226 Accuracy: 0.81875 Step: 1133\n",
      "Loss: 0.42706 Accuracy: 0.868304 Step: 1134\n",
      "Loss: 0.550937 Accuracy: 0.83125 Step: 1135\n",
      "Loss: 0.393817 Accuracy: 0.875 Step: 1136\n",
      "Loss: 0.582752 Accuracy: 0.821875 Step: 1137\n",
      "Loss: 0.435875 Accuracy: 0.863839 Step: 1138\n",
      "Loss: 0.553961 Accuracy: 0.828125 Step: 1139\n",
      "Loss: 0.415828 Accuracy: 0.870536 Step: 1140\n",
      "Loss: 0.567904 Accuracy: 0.828125 Step: 1141\n",
      "Loss: 0.427701 Accuracy: 0.857143 Step: 1142\n",
      "Loss: 0.58278 Accuracy: 0.825 Step: 1143\n",
      "Loss: 0.430557 Accuracy: 0.868304 Step: 1144\n",
      "Loss: 0.575312 Accuracy: 0.825 Step: 1145\n",
      "Loss: 0.417487 Accuracy: 0.863839 Step: 1146\n",
      "Loss: 0.567142 Accuracy: 0.8125 Step: 1147\n",
      "Loss: 0.423537 Accuracy: 0.870536 Step: 1148\n",
      "Loss: 0.55387 Accuracy: 0.815625 Step: 1149\n",
      "Loss: 0.423662 Accuracy: 0.883929 Step: 1150\n",
      "Loss: 0.558274 Accuracy: 0.828125 Step: 1151\n",
      "Loss: 0.43012 Accuracy: 0.857143 Step: 1152\n",
      "Loss: 0.560472 Accuracy: 0.834375 Step: 1153\n",
      "Loss: 0.418116 Accuracy: 0.861607 Step: 1154\n",
      "Loss: 0.546941 Accuracy: 0.81875 Step: 1155\n",
      "Loss: 0.422082 Accuracy: 0.861607 Step: 1156\n",
      "Loss: 0.53743 Accuracy: 0.83125 Step: 1157\n",
      "Loss: 0.419742 Accuracy: 0.866071 Step: 1158\n",
      "Loss: 0.570654 Accuracy: 0.8375 Step: 1159\n",
      "Loss: 0.40404 Accuracy: 0.888393 Step: 1160\n",
      "Loss: 0.427261 Accuracy: 0.875 Step: 1161\n",
      "Loss: 0.55355 Accuracy: 0.825 Step: 1162\n",
      "Loss: 0.589401 Accuracy: 0.83125 Step: 1163\n",
      "Loss: 0.403873 Accuracy: 0.872768 Step: 1164\n",
      "Loss: 0.577845 Accuracy: 0.834375 Step: 1165\n",
      "Loss: 0.402281 Accuracy: 0.879464 Step: 1166\n",
      "Loss: 0.569849 Accuracy: 0.8375 Step: 1167\n",
      "Loss: 0.406139 Accuracy: 0.870536 Step: 1168\n",
      "Loss: 0.554873 Accuracy: 0.8375 Step: 1169\n",
      "Loss: 0.406604 Accuracy: 0.875 Step: 1170\n",
      "Loss: 0.404601 Accuracy: 0.866071 Step: 1171\n",
      "Loss: 0.57023 Accuracy: 0.821875 Step: 1172\n",
      "Loss: 0.39794 Accuracy: 0.872768 Step: 1173\n",
      "Loss: 0.509652 Accuracy: 0.83125 Step: 1174\n",
      "Loss: 0.565921 Accuracy: 0.821875 Step: 1175\n",
      "Loss: 0.393992 Accuracy: 0.892857 Step: 1176\n",
      "Loss: 0.55475 Accuracy: 0.83125 Step: 1177\n",
      "Loss: 0.431209 Accuracy: 0.875 Step: 1178\n",
      "Loss: 0.557456 Accuracy: 0.83125 Step: 1179\n",
      "Loss: 0.390302 Accuracy: 0.881696 Step: 1180\n",
      "Loss: 0.401258 Accuracy: 0.875 Step: 1181\n",
      "Loss: 0.560621 Accuracy: 0.8125 Step: 1182\n",
      "Loss: 0.416638 Accuracy: 0.881696 Step: 1183\n",
      "Loss: 0.569411 Accuracy: 0.825 Step: 1184\n",
      "Loss: 0.384965 Accuracy: 0.868304 Step: 1185\n",
      "Loss: 0.558126 Accuracy: 0.828125 Step: 1186\n",
      "Loss: 0.38388 Accuracy: 0.879464 Step: 1187\n",
      "Loss: 0.589526 Accuracy: 0.825 Step: 1188\n",
      "Loss: 0.407145 Accuracy: 0.868304 Step: 1189\n",
      "Loss: 0.569181 Accuracy: 0.81875 Step: 1190\n",
      "Loss: 0.390603 Accuracy: 0.872768 Step: 1191\n",
      "Loss: 0.554258 Accuracy: 0.83125 Step: 1192\n",
      "Loss: 0.409336 Accuracy: 0.868304 Step: 1193\n",
      "Loss: 0.522602 Accuracy: 0.815625 Step: 1194\n",
      "Loss: 0.393522 Accuracy: 0.870536 Step: 1195\n",
      "Loss: 0.553945 Accuracy: 0.828125 Step: 1196\n",
      "Loss: 0.401854 Accuracy: 0.875 Step: 1197\n",
      "Loss: 0.550261 Accuracy: 0.821875 Step: 1198\n",
      "Loss: 0.41129 Accuracy: 0.861607 Step: 1199\n",
      "Loss: 0.55231 Accuracy: 0.81875 Step: 1200\n",
      "GT: [9 9 6 1 0]\n",
      "Predicted: [3 9 6 1 0]\n",
      "------\n",
      "GT: [6 8 7 5 1]\n",
      "Predicted: [3 8 7 5 1]\n",
      "------\n",
      "GT: [4 6 4 2 1]\n",
      "Predicted: [3 6 4 2 1]\n",
      "------\n",
      "Loss: 0.38377 Accuracy: 0.872768 Step: 1201\n",
      "Loss: 0.398837 Accuracy: 0.888393 Step: 1202\n",
      "Loss: 0.532806 Accuracy: 0.8375 Step: 1203\n",
      "Loss: 0.544776 Accuracy: 0.83125 Step: 1204\n",
      "Loss: 0.417508 Accuracy: 0.879464 Step: 1205\n",
      "Loss: 0.527835 Accuracy: 0.815625 Step: 1206\n",
      "Loss: 0.382451 Accuracy: 0.883929 Step: 1207\n",
      "Loss: 0.551811 Accuracy: 0.815625 Step: 1208\n",
      "Loss: 0.403554 Accuracy: 0.866071 Step: 1209\n",
      "Loss: 0.550129 Accuracy: 0.828125 Step: 1210\n",
      "Loss: 0.409802 Accuracy: 0.875 Step: 1211\n",
      "Loss: 0.533163 Accuracy: 0.8125 Step: 1212\n",
      "Loss: 0.397703 Accuracy: 0.879464 Step: 1213\n",
      "Loss: 0.568912 Accuracy: 0.825 Step: 1214\n",
      "Loss: 0.38225 Accuracy: 0.875 Step: 1215\n",
      "Loss: 0.510988 Accuracy: 0.840625 Step: 1216\n",
      "Loss: 0.393984 Accuracy: 0.881696 Step: 1217\n",
      "Loss: 0.532299 Accuracy: 0.828125 Step: 1218\n",
      "Loss: 0.569567 Accuracy: 0.821875 Step: 1219\n",
      "Loss: 0.389639 Accuracy: 0.879464 Step: 1220\n",
      "Loss: 0.406778 Accuracy: 0.872768 Step: 1221\n",
      "Loss: 0.544577 Accuracy: 0.825 Step: 1222\n",
      "Loss: 0.405811 Accuracy: 0.879464 Step: 1223\n",
      "Loss: 0.522872 Accuracy: 0.821875 Step: 1224\n",
      "Loss: 0.391318 Accuracy: 0.877232 Step: 1225\n",
      "Loss: 0.553248 Accuracy: 0.828125 Step: 1226\n",
      "Loss: 0.388309 Accuracy: 0.877232 Step: 1227\n",
      "Loss: 0.551236 Accuracy: 0.815625 Step: 1228\n",
      "Loss: 0.407374 Accuracy: 0.870536 Step: 1229\n",
      "Loss: 0.580483 Accuracy: 0.834375 Step: 1230\n",
      "Loss: 0.40282 Accuracy: 0.892857 Step: 1231\n",
      "Loss: 0.56508 Accuracy: 0.828125 Step: 1232\n",
      "Loss: 0.534955 Accuracy: 0.828125 Step: 1233\n",
      "Loss: 0.399551 Accuracy: 0.883929 Step: 1234\n",
      "Loss: 0.407343 Accuracy: 0.875 Step: 1235\n",
      "Loss: 0.511917 Accuracy: 0.83125 Step: 1236\n",
      "Loss: 0.551608 Accuracy: 0.83125 Step: 1237\n",
      "Loss: 0.407568 Accuracy: 0.877232 Step: 1238\n",
      "Loss: 0.563618 Accuracy: 0.8125 Step: 1239\n",
      "Loss: 0.406798 Accuracy: 0.879464 Step: 1240\n",
      "Loss: 0.546438 Accuracy: 0.840625 Step: 1241\n",
      "Loss: 0.399694 Accuracy: 0.879464 Step: 1242\n",
      "Loss: 0.562444 Accuracy: 0.83125 Step: 1243\n",
      "Loss: 0.41645 Accuracy: 0.870536 Step: 1244\n",
      "Loss: 0.568997 Accuracy: 0.834375 Step: 1245\n",
      "Loss: 0.402876 Accuracy: 0.875 Step: 1246\n",
      "Loss: 0.407975 Accuracy: 0.868304 Step: 1247\n",
      "Loss: 0.548296 Accuracy: 0.8125 Step: 1248\n",
      "Loss: 0.580682 Accuracy: 0.834375 Step: 1249\n",
      "Loss: 0.38855 Accuracy: 0.883929 Step: 1250\n",
      "Loss: 0.558938 Accuracy: 0.81875 Step: 1251\n",
      "Loss: 0.406141 Accuracy: 0.888393 Step: 1252\n",
      "Loss: 0.553434 Accuracy: 0.825 Step: 1253\n",
      "Loss: 0.394454 Accuracy: 0.877232 Step: 1254\n",
      "Loss: 0.560556 Accuracy: 0.825 Step: 1255\n",
      "Loss: 0.409647 Accuracy: 0.875 Step: 1256\n",
      "Loss: 0.529922 Accuracy: 0.828125 Step: 1257\n",
      "Loss: 0.396702 Accuracy: 0.877232 Step: 1258\n",
      "Loss: 0.534381 Accuracy: 0.8375 Step: 1259\n",
      "Loss: 0.393331 Accuracy: 0.883929 Step: 1260\n",
      "Loss: 0.535538 Accuracy: 0.8125 Step: 1261\n",
      "Loss: 0.401298 Accuracy: 0.881696 Step: 1262\n",
      "Loss: 0.531405 Accuracy: 0.825 Step: 1263\n",
      "Loss: 0.401452 Accuracy: 0.881696 Step: 1264\n",
      "Loss: 0.547431 Accuracy: 0.809375 Step: 1265\n",
      "Loss: 0.422341 Accuracy: 0.875 Step: 1266\n",
      "Loss: 0.539299 Accuracy: 0.825 Step: 1267\n",
      "Loss: 0.392092 Accuracy: 0.870536 Step: 1268\n",
      "Loss: 0.55544 Accuracy: 0.821875 Step: 1269\n",
      "Loss: 0.39727 Accuracy: 0.872768 Step: 1270\n",
      "Loss: 0.541454 Accuracy: 0.8125 Step: 1271\n",
      "Loss: 0.406875 Accuracy: 0.877232 Step: 1272\n",
      "Loss: 0.536667 Accuracy: 0.828125 Step: 1273\n",
      "Loss: 0.399823 Accuracy: 0.868304 Step: 1274\n",
      "Loss: 0.521067 Accuracy: 0.815625 Step: 1275\n",
      "Loss: 0.400531 Accuracy: 0.868304 Step: 1276\n",
      "Loss: 0.389418 Accuracy: 0.870536 Step: 1277\n",
      "Loss: 0.571096 Accuracy: 0.83125 Step: 1278\n",
      "Loss: 0.394246 Accuracy: 0.870536 Step: 1279\n",
      "Loss: 0.55529 Accuracy: 0.81875 Step: 1280\n",
      "Loss: 0.565468 Accuracy: 0.809375 Step: 1281\n",
      "Loss: 0.409605 Accuracy: 0.870536 Step: 1282\n",
      "Loss: 0.537338 Accuracy: 0.825 Step: 1283\n",
      "Loss: 0.37962 Accuracy: 0.881696 Step: 1284\n",
      "Loss: 0.543565 Accuracy: 0.83125 Step: 1285\n",
      "Loss: 0.386682 Accuracy: 0.870536 Step: 1286\n",
      "Loss: 0.549097 Accuracy: 0.815625 Step: 1287\n",
      "Loss: 0.380575 Accuracy: 0.877232 Step: 1288\n",
      "Loss: 0.536273 Accuracy: 0.81875 Step: 1289\n",
      "Loss: 0.404426 Accuracy: 0.870536 Step: 1290\n",
      "Loss: 0.51291 Accuracy: 0.83125 Step: 1291\n",
      "Loss: 0.37372 Accuracy: 0.875 Step: 1292\n",
      "Loss: 0.552578 Accuracy: 0.825 Step: 1293\n",
      "Loss: 0.403713 Accuracy: 0.872768 Step: 1294\n",
      "Loss: 0.563588 Accuracy: 0.834375 Step: 1295\n",
      "Loss: 0.397278 Accuracy: 0.875 Step: 1296\n",
      "Loss: 0.540929 Accuracy: 0.83125 Step: 1297\n",
      "Loss: 0.40249 Accuracy: 0.863839 Step: 1298\n",
      "Loss: 0.561465 Accuracy: 0.81875 Step: 1299\n",
      "Loss: 0.395166 Accuracy: 0.868304 Step: 1300\n",
      "GT: [4 3 4 4 3 1 0]\n",
      "Predicted: [6 3 4 4 3 1 0]\n",
      "------\n",
      "GT: [5 3 4 9 3 2 1]\n",
      "Predicted: [6 3 4 9 3 2 1]\n",
      "------\n",
      "GT: [3 4 5 7 3 1 0]\n",
      "Predicted: [6 4 5 7 3 1 0]\n",
      "------\n",
      "Loss: 0.555877 Accuracy: 0.828125 Step: 1301\n",
      "Loss: 0.396261 Accuracy: 0.870536 Step: 1302\n",
      "Loss: 0.549896 Accuracy: 0.815625 Step: 1303\n",
      "Loss: 0.410799 Accuracy: 0.870536 Step: 1304\n",
      "Loss: 0.537243 Accuracy: 0.821875 Step: 1305\n",
      "Loss: 0.40234 Accuracy: 0.879464 Step: 1306\n",
      "Loss: 0.555807 Accuracy: 0.825 Step: 1307\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-d238dd1142de>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcoord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglobal_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder_embedding_ids\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Accuracy:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Step:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1037\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1019\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1023\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default() as graph:\n",
    "    model = Seq2Seq(150, [15,50])\n",
    "    queue = InputPipeline(['./data/test_number_copy.tfrecords'], batch_size = 64, n_epochs = 50)\n",
    "    \n",
    "    input_seq, target_seq, history, input_seq_len, target_seq_len, history_size, history_seq_len = queue.inputs()\n",
    "    model.graph(input_seq, target_seq, input_seq_len, target_seq_len, history, history_size, history_seq_len)\n",
    "    \n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=1e-3).minimize(model.loss, global_step=global_step)\n",
    "\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.local_variables_initializer())\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        coord = tf.train.Coordinator()\n",
    "        threads = tf.train.start_queue_runners(coord=coord)\n",
    "        \n",
    "        while not coord.should_stop():\n",
    "            _, l, a, s, targets, generated = sess.run([train_op, model.loss, model.accuracy, global_step, model._target_seq, model.decoder_embedding_ids])\n",
    "\n",
    "            print('Loss:', l, 'Accuracy:', a, 'Step:', s)\n",
    "            if s % 100 == 0:\n",
    "                for i in range(3):\n",
    "                    print(\"GT:\", targets.T[i])\n",
    "                    print(\"Predicted:\", generated.T[i])\n",
    "                    print('------')\n",
    "        \n",
    "        coord.join(threads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/gcloud_seq2seq_v5/6/model.ckpt-83266\n"
     ]
    },
    {
     "ename": "NotFoundError",
     "evalue": "Key decoder/candidate_state/W not found in checkpoint\n\t [[Node: save/RestoreV2_12 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_12/tensor_names, save/RestoreV2_12/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_12', defined at:\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-135-ef9ffcb1523c>\", line 15, in <module>\n    tf.train.Saver().restore(sess, './models/gcloud_seq2seq_v5/6/model.ckpt-83266')\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1056, in __init__\n    self.build()\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1086, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 669, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Key decoder/candidate_state/W not found in checkpoint\n\t [[Node: save/RestoreV2_12 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_12/tensor_names, save/RestoreV2_12/shape_and_slices)]]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1038\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1039\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1040\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1020\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1021\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1022\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, type, value, traceback)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                 \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/framework/errors_impl.py\u001b[0m in \u001b[0;36mraise_exception_on_not_ok_status\u001b[0;34m()\u001b[0m\n\u001b[1;32m    465\u001b[0m           \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpywrap_tensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m           pywrap_tensorflow.TF_GetCode(status))\n\u001b[0m\u001b[1;32m    467\u001b[0m   \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key decoder/candidate_state/W not found in checkpoint\n\t [[Node: save/RestoreV2_12 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_12/tensor_names, save/RestoreV2_12/shape_and_slices)]]",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-135-ef9ffcb1523c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSeq2Seq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m7800\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_seq_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_seq_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSaver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'./models/gcloud_seq2seq_v5/6/model.ckpt-83266'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/training/saver.py\u001b[0m in \u001b[0;36mrestore\u001b[0;34m(self, sess, save_path)\u001b[0m\n\u001b[1;32m   1455\u001b[0m     \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Restoring parameters from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1456\u001b[0m     sess.run(self.saver_def.restore_op_name,\n\u001b[0;32m-> 1457\u001b[0;31m              {self.saver_def.filename_tensor_name: save_path})\n\u001b[0m\u001b[1;32m   1458\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1459\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    980\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 982\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    983\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1032\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1033\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1034\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode_def\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotFoundError\u001b[0m: Key decoder/candidate_state/W not found in checkpoint\n\t [[Node: save/RestoreV2_12 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_12/tensor_names, save/RestoreV2_12/shape_and_slices)]]\n\nCaused by op 'save/RestoreV2_12', defined at:\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 193, in _run_module_as_main\n    \"__main__\", mod_spec)\n  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/runpy.py\", line 85, in _run_code\n    exec(code, run_globals)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/__main__.py\", line 3, in <module>\n    app.launch_new_instance()\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/traitlets/config/application.py\", line 658, in launch_instance\n    app.start()\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/kernelapp.py\", line 474, in start\n    ioloop.IOLoop.instance().start()\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/zmq/eventloop/ioloop.py\", line 177, in start\n    super(ZMQIOLoop, self).start()\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tornado/ioloop.py\", line 887, in start\n    handler_func(fd_obj, events)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 440, in _handle_events\n    self._handle_recv()\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 472, in _handle_recv\n    self._run_callback(callback, msg)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/zmq/eventloop/zmqstream.py\", line 414, in _run_callback\n    callback(*args, **kwargs)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tornado/stack_context.py\", line 275, in null_wrapper\n    return fn(*args, **kwargs)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 276, in dispatcher\n    return self.dispatch_shell(stream, msg)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 228, in dispatch_shell\n    handler(stream, idents, msg)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/kernelbase.py\", line 390, in execute_request\n    user_expressions, allow_stdin)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/ipkernel.py\", line 196, in do_execute\n    res = shell.run_cell(code, store_history=store_history, silent=silent)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/zmqshell.py\", line 501, in run_cell\n    return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2717, in run_cell\n    interactivity=interactivity, compiler=compiler, result=result)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2827, in run_ast_nodes\n    if self.run_code(code, result):\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/IPython/core/interactiveshell.py\", line 2881, in run_code\n    exec(code_obj, self.user_global_ns, self.user_ns)\n  File \"<ipython-input-135-ef9ffcb1523c>\", line 15, in <module>\n    tf.train.Saver().restore(sess, './models/gcloud_seq2seq_v5/6/model.ckpt-83266')\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1056, in __init__\n    self.build()\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 1086, in build\n    restore_sequentially=self._restore_sequentially)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 691, in build\n    restore_sequentially, reshape)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 407, in _AddRestoreOps\n    tensors = self.restore_op(filename_tensor, saveable, preferred_shard)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/training/saver.py\", line 247, in restore_op\n    [spec.tensor.dtype])[0])\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/ops/gen_io_ops.py\", line 669, in restore_v2\n    dtypes=dtypes, name=name)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py\", line 768, in apply_op\n    op_def=op_def)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 2336, in create_op\n    original_op=self._default_original_op, op_def=op_def)\n  File \"/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/framework/ops.py\", line 1228, in __init__\n    self._traceback = _extract_stack()\n\nNotFoundError (see above for traceback): Key decoder/candidate_state/W not found in checkpoint\n\t [[Node: save/RestoreV2_12 = RestoreV2[dtypes=[DT_FLOAT], _device=\"/job:localhost/replica:0/task:0/cpu:0\"](_recv_save/Const_0, save/RestoreV2_12/tensor_names, save/RestoreV2_12/shape_and_slices)]]\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "sess = tf.InteractiveSession(graph=graph)\n",
    "\n",
    "input_seq = tf.placeholder(tf.int32, [None, None])\n",
    "target_seq = tf.placeholder(tf.int32, [None, None])\n",
    "input_seq_len = tf.placeholder(tf.int32, [None])\n",
    "target_seq_len = tf.placeholder(tf.int32, [None])\n",
    "history = tf.placeholder(tf.int32, [None, None, None])\n",
    "history_size = tf.placeholder(tf.int32, [None])\n",
    "history_seq_len = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "\n",
    "model = Seq2Seq(500, [7800,300])\n",
    "model.graph(input_seq, target_seq, input_seq_len, target_seq_len, history, history_size, history_seq_len)\n",
    "tf.train.Saver().restore(sess, './models/gcloud_seq2seq_v5/6/model.ckpt-83266')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import nltk\n",
    "word_index = json.load(open('./data/frames/word_index.json'))\n",
    "dictionary = {}\n",
    "\n",
    "for word, idx in word_index.items():\n",
    "    dictionary[idx] = word\n",
    "\n",
    "def parse_message(message):\n",
    "    tokens = nltk.word_tokenize(message)\n",
    "    \n",
    "    return np.array([word_index[token] if token in word_index else word_index['<UNK>'] for token in tokens])\n",
    "\n",
    "def pad_sequences(values, length, add_eos=False):\n",
    "    result = []\n",
    "    for row in values:\n",
    "        row_value = list(row)\n",
    "        if add_eos:\n",
    "            row_value.append(1)\n",
    "        if len(row_value) < length:\n",
    "            row_value.extend([0] * (length - len(row_value)))\n",
    "        if len(row_value) > length:\n",
    "            if add_eos:\n",
    "                row_value = row_value[:length-1] + [1]\n",
    "            else:\n",
    "                row_value = row_value[:length]\n",
    "        result.append(row_value)\n",
    "    return np.array(result)\n",
    "\n",
    "def conversation_input(message, history_messages):\n",
    "    history = [parse_message(m) for m in history_messages]\n",
    "    history_size = [len(history)]\n",
    "    history = np.array([pad_sequences(history, max(map(len, history)))]).swapaxes(0,1)\n",
    "    message = np.array([parse_message(message)]).T\n",
    "    \n",
    "    return message, [message.shape[0]], history, history_size\n",
    "\n",
    "def response(message, history_messages):\n",
    "    message, message_len, history, history_size = conversation_input(message, history_messages)\n",
    "    \n",
    "    output, output_seq_len = sess.run([model.decoder_embedding_ids, model._target_seq_len], feed_dict={\n",
    "        model._input_seq: message,\n",
    "        model._input_seq_len: message_len,\n",
    "        model._target_seq_len: np.array(message_len) + 10,\n",
    "        model._history: history,\n",
    "        model._history_size: history_size\n",
    "    })\n",
    "    \n",
    "    print(output_seq_len)\n",
    "    \n",
    "    return ' '.join([dictionary[i] for i in output.reshape(-1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-5d0c4dc2b20e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mresponse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'business'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Hi'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Hello'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'How much does it cost to fly to bahamas?'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Economy class or business?'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-86-d5c09e732133>\u001b[0m in \u001b[0;36mresponse\u001b[0;34m(message, history_messages)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_target_seq_len\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage_len\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_history\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_history_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mhistory_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     })\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    776\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 778\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    779\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    780\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    912\u001b[0m     \u001b[0;31m# Check session.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Attempted to use a closed Session.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    915\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "response('business', ['Hi', 'Hello', 'How much does it cost to fly to bahamas?', 'Economy class or business?'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
