{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as tf_rnn\n",
    "import json\n",
    "from jamesbot.utils.padding import pad_sequences, pad_eos_indices\n",
    "from tensorflow.contrib import seq2seq\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "word_dict = json.load(open('./data/movie_dialogs/word_dict.json'))\n",
    "embeddings = json.load(open('./data/movie_dialogs/word_embeddings.json'))\n",
    "train_data = json.load(open('./data/movie_dialogs/samples_train.json'))\n",
    "corpus_filtered = json.load(open('./data/movie_dialogs/corpus_filtered.json'))\n",
    "\n",
    "word_index = {val: key for (key, val) in word_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def embed(text):\n",
    "    return [word_dict.get(token, 2) for token in nltk.word_tokenize(str(text).lower())]\n",
    "\n",
    "def decode(token_ids):\n",
    "    return ' '.join([word_index[token_id] for token_id in token_ids[:-1]])\n",
    "\n",
    "def discount_rewards(rewards, discount_factor=0.8):\n",
    "    result = np.zeros(len(rewards))\n",
    "    total_reward = 0.0\n",
    "    \n",
    "    for i in reversed(range(len(rewards))):\n",
    "        total_reward = discount_factor*total_reward + rewards[i]\n",
    "        result[i] = total_reward\n",
    "    \n",
    "    return result     \n",
    "\n",
    "def gather_token_probabilities(decoder_outputs, token_ids):\n",
    "    batch_size, n_tokens, _ = tf.unstack(tf.shape(decoder_outputs))\n",
    "\n",
    "    gather_indices = tf.concat([\n",
    "        tf.tile(tf.reshape(tf.range(batch_size), [batch_size, 1, 1]), [1, n_tokens, 1]),\n",
    "        tf.tile(tf.reshape(tf.range(n_tokens), [1, n_tokens, 1]), [batch_size, 1, 1]),\n",
    "        tf.expand_dims(token_ids, -1)\n",
    "    ], -1)\n",
    "\n",
    "\n",
    "    return tf.gather_nd(decoder_outputs, gather_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference\n",
      "Embeddings: [9858, 300]\n",
      "INFO:tensorflow:Restoring parameters from ./models/gcloud_simple_agent_15/model.ckpt-19\n",
      "Inference\n",
      "Embeddings: [9858, 300]\n",
      "INFO:tensorflow:Restoring parameters from ./models/gcloud_simple_agent_15/model.ckpt-19\n"
     ]
    }
   ],
   "source": [
    "class ChatbotAgent:\n",
    "    \n",
    "    def __init__(self, inputs, inputs_length, previous_outputs, previous_outputs_length, targets=None, targets_length=None, word_embeddings_shape=None, cell_size=300, train=False, decoder_sampling_p = 0.0):\n",
    "        assert word_embeddings_shape is not None\n",
    "        if train:\n",
    "            assert targets is not None\n",
    "            assert targets_length is not None\n",
    "        \n",
    "        self._inputs = inputs\n",
    "        self._inputs_length = inputs_length\n",
    "        self._previous_outputs = previous_outputs\n",
    "        self._previous_outputs_length = previous_outputs_length\n",
    "        \n",
    "        self._targets = targets\n",
    "        self._targets_length = targets_length\n",
    "        \n",
    "        self._word_embeddings_shape = list(word_embeddings_shape)\n",
    "        self._cell_size = int(cell_size)\n",
    "        self._train = bool(train)\n",
    "        self._decoder_sampling_p = decoder_sampling_p\n",
    "        \n",
    "        self._embeddings_module()\n",
    "        self._input_encoder_module()\n",
    "        self._output_decoder_module()\n",
    "        \n",
    "        print('Embeddings:', self._word_embeddings_shape)\n",
    "    \n",
    "    def _base_rnn_cell(self):\n",
    "        return tf_rnn.GRUCell(self._cell_size, activation=tf.nn.tanh)\n",
    "    \n",
    "    def _encoder(self, inputs, inputs_length, reuse=False):\n",
    "        with tf.variable_scope('encoder', reuse = reuse):\n",
    "            _birnn_outputs, _ = tf.nn.bidirectional_dynamic_rnn(\n",
    "                self._base_rnn_cell(), self._base_rnn_cell(),\n",
    "                inputs = inputs,\n",
    "                sequence_length = inputs_length,\n",
    "                dtype = tf.float32\n",
    "            )\n",
    "            \n",
    "            _outputs, _state = tf.nn.dynamic_rnn(\n",
    "                self._base_rnn_cell(),\n",
    "                inputs = tf.concat(_birnn_outputs, -1),\n",
    "                sequence_length = inputs_length,\n",
    "                dtype = tf.float32\n",
    "            )\n",
    "            \n",
    "            return _outputs, _state\n",
    "    \n",
    "    def _embeddings_module(self):\n",
    "        with tf.name_scope('embeddings_module'):\n",
    "            self._word_embeddings = tf.Variable(tf.random_normal(self._word_embeddings_shape, stddev=.3), trainable=True)\n",
    "            \n",
    "            self._inputs_embedded = tf.nn.embedding_lookup(self._word_embeddings, self._inputs)\n",
    "            self._previous_outputs_embedded = tf.nn.embedding_lookup(self._word_embeddings, self._previous_outputs)\n",
    "    \n",
    "    def _input_encoder_module(self):\n",
    "        with tf.name_scope('input_encoder'):\n",
    "            self._input_encoder_outputs, self._input_encoder_state = self._encoder(self._inputs_embedded, self._inputs_length)\n",
    "            \n",
    "        with tf.name_scope('previous_output_encoder'):\n",
    "            self._previous_output_encoder_outputs, self._previous_output_encoder_state = self._encoder(self._previous_outputs_embedded, self._previous_outputs_length, reuse=True)\n",
    "\n",
    "    def _output_decoder_module(self):\n",
    "        with tf.variable_scope('output_decoder_module'):\n",
    "            batch_size, _ = tf.unstack(tf.shape(self._inputs))\n",
    "\n",
    "            if self._train:\n",
    "                # Training helper\n",
    "                embedded_targets = tf.nn.embedding_lookup(self._word_embeddings, self._targets)\n",
    "                helper = seq2seq.ScheduledEmbeddingTrainingHelper(\n",
    "                    inputs = embedded_targets,\n",
    "                    sequence_length = self._targets_length,\n",
    "                    embedding = self._word_embeddings,\n",
    "                    sampling_probability = self._decoder_sampling_p\n",
    "                )  \n",
    "            else:\n",
    "                # Inference helper\n",
    "                print('Inference')\n",
    "                helper = seq2seq.GreedyEmbeddingHelper(\n",
    "                    embedding = self._word_embeddings,\n",
    "                    start_tokens = tf.tile([0], [batch_size]),\n",
    "                    end_token = 1\n",
    "                )\n",
    "\n",
    "            cell, initial_state = self._decoder_cell()            \n",
    "            decoder = seq2seq.BasicDecoder(\n",
    "                cell = cell,\n",
    "                helper = helper,\n",
    "                initial_state = initial_state,\n",
    "                output_layer = Dense(self._word_embeddings_shape[0])\n",
    "            )\n",
    "            \n",
    "            decoder_outputs, _, _ = seq2seq.dynamic_decode(\n",
    "                decoder = decoder,\n",
    "                impute_finished = True,\n",
    "                maximum_iterations = 51\n",
    "            )\n",
    "            self._decoder_logits = decoder_outputs.rnn_output\n",
    "            self.decoder_token_p = tf.nn.softmax(self._decoder_logits, -1)\n",
    "            self.decoder_token_ids = tf.argmax(self._decoder_logits, -1, output_type=tf.int32)\n",
    "    \n",
    "    def _decoder_cell(self):\n",
    "        batch_size, _ = tf.unstack(tf.shape(self._inputs))\n",
    "        \n",
    "        # Project encoder state to lower dimensionality\n",
    "        _encoder_state_proj = tf.layers.dense(\n",
    "            tf.concat([self._input_encoder_state, self._previous_output_encoder_state], -1),\n",
    "            self._cell_size\n",
    "        )\n",
    "\n",
    "        _attention_mechanism = seq2seq.BahdanauAttention(\n",
    "            num_units = self._cell_size,\n",
    "            memory = self._input_encoder_outputs,\n",
    "            memory_sequence_length = self._inputs_length\n",
    "        )\n",
    "        \n",
    "        _attentive_cell = seq2seq.AttentionWrapper(\n",
    "            cell = self._base_rnn_cell(),\n",
    "            attention_mechanism = _attention_mechanism,\n",
    "            attention_layer_size = self._cell_size,\n",
    "            initial_cell_state = _encoder_state_proj\n",
    "        )\n",
    "        \n",
    "        stacked_cell = tf_rnn.MultiRNNCell([\n",
    "            _attentive_cell,\n",
    "            self._base_rnn_cell()\n",
    "        ])        \n",
    "        \n",
    "        initial_state = tuple([\n",
    "            _attentive_cell.zero_state(batch_size, dtype=tf.float32),\n",
    "            _encoder_state_proj\n",
    "        ])\n",
    "        \n",
    "        return stacked_cell, initial_state\n",
    "    \n",
    "\n",
    "class ChatbotAgentRunner:\n",
    "    \n",
    "    def __init__(self, embeddings_shape, checkpoint_path):\n",
    "        self._graph = tf.Graph()\n",
    "        self._sess = tf.InteractiveSession(graph=self._graph)\n",
    "        \n",
    "        with self._graph.as_default():\n",
    "            self.inputs = tf.placeholder(tf.int32, [None, None])\n",
    "            self.inputs_length = tf.placeholder(tf.int32, [None])\n",
    "            self.previous_outputs = tf.placeholder(tf.int32, [None, None])\n",
    "            self.previous_outputs_length = tf.placeholder(tf.int32, [None])\n",
    "            \n",
    "            self.agent = ChatbotAgent(\n",
    "                self.inputs, self.inputs_length, self.previous_outputs, self.previous_outputs_length,\n",
    "                word_embeddings_shape=embeddings_shape, train=False,\n",
    "            )\n",
    "            \n",
    "            self._update_ops()\n",
    "            \n",
    "            saver = tf.train.Saver()\n",
    "            saver.restore(self._sess, checkpoint_path)\n",
    "            \n",
    "    def _update_ops(self):\n",
    "        self.responses = tf.placeholder(tf.int32, [None, None])\n",
    "        self.responses_length = tf.placeholder(tf.int32, [None])\n",
    "        self.rewards = tf.placeholder(tf.float32, [None])\n",
    "        \n",
    "        token_mask = tf.sequence_mask(self.responses_length, tf.reduce_max(self.responses_length), dtype=tf.float32)\n",
    "        self._token_probabilities = tf.log(gather_token_probabilities(self.agent.decoder_token_p, self.responses)) * token_mask\n",
    "        \n",
    "        self.loss = -tf.reduce_mean(self._token_probabilities * tf.expand_dims(self.rewards, -1))\n",
    "        \n",
    "            \n",
    "    def run(self, inputs, previous_outputs):\n",
    "        ops = [\n",
    "            self.agent.decoder_token_ids,\n",
    "            self.agent.decoder_token_p,\n",
    "            tf.concat([\n",
    "                self.agent._input_encoder_state,\n",
    "                self.agent._previous_output_encoder_state\n",
    "            ], -1)\n",
    "        ]\n",
    "        \n",
    "        fd = {\n",
    "            self.inputs: inputs,\n",
    "            self.inputs_length: list(map(len, inputs)),\n",
    "            self.previous_outputs: previous_outputs,\n",
    "            self.previous_outputs_length: list(map(len, previous_outputs))   \n",
    "        }\n",
    "        \n",
    "        return self._sess.run(ops, feed_dict=fd)\n",
    "\n",
    "class Trainer:\n",
    "    pass\n",
    "    \n",
    "class CrossEntropyTrainer(Trainer):\n",
    "    \n",
    "    def __init__(self, embeddings_shape):\n",
    "        self.inputs = tf.placeholder(tf.int32, [None, None])\n",
    "        self.inputs_length = tf.placeholder(tf.int32, [None])\n",
    "        self.previous_outputs = tf.placeholder(tf.int32, [None, None])\n",
    "        self.previous_outputs_length = tf.placeholder(tf.int32, [None])\n",
    "        \n",
    "        self.targets = tf.placeholder(tf.int32, [None, None])\n",
    "        self.targets_length = tf.placeholder(tf.int32, [None])\n",
    "        self._padded_targets = pad_eos_indices(self.targets, self.targets_length)\n",
    "        self._padded_targets_length = self.targets_length + 2\n",
    "        \n",
    "        self.agent = ChatbotAgent(\n",
    "            self.inputs, self.inputs_length, self.previous_outputs, self.previous_outputs_length,\n",
    "            self._padded_targets, self._padded_targets_length,\n",
    "            word_embeddings_shape=embeddings_shape, train=True,\n",
    "            decoder_sampling_p=0.0\n",
    "        )\n",
    "        \n",
    "        self._build_loss()\n",
    "    \n",
    "    def embeddings_initializer(self):\n",
    "        placeholder = tf.placeholder(tf.float32)\n",
    "        init_op = self.agent._word_embeddings.assign(placeholder)\n",
    "        return placeholder, init_op\n",
    "        \n",
    "    def _build_loss(self):\n",
    "        loss_targets = pad_eos_indices(self.targets, self.targets_length, prepad=False)\n",
    "        \n",
    "        stepwise_ce = tf.nn.softmax_cross_entropy_with_logits(\n",
    "            labels = tf.one_hot(loss_targets, self.agent._word_embeddings_shape[0]),\n",
    "            logits = self.agent._decoder_logits\n",
    "        )\n",
    "        self.loss = tf.reduce_mean(stepwise_ce)\n",
    "        \n",
    "        self.accuracy = tf.reduce_mean(tf.reduce_mean(tf.cast(tf.equal(loss_targets, self.agent.decoder_token_ids), tf.float32), -1))\n",
    "        \n",
    "        tf.summary.scalar('loss', self.loss)\n",
    "        tf.summary.scalar('accuracy', self.accuracy)\n",
    "        \n",
    "class A2CTrainer(Trainer):\n",
    "    \n",
    "    DULL_SET = [\n",
    "        \"I don't know what you're talking about.\",\n",
    "        \"I don't know.\",\n",
    "        \"You don't know.\",\n",
    "        \"You know what I mean.\",\n",
    "        \"I know what you mean.\",\n",
    "        \"You know what I'm saying.\",\n",
    "        \"You don't know anything.\",\n",
    "        \"I'm not sure.\"\n",
    "    ]\n",
    "    \n",
    "    LR = 1e-2\n",
    "    \n",
    "    def __init__(self, embeddings_shape, seq2seq_checkpoint):\n",
    "        self._seq2seq_checkpoint = seq2seq_checkpoint\n",
    "        self._dull_set_embedded = list(map(embed, self.DULL_SET))\n",
    "        self._previous_state = None    \n",
    "            \n",
    "        self._agent1 = ChatbotAgentRunner(embeddings_shape, seq2seq_checkpoint)\n",
    "        self._agent2 = ChatbotAgentRunner(embeddings_shape, seq2seq_checkpoint)\n",
    "        \n",
    "        with self._agent1._graph.as_default():\n",
    "            self._agent1_train = tf.train.GradientDescentOptimizer(learning_rate=self.LR).minimize(self._agent1.loss)\n",
    "            \n",
    "        with self._agent2._graph.as_default():\n",
    "            self._agent2_train = tf.train.GradientDescentOptimizer(learning_rate=self.LR).minimize(self._agent2.loss)\n",
    "            \n",
    "    def _dull_response_reward(self, decoder_token_p):\n",
    "        result = []\n",
    "        for tokens in decoder_token_p:\n",
    "            n_tokens = tokens.shape[0] - 1\n",
    "            dull_p = []\n",
    "            \n",
    "            for dull_seq in self._dull_set_embedded:\n",
    "                if n_tokens > len(dull_seq):\n",
    "                    dull_seq_p = tokens[range(len(dull_seq)), dull_seq]\n",
    "                else:\n",
    "                    dull_seq_p = tokens[range(n_tokens), dull_seq[:n_tokens]]\n",
    "\n",
    "                dull_p.append(np.log(np.prod(dull_seq_p) + 1e-12) / len(dull_seq))\n",
    "\n",
    "            result.append(-np.sum(dull_p) / len(self._dull_set_embedded))\n",
    "            \n",
    "        if np.any(np.isnan(result)):\n",
    "            print('dull:', result)\n",
    "        \n",
    "        return np.asarray(result, dtype=np.float32)\n",
    "    \n",
    "    def _information_flow_reward(self, previous_state, state):\n",
    "        if previous_state is None:\n",
    "            return np.asarray([1.0]*state.shape[0])\n",
    "        \n",
    "        vec1_norm = np.sqrt(np.sum(np.square(previous_state), axis=-1))\n",
    "        vec2_norm = np.sqrt(np.sum(np.square(state), axis=-1))\n",
    "        \n",
    "        result = -np.log(np.clip(np.dot(previous_state, state.T) / (vec1_norm * vec2_norm), 1e-12, np.inf))\n",
    "        \n",
    "        if np.any(np.isnan(result)):\n",
    "            print('info:', result, vec1_norm, vec2_norm, np.sum(previous_state * state, axis=-1))\n",
    "            \n",
    "        return result\n",
    "\n",
    "    def _num_unk_reward(self, decoder_token_p):\n",
    "        token_ids = np.argmax(decoder_token_p, axis=-1)\n",
    "        \n",
    "        return token_ids[token_ids == 2].sum()\n",
    "\n",
    "    def _reward(self, decoder_token_p, previous_state, state):\n",
    "        dull_response_reward = self._dull_response_reward(decoder_token_p)\n",
    "        information_flow_reward = self._information_flow_reward(previous_state, state)\n",
    "        unk_reward = self._num_unk_reward(decoder_token_p)\n",
    "        \n",
    "        return 0.25*dull_response_reward + 0.25*information_flow_reward - 0.25*unk_reward\n",
    "    \n",
    "    def _append_agent_state(self, agent, inputs, previous_output, response, state, reward):\n",
    "        self._state[agent]['inputs'].append(list(inputs))\n",
    "        self._state[agent]['previous_outputs'].append(list(previous_output))\n",
    "        self._state[agent]['responses'].append(list(response))\n",
    "        self._state[agent]['rewards'].append(reward)\n",
    "        self._state[agent]['previous_state'] = state\n",
    "        \n",
    "    def _parse_states(self, agent):\n",
    "        previous_outputs = self._state[agent]['previous_outputs']\n",
    "        previous_outputs_length = list(map(len, previous_outputs))\n",
    "        inputs = self._state[agent]['inputs']\n",
    "        inputs_length = list(map(len, inputs))\n",
    "        responses = self._state[agent]['responses']\n",
    "        responses_length = list(map(len, responses))\n",
    "        \n",
    "        return {\n",
    "            'previous_outputs': pad_sequences(previous_outputs, max(previous_outputs_length)),\n",
    "            'previous_outputs_length': previous_outputs_length,\n",
    "            'inputs': pad_sequences(inputs, max(inputs_length)),\n",
    "            'inputs_length': inputs_length,\n",
    "            'responses': pad_sequences(responses, max(responses_length)),\n",
    "            'responses_length': responses_length,\n",
    "            'discounted_rewards': discount_rewards(self._state[agent]['rewards'])\n",
    "        }   \n",
    "    \n",
    "    def reset(self):\n",
    "        self._state = {\n",
    "            agent: {\n",
    "                'inputs': [],\n",
    "                'previous_outputs': [],\n",
    "                'responses': [],\n",
    "                'rewards': [],\n",
    "                'previous_state': None\n",
    "            } for agent in ['agent_1', 'agent_2']\n",
    "        }\n",
    "\n",
    "    def simulate(self, inputs, display=True):\n",
    "        if display:\n",
    "            print('Initial:', decode(inputs[0]))\n",
    "        \n",
    "        ag1_output, token_p, state = self._agent1.run(inputs, [[0]])\n",
    "        self._append_agent_state('agent_1', inputs[0], [0], ag1_output[0], state, self._reward(token_p, self._state['agent_1']['previous_state'], state)[0])\n",
    "        if display:\n",
    "            print('Agent 1:', decode(ag1_output[0]))\n",
    "        \n",
    "        ag2_output, token_p, state = self._agent2.run(ag1_output, inputs)\n",
    "        self._append_agent_state('agent_2', ag1_output[0], inputs[0], ag2_output[0], state, self._reward(token_p, self._state['agent_2']['previous_state'], state)[0])\n",
    "        if display:\n",
    "            print('Agent 2:', decode(ag2_output[0]))\n",
    "            print('----------------------------')\n",
    "            \n",
    "        for i in range(np.random.randint(5, 12)):\n",
    "            ag1_previous_output = ag1_output\n",
    "            ag1_output, token_p, state = self._agent1.run(ag2_output, ag1_previous_output)\n",
    "            self._append_agent_state('agent_1', ag2_output[0], ag1_previous_output[0], ag1_output[0], state, self._reward(token_p, self._state['agent_1']['previous_state'], state)[0])\n",
    "            if display:\n",
    "                print('Agent 1:', decode(ag1_output[0]))\n",
    "            \n",
    "            ag2_previous_output = ag2_output\n",
    "            ag2_output, token_p, state = self._agent2.run(ag1_output, ag2_output)\n",
    "            self._append_agent_state('agent_2', ag1_output[0], ag2_previous_output[0], ag2_output[0], state, self._reward(token_p, self._state['agent_2']['previous_state'], state)[0])\n",
    "            if display:\n",
    "                print('Agent 2:', decode(ag2_output[0]))\n",
    "                print('----------------------------')  \n",
    "    \n",
    "    def update(self):\n",
    "        agent = self._agent1\n",
    "        state = self._parse_states('agent_1')\n",
    "    \n",
    "        _, loss_val = agent._sess.run([self._agent1_train, self._agent1.loss], feed_dict = {\n",
    "            agent.inputs: state['inputs'],\n",
    "            agent.inputs_length: state['inputs_length'],\n",
    "            agent.previous_outputs: state['previous_outputs'],\n",
    "            agent.previous_outputs_length: state['previous_outputs_length'],\n",
    "            agent.responses: state['responses'],\n",
    "            agent.responses_length: state['responses_length'],\n",
    "            agent.rewards: state['discounted_rewards']\n",
    "        })\n",
    "        \n",
    "        print('Agent 1 - loss:', loss_val, 'total reward:', np.sum(state['discounted_rewards']))\n",
    "        \n",
    "        agent = self._agent2\n",
    "        state = self._parse_states('agent_2')\n",
    "    \n",
    "        _, loss_val = agent._sess.run([self._agent2_train, self._agent2.loss], feed_dict = {\n",
    "            agent.inputs: state['inputs'],\n",
    "            agent.inputs_length: state['inputs_length'],\n",
    "            agent.previous_outputs: state['previous_outputs'],\n",
    "            agent.previous_outputs_length: state['previous_outputs_length'],\n",
    "            agent.responses: state['responses'],\n",
    "            agent.responses_length: state['responses_length'],\n",
    "            agent.rewards: state['discounted_rewards']\n",
    "        })\n",
    "        \n",
    "        print('Agent 2 - loss:', loss_val, 'total reward:', np.sum(state['discounted_rewards']))\n",
    "        \n",
    "trainer = A2CTrainer([len(embeddings), 300], './models/gcloud_simple_agent_15/model.ckpt-19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent 1 - loss: 1.19111 total reward: 50.9938213825\n",
      "Agent 2 - loss: 1.74954 total reward: 44.6959036589\n",
      "Agent 1 - loss: 0.932775 total reward: 24.4135916233\n",
      "Agent 2 - loss: 1.49049 total reward: 24.332575202\n",
      "Agent 1 - loss: 1.1643 total reward: 46.1533446312\n",
      "Agent 2 - loss: 0.420521 total reward: 29.7255089283\n",
      "Agent 1 - loss: 0.893503 total reward: 19.075879097\n",
      "Agent 2 - loss: 1.29521 total reward: 21.700063467\n",
      "Agent 1 - loss: 1.45319 total reward: 24.724006772\n",
      "Agent 2 - loss: 0.224982 total reward: 12.8042414784\n",
      "Agent 1 - loss: 1.6543 total reward: 19.9990317822\n",
      "Agent 2 - loss: 1.00361 total reward: 14.936003089\n",
      "Agent 1 - loss: 1.14114 total reward: 22.4859746695\n",
      "Agent 2 - loss: 1.17767 total reward: 18.1365824938\n",
      "Agent 1 - loss: 1.4884 total reward: 44.5918121338\n",
      "Agent 2 - loss: 1.02417 total reward: 41.6990408897\n",
      "Agent 1 - loss: 1.49253 total reward: 42.5035976171\n",
      "Agent 2 - loss: -0.518873 total reward: 5.13156783581\n",
      "Agent 1 - loss: 1.11955 total reward: 35.2150957584\n",
      "Agent 2 - loss: 1.494 total reward: 40.6314425468\n",
      "Agent 1 - loss: 0.880483 total reward: 38.5286978334\n",
      "Agent 2 - loss: 1.23778 total reward: 44.9239524603\n",
      "Agent 1 - loss: 0.990942 total reward: 29.4332829118\n",
      "Agent 2 - loss: 1.62487 total reward: 33.2207129002\n",
      "Agent 1 - loss: 1.32616 total reward: 16.9144037962\n",
      "Agent 2 - loss: 1.42486 total reward: 19.554759264\n",
      "Agent 1 - loss: 0.842035 total reward: 27.2178771496\n",
      "Agent 2 - loss: 1.95706 total reward: 33.1110367775\n",
      "Agent 1 - loss: 0.645057 total reward: 18.553768754\n",
      "Agent 2 - loss: 1.22463 total reward: 23.724678278\n",
      "Agent 1 - loss: 0.543504 total reward: 24.8168119788\n",
      "Agent 2 - loss: 0.961587 total reward: 27.4183856249\n",
      "Agent 1 - loss: 0.618333 total reward: 38.8956241608\n",
      "Agent 2 - loss: 1.59376 total reward: 47.4177455902\n",
      "Agent 1 - loss: 0.801987 total reward: 19.9332232475\n",
      "Agent 2 - loss: 1.42759 total reward: 19.4516586661\n",
      "Agent 1 - loss: 0.774854 total reward: 23.8942311406\n",
      "Agent 2 - loss: 0.606321 total reward: 19.6712935567\n",
      "Agent 1 - loss: 0.452095 total reward: 42.6133209467\n",
      "Agent 2 - loss: 0.95838 total reward: 37.9838222861\n",
      "Agent 1 - loss: 0.261923 total reward: 44.0911634564\n",
      "Agent 2 - loss: 0.596956 total reward: 38.5907137394\n",
      "Agent 1 - loss: 0.714481 total reward: 40.1136811972\n",
      "Agent 2 - loss: 1.22375 total reward: 36.8809610605\n",
      "Agent 1 - loss: 0.738878 total reward: 39.7838947773\n",
      "Agent 2 - loss: 0.605255 total reward: 43.7018327713\n",
      "Agent 1 - loss: 0.307838 total reward: 13.9371093512\n",
      "Agent 2 - loss: 1.23683 total reward: 18.2326329947\n",
      "Agent 1 - loss: 0.564503 total reward: 27.6986718178\n",
      "Agent 2 - loss: 0.365065 total reward: 23.9945748448\n",
      "Agent 1 - loss: 0.0975411 total reward: 32.3913829327\n",
      "Agent 2 - loss: 0.5249 total reward: 30.6847224236\n",
      "Agent 1 - loss: 0.916918 total reward: 55.6913762093\n",
      "Agent 2 - loss: 0.152743 total reward: 40.7008104324\n",
      "Agent 1 - loss: 0.361852 total reward: 44.8747669458\n",
      "Agent 2 - loss: 0.244523 total reward: 40.6307762265\n",
      "Agent 1 - loss: 0.47779 total reward: 35.2897174358\n",
      "Agent 2 - loss: 0.100057 total reward: 32.7897855639\n",
      "Agent 1 - loss: 0.2494 total reward: 44.8297451735\n",
      "Agent 2 - loss: 0.174066 total reward: 41.9434909225\n",
      "Agent 1 - loss: 0.234354 total reward: 30.5856740475\n",
      "Agent 2 - loss: 0.63697 total reward: 27.1091139317\n",
      "Agent 1 - loss: 0.46708 total reward: 29.7001287341\n",
      "Agent 2 - loss: 0.450693 total reward: 30.3517899513\n",
      "Agent 1 - loss: 0.547239 total reward: 21.9993385077\n",
      "Agent 2 - loss: 0.585883 total reward: 21.7000061274\n",
      "Agent 1 - loss: 0.350717 total reward: 41.2348353267\n",
      "Agent 2 - loss: 0.617095 total reward: 30.0351154804\n",
      "Agent 1 - loss: 0.688682 total reward: 25.7333790064\n",
      "Agent 2 - loss: 0.343671 total reward: 24.165679872\n",
      "Agent 1 - loss: 0.316368 total reward: 25.005446732\n",
      "Agent 2 - loss: 0.507678 total reward: 25.7070839405\n",
      "Agent 1 - loss: 0.433116 total reward: 44.8824651241\n",
      "Agent 2 - loss: 0.306041 total reward: 43.8011671901\n",
      "Agent 1 - loss: 0.327243 total reward: 34.2857320905\n",
      "Agent 2 - loss: 0.158166 total reward: 34.4414943457\n",
      "Agent 1 - loss: 0.647532 total reward: 16.3169012666\n",
      "Agent 2 - loss: 0.422922 total reward: 16.5790559053\n",
      "Agent 1 - loss: 0.284433 total reward: 43.8582122922\n",
      "Agent 2 - loss: 0.266268 total reward: 43.4932961464\n",
      "Agent 1 - loss: 0.527155 total reward: 20.5058763027\n",
      "Agent 2 - loss: 0.658695 total reward: 20.355080843\n",
      "Agent 1 - loss: 0.694442 total reward: 17.318381846\n",
      "Agent 2 - loss: 0.774947 total reward: 17.0263713598\n",
      "Agent 1 - loss: 0.732648 total reward: 16.920866549\n",
      "Agent 2 - loss: 0.288717 total reward: 16.3139646053\n",
      "Agent 1 - loss: 0.367009 total reward: 39.7054589987\n",
      "Agent 2 - loss: 0.291283 total reward: 38.9925031662\n",
      "Agent 1 - loss: 0.428895 total reward: 35.3932660818\n",
      "Agent 2 - loss: 0.122368 total reward: 31.8953955173\n",
      "Agent 1 - loss: 0.0231884 total reward: 25.8755681515\n",
      "Agent 2 - loss: 0.646114 total reward: 23.3195700645\n",
      "Agent 1 - loss: 0.0348605 total reward: 41.219173789\n",
      "Agent 2 - loss: 0.254605 total reward: 36.7740390301\n",
      "Agent 1 - loss: 0.428104 total reward: 31.728469491\n",
      "Agent 2 - loss: 0.309151 total reward: 28.2152051926\n",
      "Agent 1 - loss: 0.402051 total reward: 39.7700461149\n",
      "Agent 2 - loss: 0.138651 total reward: 38.8288888931\n",
      "Agent 1 - loss: 0.212258 total reward: 35.4127328992\n",
      "Agent 2 - loss: 0.130052 total reward: 33.7181019783\n",
      "Agent 1 - loss: 0.625057 total reward: 41.540826261\n",
      "Agent 2 - loss: 0.398717 total reward: 39.2454222441\n",
      "Agent 1 - loss: 0.421546 total reward: 27.0282051563\n",
      "Agent 2 - loss: 0.0986133 total reward: 24.392980516\n",
      "Agent 1 - loss: 0.967987 total reward: 28.0769311786\n",
      "Agent 2 - loss: 0.287639 total reward: 25.4569159746\n",
      "Agent 1 - loss: 0.519231 total reward: 31.4023743272\n",
      "Agent 2 - loss: 0.432842 total reward: 29.6641054153\n",
      "Agent 1 - loss: 0.595164 total reward: 41.4289807081\n",
      "Agent 2 - loss: 0.303091 total reward: 39.5263828039\n",
      "Agent 1 - loss: 0.622647 total reward: 30.612865448\n",
      "Agent 2 - loss: 0.423876 total reward: 30.096766293\n",
      "Agent 1 - loss: 0.0303403 total reward: 31.4714682102\n",
      "Agent 2 - loss: 0.171671 total reward: 29.7652078867\n",
      "Agent 1 - loss: 0.0201071 total reward: 41.3227491975\n",
      "Agent 2 - loss: 0.109527 total reward: 39.7223898172\n",
      "Agent 1 - loss: 0.324104 total reward: 40.1465318799\n",
      "Agent 2 - loss: 0.339373 total reward: 39.6959670782\n",
      "Agent 1 - loss: 0.500922 total reward: 42.6197041273\n",
      "Agent 2 - loss: 0.249303 total reward: 39.7626560926\n",
      "Agent 1 - loss: 0.55466 total reward: 18.0911191702\n",
      "Agent 2 - loss: 0.191444 total reward: 15.9583448768\n",
      "Agent 1 - loss: 0.0617921 total reward: 41.0242093801\n",
      "Agent 2 - loss: 0.0266746 total reward: 38.9543646574\n",
      "Agent 1 - loss: 0.331328 total reward: 18.0363310575\n",
      "Agent 2 - loss: 0.642621 total reward: 15.6845112443\n",
      "Agent 1 - loss: 0.738113 total reward: 21.7437630296\n",
      "Agent 2 - loss: 0.127614 total reward: 20.7510257959\n",
      "Agent 1 - loss: 0.0457531 total reward: 22.4610708356\n",
      "Agent 2 - loss: 0.123513 total reward: 21.713698864\n",
      "Agent 1 - loss: 0.710963 total reward: 18.8583078384\n",
      "Agent 2 - loss: 0.232964 total reward: 15.9377896786\n",
      "Agent 1 - loss: 0.359392 total reward: 17.8607637882\n",
      "Agent 2 - loss: 0.230396 total reward: 16.5155075788\n",
      "Agent 1 - loss: 0.541074 total reward: 33.9084465504\n",
      "Agent 2 - loss: 0.152564 total reward: 18.2579105496\n",
      "Agent 1 - loss: 0.526942 total reward: 27.7112207413\n",
      "Agent 2 - loss: 0.224922 total reward: 22.8346657753\n",
      "Agent 1 - loss: 0.116786 total reward: 42.2578716278\n",
      "Agent 2 - loss: 0.048012 total reward: 35.8772306442\n",
      "Agent 1 - loss: 0.657347 total reward: 49.4051320553\n",
      "Agent 2 - loss: 0.0753913 total reward: 40.5348343253\n",
      "Agent 1 - loss: 0.139751 total reward: 37.8647534847\n",
      "Agent 2 - loss: 0.0752943 total reward: 32.1572496891\n",
      "Agent 1 - loss: 0.291757 total reward: 38.5492064953\n",
      "Agent 2 - loss: 0.0520312 total reward: 31.8248403072\n",
      "Agent 1 - loss: 0.414097 total reward: 19.2839534283\n",
      "Agent 2 - loss: 0.191377 total reward: 15.0589580536\n",
      "Agent 1 - loss: 0.630611 total reward: 38.8394883871\n",
      "Agent 2 - loss: 0.194294 total reward: 31.8141768575\n",
      "Agent 1 - loss: 0.280049 total reward: 43.8132884502\n",
      "Agent 2 - loss: 0.0253464 total reward: 35.9826621413\n",
      "Agent 1 - loss: 0.194206 total reward: 23.2415118217\n",
      "Agent 2 - loss: 0.000730694 total reward: 19.1425793767\n",
      "Agent 1 - loss: 1.15113 total reward: 44.3707778454\n",
      "Agent 2 - loss: 0.142949 total reward: 27.4908562899\n",
      "Agent 1 - loss: 0.356223 total reward: 39.0011856556\n",
      "Agent 2 - loss: 0.10981 total reward: 30.902184546\n",
      "Agent 1 - loss: 0.859047 total reward: 27.2993088961\n",
      "Agent 2 - loss: 0.49584 total reward: 19.7843788862\n",
      "Agent 1 - loss: 0.562787 total reward: 18.8917803764\n",
      "Agent 2 - loss: 0.0495756 total reward: 15.7434350848\n",
      "Agent 1 - loss: 0.9364 total reward: 60.7464610338\n",
      "Agent 2 - loss: 0.00891207 total reward: 41.4744115472\n",
      "Agent 1 - loss: 0.441426 total reward: 23.7234392166\n",
      "Agent 2 - loss: 0.115853 total reward: 19.6937778592\n",
      "Agent 1 - loss: 1.35169 total reward: 30.363868475\n",
      "Agent 2 - loss: 0.0220754 total reward: 15.4601480961\n",
      "Agent 1 - loss: 0.578232 total reward: 33.8750994205\n",
      "Agent 2 - loss: 0.161675 total reward: 27.6672166586\n",
      "Agent 1 - loss: 1.06989 total reward: 34.400426507\n",
      "Agent 2 - loss: 0.0703945 total reward: 19.8284291029\n",
      "Agent 1 - loss: 0.822592 total reward: 19.2956379652\n",
      "Agent 2 - loss: 0.341685 total reward: 16.5323649645\n",
      "Agent 1 - loss: 0.271014 total reward: 38.8057409525\n",
      "Agent 2 - loss: 0.000426977 total reward: 32.9117157459\n",
      "Agent 1 - loss: 0.339222 total reward: 60.112506628\n",
      "Agent 2 - loss: 0.0758315 total reward: 42.2210005522\n",
      "Agent 1 - loss: 0.418956 total reward: 33.2604163885\n",
      "Agent 2 - loss: 0.309519 total reward: 29.1011180878\n",
      "Agent 1 - loss: 0.269527 total reward: 60.0426037312\n",
      "Agent 2 - loss: 0.0110508 total reward: 43.7521232367\n",
      "Agent 1 - loss: 0.809041 total reward: 54.6715826988\n",
      "Agent 2 - loss: 0.00811501 total reward: 38.4729671478\n",
      "Agent 1 - loss: 0.488101 total reward: 23.3043464422\n",
      "Agent 2 - loss: 0.0122964 total reward: 20.3606204987\n",
      "Agent 1 - loss: 0.279127 total reward: 38.2532508373\n",
      "Agent 2 - loss: 0.00379643 total reward: 33.8477246761\n",
      "Agent 1 - loss: 0.792119 total reward: 44.5965682268\n",
      "Agent 2 - loss: 0.0127637 total reward: 29.6087049246\n",
      "Agent 1 - loss: 0.343161 total reward: 54.7477172613\n",
      "Agent 2 - loss: 0.0230146 total reward: 38.732175231\n",
      "Agent 1 - loss: 0.427199 total reward: 43.987126112\n",
      "Agent 2 - loss: 0.00590392 total reward: 39.4443969727\n",
      "Agent 1 - loss: 0.380295 total reward: 38.0832964182\n",
      "Agent 2 - loss: 0.024409 total reward: 34.6314712763\n",
      "Agent 1 - loss: 0.300384 total reward: 39.5284664631\n",
      "Agent 2 - loss: 0.00629868 total reward: 34.2639155388\n",
      "Agent 1 - loss: 0.202655 total reward: 38.397895813\n",
      "Agent 2 - loss: 0.0334017 total reward: 34.4155657291\n",
      "Agent 1 - loss: 0.864027 total reward: 55.3130437136\n",
      "Agent 2 - loss: 0.0384483 total reward: 39.0576502085\n",
      "Agent 1 - loss: 0.0639502 total reward: 37.6305229664\n",
      "Agent 2 - loss: 0.00181195 total reward: 34.3374496698\n",
      "Agent 1 - loss: 0.173899 total reward: 49.7944253683\n",
      "Agent 2 - loss: 0.000782781 total reward: 43.887678504\n",
      "Agent 1 - loss: 0.561894 total reward: 38.4848613739\n",
      "Agent 2 - loss: 0.00665271 total reward: 34.4993835688\n",
      "Agent 1 - loss: 0.330739 total reward: 44.0091277361\n",
      "Agent 2 - loss: 0.00456088 total reward: 38.6717562675\n",
      "Agent 1 - loss: 0.333751 total reward: 50.4196305275\n",
      "Agent 2 - loss: 0.00116503 total reward: 43.5944509506\n",
      "Agent 1 - loss: 0.280077 total reward: 30.3652220964\n",
      "Agent 2 - loss: 0.00702448 total reward: 16.2604049444\n",
      "Agent 1 - loss: 0.426697 total reward: 28.3906313181\n",
      "Agent 2 - loss: 0.00124633 total reward: 24.9063928127\n",
      "Agent 1 - loss: 0.512021 total reward: 19.1065855026\n",
      "Agent 2 - loss: 0.000406756 total reward: 16.4633857012\n",
      "Agent 1 - loss: 0.158523 total reward: 49.4803277254\n",
      "Agent 2 - loss: 0.00718109 total reward: 33.9317973852\n",
      "Agent 1 - loss: 0.203892 total reward: 33.5985908508\n",
      "Agent 2 - loss: 0.000200243 total reward: 29.440936923\n",
      "Agent 1 - loss: 0.29142 total reward: 44.5747466087\n",
      "Agent 2 - loss: 0.00891002 total reward: 29.4457358122\n",
      "Agent 1 - loss: 0.0781419 total reward: 23.8055121899\n",
      "Agent 2 - loss: 0.00109177 total reward: 20.5843023062\n",
      "Agent 1 - loss: 0.156026 total reward: 23.4175391197\n",
      "Agent 2 - loss: 0.00206697 total reward: 20.4686328173\n",
      "Agent 1 - loss: 0.229352 total reward: 49.3359085321\n",
      "Agent 2 - loss: 0.000370809 total reward: 43.876357913\n",
      "Agent 1 - loss: 0.763245 total reward: 55.1735249758\n",
      "Agent 2 - loss: 0.000148058 total reward: 39.0580407381\n",
      "Agent 1 - loss: 0.0592001 total reward: 29.6604566574\n",
      "Agent 2 - loss: 0.00206697 total reward: 25.0993916988\n",
      "Agent 1 - loss: 0.216995 total reward: 44.0256067514\n",
      "Agent 2 - loss: 0.00379863 total reward: 39.0386976004\n",
      "Agent 1 - loss: 0.205225 total reward: 39.9516283274\n",
      "Agent 2 - loss: 0.00530356 total reward: 34.1437237263\n",
      "Agent 1 - loss: 0.254831 total reward: 50.016861558\n",
      "Agent 2 - loss: 0.0013202 total reward: 43.885897398\n",
      "Agent 1 - loss: 0.0116603 total reward: 23.4003947973\n",
      "Agent 2 - loss: 0.000904884 total reward: 20.5301287174\n",
      "Agent 1 - loss: 0.121564 total reward: 24.0921369791\n",
      "Agent 2 - loss: 0.000288427 total reward: 20.5804995298\n",
      "Agent 1 - loss: 0.0919577 total reward: 49.7863122225\n",
      "Agent 2 - loss: 0.00428544 total reward: 43.9476046562\n",
      "Agent 1 - loss: 0.0642004 total reward: 32.9252492189\n",
      "Agent 2 - loss: 0.00542632 total reward: 29.462215066\n",
      "Agent 1 - loss: 0.245675 total reward: 33.6884040833\n",
      "Agent 2 - loss: 0.0458689 total reward: 29.5281047821\n",
      "Agent 1 - loss: 0.0592108 total reward: 45.6627388\n",
      "Agent 2 - loss: 0.0022505 total reward: 39.0664697886\n",
      "Agent 1 - loss: 0.348157 total reward: 32.8940749168\n",
      "Agent 2 - loss: 0.00912299 total reward: 29.3809174299\n",
      "Agent 1 - loss: 0.0101173 total reward: 44.1597113609\n",
      "Agent 2 - loss: 0.00117039 total reward: 39.2705014944\n",
      "Agent 1 - loss: 0.0137933 total reward: 30.3315585852\n",
      "Agent 2 - loss: 0.000965199 total reward: 16.4724193811\n",
      "Agent 1 - loss: 0.212559 total reward: 30.3454768658\n",
      "Agent 2 - loss: 0.000518455 total reward: 16.596370101\n",
      "Agent 1 - loss: 1.09184 total reward: 34.5842410326\n",
      "Agent 2 - loss: 0.10345 total reward: 21.0439146757\n",
      "Agent 1 - loss: 0.0155071 total reward: 23.6201859713\n",
      "Agent 2 - loss: 0.00503421 total reward: 20.6376725435\n",
      "Agent 1 - loss: 0.321436 total reward: 38.5411550999\n",
      "Agent 2 - loss: 0.000575768 total reward: 34.420656085\n",
      "Agent 1 - loss: 0.00512546 total reward: 38.2742769718\n",
      "Agent 2 - loss: 0.000211034 total reward: 34.3133054972\n",
      "Agent 1 - loss: 0.345839 total reward: 49.427017808\n",
      "Agent 2 - loss: 0.000321352 total reward: 44.0518558025\n",
      "Agent 1 - loss: 0.208743 total reward: 33.2284556627\n",
      "Agent 2 - loss: 0.0130405 total reward: 29.5420058966\n",
      "Agent 1 - loss: 0.229994 total reward: 18.8250981569\n",
      "Agent 2 - loss: 9.26991e-05 total reward: 16.4757791758\n",
      "Agent 1 - loss: 0.0372036 total reward: 20.7947698832\n",
      "Agent 2 - loss: 0.0412188 total reward: 16.8117461205\n",
      "Agent 1 - loss: 0.0354691 total reward: 28.6347792149\n",
      "Agent 2 - loss: 0.000349097 total reward: 24.7704914808\n",
      "Agent 1 - loss: 0.0915206 total reward: 43.5019556284\n",
      "Agent 2 - loss: 0.000105528 total reward: 38.6599912643\n",
      "Agent 1 - loss: 0.0512454 total reward: 49.0652909279\n",
      "Agent 2 - loss: 8.92728e-05 total reward: 43.5258100033\n",
      "Agent 1 - loss: 0.0717206 total reward: 33.5657116175\n",
      "Agent 2 - loss: 0.0028788 total reward: 29.1735388041\n",
      "Agent 1 - loss: 0.355442 total reward: 35.0408542156\n",
      "Agent 2 - loss: 0.0662997 total reward: 20.2274650335\n",
      "Agent 1 - loss: 0.213893 total reward: 30.3386720419\n",
      "Agent 2 - loss: 0.000270512 total reward: 16.4431461096\n",
      "Agent 1 - loss: 0.0751815 total reward: 34.2609809637\n",
      "Agent 2 - loss: 0.000155289 total reward: 29.5889402628\n",
      "Agent 1 - loss: 0.00441538 total reward: 49.2046582699\n",
      "Agent 2 - loss: 0.000309103 total reward: 43.9166094065\n",
      "Agent 1 - loss: 0.0885568 total reward: 35.0120110512\n",
      "Agent 2 - loss: 0.0225185 total reward: 20.3785521984\n",
      "Agent 1 - loss: 0.418406 total reward: 34.8746056557\n",
      "Agent 2 - loss: 0.00239139 total reward: 20.758277297\n",
      "Agent 1 - loss: 0.0869164 total reward: 18.8773721457\n",
      "Agent 2 - loss: 0.0087623 total reward: 16.3731073141\n",
      "Agent 1 - loss: 0.00990407 total reward: 39.050753355\n",
      "Agent 2 - loss: 0.00023784 total reward: 34.4807310104\n",
      "Agent 1 - loss: 0.115162 total reward: 39.6980030537\n",
      "Agent 2 - loss: 0.00141552 total reward: 34.5359390974\n",
      "Agent 1 - loss: 0.106463 total reward: 30.3312587738\n",
      "Agent 2 - loss: 0.016883 total reward: 16.5631593466\n",
      "Agent 1 - loss: 0.068077 total reward: 55.1962631941\n",
      "Agent 2 - loss: 0.000289768 total reward: 39.3969472647\n",
      "Agent 1 - loss: 0.00453987 total reward: 34.8785450459\n",
      "Agent 2 - loss: 0.000216485 total reward: 29.8438383341\n",
      "Agent 1 - loss: 0.0879063 total reward: 44.2192003727\n",
      "Agent 2 - loss: 0.000106229 total reward: 39.3896996975\n",
      "Agent 1 - loss: 0.0152812 total reward: 19.4830815792\n",
      "Agent 2 - loss: 3.38357e-05 total reward: 16.6245203018\n",
      "Agent 1 - loss: 0.0126291 total reward: 28.2155669928\n",
      "Agent 2 - loss: 0.000747389 total reward: 25.2515048981\n",
      "Agent 1 - loss: 0.00875001 total reward: 44.7704856396\n",
      "Agent 2 - loss: 0.0201259 total reward: 29.8506624699\n",
      "Agent 1 - loss: 0.0638274 total reward: 49.8129090071\n",
      "Agent 2 - loss: 0.00144126 total reward: 44.1459937096\n",
      "Agent 1 - loss: 0.0108632 total reward: 43.950289011\n",
      "Agent 2 - loss: 3.38748e-05 total reward: 39.4292030334\n",
      "Agent 1 - loss: 0.0134528 total reward: 23.6082445383\n",
      "Agent 2 - loss: 0.00790273 total reward: 20.7043159008\n",
      "Agent 1 - loss: 0.0840451 total reward: 49.9211289883\n",
      "Agent 2 - loss: 0.000192529 total reward: 34.6638840437\n",
      "Agent 1 - loss: 0.017071 total reward: 49.7648717165\n",
      "Agent 2 - loss: 2.45967e-05 total reward: 44.3481470346\n",
      "Agent 1 - loss: 0.00171979 total reward: 49.7509186268\n",
      "Agent 2 - loss: 0.000245077 total reward: 44.2919915915\n",
      "Agent 1 - loss: 0.0176366 total reward: 19.0605556965\n",
      "Agent 2 - loss: 0.00229747 total reward: 16.6789802313\n",
      "Agent 1 - loss: 0.0178601 total reward: 28.6727852821\n",
      "Agent 2 - loss: 0.0124278 total reward: 25.2733470201\n",
      "Agent 1 - loss: 0.00166164 total reward: 43.9951561689\n",
      "Agent 2 - loss: 0.00152547 total reward: 39.2020533085\n",
      "Agent 1 - loss: 0.0111332 total reward: 19.1392427683\n",
      "Agent 2 - loss: 0.000157842 total reward: 16.6215618849\n",
      "Agent 1 - loss: 0.0214215 total reward: 23.9195824862\n",
      "Agent 2 - loss: 0.000344404 total reward: 20.7433236837\n",
      "Agent 1 - loss: 0.00413544 total reward: 34.1804161072\n",
      "Agent 2 - loss: 0.000643498 total reward: 29.7494254112\n",
      "Agent 1 - loss: 0.0289682 total reward: 28.6808381081\n",
      "Agent 2 - loss: 0.0252615 total reward: 25.3117319345\n",
      "Agent 1 - loss: 0.00632534 total reward: 43.8572198153\n",
      "Agent 2 - loss: 0.000367732 total reward: 39.0641361475\n",
      "Agent 1 - loss: 0.015493 total reward: 44.7450760603\n",
      "Agent 2 - loss: 0.000314187 total reward: 29.678029418\n",
      "Agent 1 - loss: 0.218646 total reward: 23.8664588928\n",
      "Agent 2 - loss: 0.00236862 total reward: 20.7507972717\n",
      "Agent 1 - loss: 0.0724858 total reward: 30.3214643002\n",
      "Agent 2 - loss: 0.00214869 total reward: 16.5200885534\n",
      "Agent 1 - loss: 0.0158498 total reward: 28.9578909874\n",
      "Agent 2 - loss: 0.00250952 total reward: 25.0399173498\n",
      "Agent 1 - loss: 0.00290835 total reward: 50.2472581863\n",
      "Agent 2 - loss: 0.00095522 total reward: 44.0387032032\n",
      "Agent 1 - loss: 0.030052 total reward: 18.6800972223\n",
      "Agent 2 - loss: 0.000134142 total reward: 16.4016902447\n",
      "Agent 1 - loss: 0.104559 total reward: 33.581599474\n",
      "Agent 2 - loss: 0.0164891 total reward: 29.7765539885\n",
      "Agent 1 - loss: 0.00296345 total reward: 29.7510620356\n",
      "Agent 2 - loss: 0.000374358 total reward: 25.0007838011\n",
      "Agent 1 - loss: 0.00880285 total reward: 28.7159475088\n",
      "Agent 2 - loss: 0.000875073 total reward: 24.9079021215\n",
      "Agent 1 - loss: 0.0235055 total reward: 30.3075139523\n",
      "Agent 2 - loss: 0.000423245 total reward: 16.5709685087\n",
      "Agent 1 - loss: 0.0187986 total reward: 49.0283995867\n",
      "Agent 2 - loss: 0.000123107 total reward: 43.8949680328\n",
      "Agent 1 - loss: 0.00691224 total reward: 49.2953196764\n",
      "Agent 2 - loss: 0.000147624 total reward: 43.9286006689\n",
      "Agent 1 - loss: 0.00429578 total reward: 38.7438861132\n",
      "Agent 2 - loss: 0.000204727 total reward: 34.24053967\n",
      "Agent 1 - loss: 0.00635953 total reward: 49.4696875811\n",
      "Agent 2 - loss: 0.000500873 total reward: 43.9623878002\n",
      "Agent 1 - loss: 0.0634606 total reward: 28.5376144648\n",
      "Agent 2 - loss: 0.000463983 total reward: 25.1040743589\n",
      "Agent 1 - loss: 0.0318423 total reward: 44.3955472708\n",
      "Agent 2 - loss: 0.000143451 total reward: 39.1007703543\n",
      "Agent 1 - loss: 0.00871419 total reward: 29.4544717073\n",
      "Agent 2 - loss: 0.00246896 total reward: 24.9833176136\n",
      "Agent 1 - loss: 0.0436824 total reward: 28.1671133041\n",
      "Agent 2 - loss: 2.64828e-05 total reward: 25.0282917023\n",
      "Agent 1 - loss: 0.00268575 total reward: 24.0741262436\n",
      "Agent 2 - loss: 0.000135369 total reward: 20.6265612841\n",
      "Agent 1 - loss: 0.00704489 total reward: 38.5971705914\n",
      "Agent 2 - loss: 0.000190682 total reward: 34.2422611713\n",
      "Agent 1 - loss: 0.178256 total reward: 34.900901556\n",
      "Agent 2 - loss: 0.0162231 total reward: 20.7753733397\n",
      "Agent 1 - loss: 0.00424839 total reward: 38.8530175686\n",
      "Agent 2 - loss: 0.000170242 total reward: 34.1910516024\n",
      "Agent 1 - loss: 0.0192112 total reward: 19.0140131712\n",
      "Agent 2 - loss: 5.91218e-05 total reward: 16.4045590162\n",
      "Agent 1 - loss: 0.00817869 total reward: 34.4626209736\n",
      "Agent 2 - loss: 0.000131422 total reward: 29.5672985315\n",
      "Agent 1 - loss: 0.0133479 total reward: 33.2488578558\n",
      "Agent 2 - loss: 2.19218e-05 total reward: 29.5118994713\n",
      "Agent 1 - loss: 0.00531623 total reward: 49.9278895855\n",
      "Agent 2 - loss: 0.00266336 total reward: 34.1982717514\n",
      "Agent 1 - loss: 0.00245685 total reward: 28.6019382477\n",
      "Agent 2 - loss: 0.000424737 total reward: 24.9339174032\n",
      "Agent 1 - loss: 0.0286401 total reward: 25.4506545067\n",
      "Agent 2 - loss: 0.000337552 total reward: 20.6389750242\n",
      "Agent 1 - loss: 0.00140312 total reward: 49.72597754\n",
      "Agent 2 - loss: 0.000170424 total reward: 43.9452996254\n",
      "Agent 1 - loss: 0.0573222 total reward: 24.5436673164\n",
      "Agent 2 - loss: 0.000594535 total reward: 20.6870797873\n",
      "Agent 1 - loss: 0.00634797 total reward: 44.7682286501\n",
      "Agent 2 - loss: 0.000229341 total reward: 29.6092605591\n",
      "Agent 1 - loss: 0.0233295 total reward: 49.6891880035\n",
      "Agent 2 - loss: 4.75194e-05 total reward: 43.9984570742\n",
      "Agent 1 - loss: 0.0181947 total reward: 55.2088862658\n",
      "Agent 2 - loss: 0.000119935 total reward: 39.0541607141\n",
      "Agent 1 - loss: 0.00958531 total reward: 28.1863787174\n",
      "Agent 2 - loss: 4.36669e-05 total reward: 25.0515158176\n",
      "Agent 1 - loss: 0.00255221 total reward: 44.174882412\n",
      "Agent 2 - loss: 4.74513e-05 total reward: 39.1175662279\n",
      "Agent 1 - loss: 0.0703138 total reward: 18.7027539015\n",
      "Agent 2 - loss: 0.000198792 total reward: 16.567117691\n",
      "Agent 1 - loss: 0.0611618 total reward: 44.6268695593\n",
      "Agent 2 - loss: 0.000153036 total reward: 39.0781060457\n",
      "Agent 1 - loss: 0.00800751 total reward: 44.1382223368\n",
      "Agent 2 - loss: 0.00111411 total reward: 38.9915568829\n",
      "Agent 1 - loss: 0.0146162 total reward: 39.1444175243\n",
      "Agent 2 - loss: 0.000895808 total reward: 34.2165621519\n",
      "Agent 1 - loss: 0.00824534 total reward: 39.9009006023\n",
      "Agent 2 - loss: 0.000213285 total reward: 34.2827390432\n",
      "Agent 1 - loss: 0.0340824 total reward: 19.6424779892\n",
      "Agent 2 - loss: 8.51688e-05 total reward: 16.4682964087\n",
      "Agent 1 - loss: 0.00540319 total reward: 19.0910288095\n",
      "Agent 2 - loss: 0.000419465 total reward: 16.5556492805\n",
      "Agent 1 - loss: 0.00694296 total reward: 49.7547051907\n",
      "Agent 2 - loss: 0.000178376 total reward: 43.9608827829\n",
      "Agent 1 - loss: 0.0140118 total reward: 19.0524442196\n",
      "Agent 2 - loss: 0.000180392 total reward: 16.5209619999\n",
      "Agent 1 - loss: 0.00121062 total reward: 34.2976723909\n",
      "Agent 2 - loss: 0.00031484 total reward: 29.5576193333\n",
      "Agent 1 - loss: 0.00856358 total reward: 39.7646143436\n",
      "Agent 2 - loss: 0.000138081 total reward: 25.0628008842\n",
      "Agent 1 - loss: 0.00575437 total reward: 44.770226717\n",
      "Agent 2 - loss: 0.000454046 total reward: 29.639770031\n",
      "Agent 1 - loss: 0.0237086 total reward: 50.0772516727\n",
      "Agent 2 - loss: 0.0125174 total reward: 43.8835129738\n",
      "Agent 1 - loss: 0.000603206 total reward: 49.4654140472\n",
      "Agent 2 - loss: 0.000252585 total reward: 44.060218215\n",
      "Agent 1 - loss: 0.00206539 total reward: 49.5355195999\n",
      "Agent 2 - loss: 4.89484e-05 total reward: 44.0720267296\n",
      "Agent 1 - loss: 0.0290012 total reward: 49.4423674345\n",
      "Agent 2 - loss: 0.000229716 total reward: 44.0069184303\n",
      "Agent 1 - loss: 0.00424216 total reward: 39.2711449862\n",
      "Agent 2 - loss: 6.16336e-05 total reward: 34.3243623972\n",
      "Agent 1 - loss: 0.0135757 total reward: 38.7323145866\n",
      "Agent 2 - loss: 9.1713e-05 total reward: 34.4329993725\n",
      "Agent 1 - loss: 0.004624 total reward: 43.6569536924\n",
      "Agent 2 - loss: 5.26105e-05 total reward: 39.1483221054\n",
      "Agent 1 - loss: 0.00455145 total reward: 23.5996490717\n",
      "Agent 2 - loss: 9.64816e-05 total reward: 20.6899371147\n",
      "Agent 1 - loss: 0.00318801 total reward: 20.5411492586\n",
      "Agent 2 - loss: 0.000300984 total reward: 16.4761904478\n",
      "Agent 1 - loss: 0.0349026 total reward: 24.7009031773\n",
      "Agent 2 - loss: 0.00178596 total reward: 20.5757305622\n",
      "Agent 1 - loss: 0.020995 total reward: 19.1453528404\n",
      "Agent 2 - loss: 0.000228565 total reward: 16.4802362919\n",
      "Agent 1 - loss: 0.548141 total reward: 23.196455121\n",
      "Agent 2 - loss: 0.00150248 total reward: 20.8652588129\n",
      "Agent 1 - loss: 0.00184626 total reward: 49.6447467804\n",
      "Agent 2 - loss: 0.00016752 total reward: 44.1379672289\n",
      "Agent 1 - loss: 0.0215397 total reward: 34.9080066681\n",
      "Agent 2 - loss: 0.0001577 total reward: 20.7009803057\n",
      "Agent 1 - loss: 0.00284349 total reward: 49.2021821737\n",
      "Agent 2 - loss: 4.55938e-05 total reward: 44.1542584896\n",
      "Agent 1 - loss: 0.115614 total reward: 28.4829797745\n",
      "Agent 2 - loss: 0.000461756 total reward: 24.9803988934\n",
      "Agent 1 - loss: 0.00193452 total reward: 49.503113389\n",
      "Agent 2 - loss: 8.53868e-05 total reward: 44.1526706219\n",
      "Agent 1 - loss: 0.117355 total reward: 43.2244523764\n",
      "Agent 2 - loss: 0.000177873 total reward: 39.088116765\n",
      "Agent 1 - loss: 0.0492316 total reward: 30.328430295\n",
      "Agent 2 - loss: 0.000416212 total reward: 16.5773714781\n",
      "Agent 1 - loss: 0.00416098 total reward: 24.1658605337\n",
      "Agent 2 - loss: 8.13168e-05 total reward: 20.6863230467\n",
      "Agent 1 - loss: 0.00133995 total reward: 33.5666600466\n",
      "Agent 2 - loss: 4.15529e-05 total reward: 29.7077862024\n",
      "Agent 1 - loss: 0.0013288 total reward: 38.6250977516\n",
      "Agent 2 - loss: 0.000840925 total reward: 34.4076961279\n",
      "Agent 1 - loss: 0.137764 total reward: 39.0652508736\n",
      "Agent 2 - loss: 0.000167083 total reward: 34.4309450388\n",
      "Agent 1 - loss: 0.00363171 total reward: 24.5795204639\n",
      "Agent 2 - loss: 0.000114863 total reward: 20.634452939\n",
      "Agent 1 - loss: 0.0192503 total reward: 28.2750610113\n",
      "Agent 2 - loss: 0.000223862 total reward: 25.0023367405\n",
      "Agent 1 - loss: 0.16272 total reward: 18.4979077578\n",
      "Agent 2 - loss: 0.0103748 total reward: 16.4850788116\n",
      "Agent 1 - loss: 0.00106852 total reward: 28.5060058832\n",
      "Agent 2 - loss: 0.000445648 total reward: 25.2654687166\n",
      "Agent 1 - loss: 0.0127589 total reward: 33.1840715408\n",
      "Agent 2 - loss: 5.36365e-05 total reward: 29.6084620953\n",
      "Agent 1 - loss: 0.016713 total reward: 23.4535084963\n",
      "Agent 2 - loss: 0.000239124 total reward: 20.6150240898\n",
      "Agent 1 - loss: 0.0031163 total reward: 49.3880182505\n",
      "Agent 2 - loss: 0.000151818 total reward: 44.1262013912\n",
      "Agent 1 - loss: 0.00193604 total reward: 43.9469225407\n",
      "Agent 2 - loss: 0.000101794 total reward: 39.2249670029\n",
      "Agent 1 - loss: 0.00389118 total reward: 44.4580928087\n",
      "Agent 2 - loss: 3.36754e-05 total reward: 39.2609521151\n",
      "Agent 1 - loss: 0.00481768 total reward: 39.0838147402\n",
      "Agent 2 - loss: 0.000735793 total reward: 34.4222140312\n",
      "Agent 1 - loss: 0.211812 total reward: 60.6158803701\n",
      "Agent 2 - loss: 3.12856e-05 total reward: 44.2197877169\n",
      "Agent 1 - loss: 0.00756502 total reward: 18.6868282557\n",
      "Agent 2 - loss: 0.000364134 total reward: 16.5230600834\n",
      "Agent 1 - loss: 0.00458006 total reward: 44.0101386309\n",
      "Agent 2 - loss: 2.05769e-05 total reward: 39.2506010532\n",
      "Agent 1 - loss: 0.001132 total reward: 50.0258471966\n",
      "Agent 2 - loss: 0.000540599 total reward: 44.1391005516\n",
      "Agent 1 - loss: 0.00312166 total reward: 38.8835633993\n",
      "Agent 2 - loss: 8.15946e-05 total reward: 34.4773201942\n",
      "Agent 1 - loss: 0.00119571 total reward: 23.3061834574\n",
      "Agent 2 - loss: 0.000110516 total reward: 20.6401660442\n",
      "Agent 1 - loss: 0.00213576 total reward: 23.3669143915\n",
      "Agent 2 - loss: 0.000555629 total reward: 20.6307822466\n",
      "Agent 1 - loss: 0.0206017 total reward: 23.7554138899\n",
      "Agent 2 - loss: 0.000629709 total reward: 20.872640729\n",
      "Agent 1 - loss: 0.000726579 total reward: 49.316937089\n",
      "Agent 2 - loss: 0.000123622 total reward: 44.1802806854\n",
      "Agent 1 - loss: 0.0132829 total reward: 28.3999612331\n",
      "Agent 2 - loss: 0.000966234 total reward: 25.2696449757\n",
      "Agent 1 - loss: 0.0453387 total reward: 30.3617999554\n",
      "Agent 2 - loss: 0.000452823 total reward: 16.6256058216\n",
      "Agent 1 - loss: 0.00132098 total reward: 33.3396195173\n",
      "Agent 2 - loss: 7.23322e-05 total reward: 29.6876547337\n",
      "Agent 1 - loss: 0.019928 total reward: 49.4701031446\n",
      "Agent 2 - loss: 3.86647e-05 total reward: 44.1871387959\n",
      "Agent 1 - loss: 0.00132278 total reward: 33.4857909679\n",
      "Agent 2 - loss: 0.000146225 total reward: 29.6535818577\n",
      "Agent 1 - loss: 0.00577053 total reward: 28.3819583654\n",
      "Agent 2 - loss: 0.000159582 total reward: 25.0678567886\n",
      "Agent 1 - loss: 0.148661 total reward: 23.6326385736\n",
      "Agent 2 - loss: 0.0522787 total reward: 20.8293573856\n",
      "Agent 1 - loss: 0.0170715 total reward: 34.9148257971\n",
      "Agent 2 - loss: 0.000308618 total reward: 20.681648612\n",
      "Agent 1 - loss: 0.000759469 total reward: 50.0012987852\n",
      "Agent 2 - loss: 0.0938959 total reward: 43.8887382746\n",
      "Agent 1 - loss: 0.0104912 total reward: 39.7899917364\n",
      "Agent 2 - loss: 0.00139445 total reward: 25.661709547\n",
      "Agent 1 - loss: 0.00616518 total reward: 23.8382171392\n",
      "Agent 2 - loss: 9.67845e-05 total reward: 21.1736854315\n",
      "Agent 1 - loss: 0.0153339 total reward: 18.7924964428\n",
      "Agent 2 - loss: 8.97266e-05 total reward: 16.8557828665\n",
      "Agent 1 - loss: 0.000849698 total reward: 28.4411439896\n",
      "Agent 2 - loss: 6.46607e-05 total reward: 25.5662047863\n",
      "Agent 1 - loss: 0.00211239 total reward: 18.8064105511\n",
      "Agent 2 - loss: 2.74211e-05 total reward: 16.8243851662\n",
      "Agent 1 - loss: 0.00827312 total reward: 49.3794385195\n",
      "Agent 2 - loss: 0.00341294 total reward: 44.8537260294\n",
      "Agent 1 - loss: 0.00164547 total reward: 49.4068138599\n",
      "Agent 2 - loss: 0.000153383 total reward: 45.0893403292\n",
      "Agent 1 - loss: 0.000992852 total reward: 49.2597500086\n",
      "Agent 2 - loss: 5.17312e-05 total reward: 45.0285320282\n",
      "Agent 1 - loss: 0.0426374 total reward: 19.7492311001\n",
      "Agent 2 - loss: 0.0020592 total reward: 16.765483737\n",
      "Agent 1 - loss: 0.00430827 total reward: 60.6574268341\n",
      "Agent 2 - loss: 2.19056e-05 total reward: 45.1137239933\n",
      "Agent 1 - loss: 0.0242667 total reward: 33.6245085001\n",
      "Agent 2 - loss: 0.000306983 total reward: 30.4117600918\n",
      "Agent 1 - loss: 0.0157271 total reward: 23.8686966896\n",
      "Agent 2 - loss: 2.312e-05 total reward: 21.1748189926\n",
      "Agent 1 - loss: 0.258927 total reward: 33.6070277691\n",
      "Agent 2 - loss: 0.000229663 total reward: 30.1844339371\n",
      "Agent 1 - loss: 0.00228593 total reward: 39.7483159304\n",
      "Agent 2 - loss: 4.71733e-05 total reward: 25.7197064161\n",
      "Agent 1 - loss: 0.00736588 total reward: 29.0758177042\n",
      "Agent 2 - loss: 0.00154509 total reward: 25.6356878281\n",
      "Agent 1 - loss: 0.0522408 total reward: 30.3686039448\n",
      "Agent 2 - loss: 0.000632593 total reward: 16.7669417858\n",
      "Agent 1 - loss: 0.0256745 total reward: 30.3175423145\n",
      "Agent 2 - loss: 0.000245965 total reward: 17.0189740658\n",
      "Agent 1 - loss: 0.00314211 total reward: 49.7270282507\n",
      "Agent 2 - loss: 0.000359614 total reward: 45.0065739155\n",
      "Agent 1 - loss: 0.00106263 total reward: 44.7981296778\n",
      "Agent 2 - loss: 6.52605e-05 total reward: 30.4137369394\n",
      "Agent 1 - loss: 0.00179746 total reward: 38.9515330791\n",
      "Agent 2 - loss: 2.68075e-05 total reward: 35.0906213522\n",
      "Agent 1 - loss: 0.000835592 total reward: 23.9068757296\n",
      "Agent 2 - loss: 2.18265e-05 total reward: 21.2548562288\n",
      "Agent 1 - loss: 0.00169343 total reward: 19.5994610786\n",
      "Agent 2 - loss: 2.03004e-05 total reward: 16.8810153008\n",
      "Agent 1 - loss: 0.0128675 total reward: 34.8977851868\n",
      "Agent 2 - loss: 3.6752e-05 total reward: 21.1269966364\n",
      "Agent 1 - loss: 0.00106799 total reward: 19.4970448017\n",
      "Agent 2 - loss: 0.0069464 total reward: 17.0380151272\n",
      "Agent 1 - loss: 0.0113575 total reward: 49.9243825674\n",
      "Agent 2 - loss: 0.000491814 total reward: 35.0883448124\n",
      "Agent 1 - loss: 0.00656275 total reward: 33.6540317535\n",
      "Agent 2 - loss: 0.000150267 total reward: 30.3416700363\n",
      "Agent 1 - loss: 0.00859249 total reward: 28.7989500761\n",
      "Agent 2 - loss: 0.000402272 total reward: 25.6189273596\n",
      "Agent 1 - loss: 0.098047 total reward: 44.7758021355\n",
      "Agent 2 - loss: 1.71191e-05 total reward: 30.369956851\n",
      "Agent 1 - loss: 0.00242779 total reward: 33.810888648\n",
      "Agent 2 - loss: 0.000200308 total reward: 30.178553462\n",
      "Agent 1 - loss: 0.194644 total reward: 19.1884069443\n",
      "Agent 2 - loss: 0.000222769 total reward: 16.7637161016\n",
      "Agent 1 - loss: 0.0127586 total reward: 24.5164010525\n",
      "Agent 2 - loss: 0.000511706 total reward: 21.0682854652\n",
      "Agent 1 - loss: 0.00622312 total reward: 49.9524348974\n",
      "Agent 2 - loss: 0.00148368 total reward: 35.25671947\n",
      "Agent 1 - loss: 0.00295364 total reward: 43.7935881615\n",
      "Agent 2 - loss: 0.000156951 total reward: 39.9268753529\n",
      "Agent 1 - loss: 0.00448032 total reward: 60.6649378538\n",
      "Agent 2 - loss: 0.00833849 total reward: 45.1494194269\n",
      "Agent 1 - loss: 0.002947 total reward: 39.2940710783\n",
      "Agent 2 - loss: 9.07863e-05 total reward: 35.0846568346\n",
      "Agent 1 - loss: 0.00554228 total reward: 33.3638277054\n",
      "Agent 2 - loss: 5.34711e-05 total reward: 30.2248489857\n",
      "Agent 1 - loss: 0.0271167 total reward: 34.9141224623\n",
      "Agent 2 - loss: 7.20824e-05 total reward: 21.0835753679\n",
      "Agent 1 - loss: 0.0186402 total reward: 28.2462866306\n",
      "Agent 2 - loss: 1.34478e-05 total reward: 25.6307839155\n",
      "Agent 1 - loss: 0.00746491 total reward: 60.6186461449\n",
      "Agent 2 - loss: 0.000112606 total reward: 45.0130972862\n",
      "Agent 1 - loss: 0.00071302 total reward: 23.5544825792\n",
      "Agent 2 - loss: 3.71763e-05 total reward: 21.2163338661\n",
      "Agent 1 - loss: 0.000596994 total reward: 39.0824267864\n",
      "Agent 2 - loss: 1.99169e-05 total reward: 35.1903094053\n",
      "Agent 1 - loss: 0.0256026 total reward: 45.1212317944\n",
      "Agent 2 - loss: 9.97579e-05 total reward: 40.0598067045\n",
      "Agent 1 - loss: 0.00298418 total reward: 38.9455040693\n",
      "Agent 2 - loss: 2.59736e-05 total reward: 35.0630543232\n",
      "Agent 1 - loss: 0.00211152 total reward: 34.1207816601\n",
      "Agent 2 - loss: 0.00011093 total reward: 30.3291217089\n",
      "Agent 1 - loss: 0.00871279 total reward: 28.0729544163\n",
      "Agent 2 - loss: 2.08014e-05 total reward: 25.5046887398\n",
      "Agent 1 - loss: 0.00485865 total reward: 43.7697480917\n",
      "Agent 2 - loss: 4.12842e-05 total reward: 39.9062976837\n",
      "Agent 1 - loss: 0.057881 total reward: 20.1625418663\n",
      "Agent 2 - loss: 0.00267205 total reward: 16.9905551672\n",
      "Agent 1 - loss: 0.00435665 total reward: 39.0438507795\n",
      "Agent 2 - loss: 6.60002e-05 total reward: 35.1151167154\n",
      "Agent 1 - loss: 0.00455215 total reward: 24.053524971\n",
      "Agent 2 - loss: 5.67131e-05 total reward: 21.1541606188\n",
      "Agent 1 - loss: 0.00127665 total reward: 33.7416049242\n",
      "Agent 2 - loss: 0.00500619 total reward: 30.3357481956\n",
      "Agent 1 - loss: 0.00163554 total reward: 20.1843863726\n",
      "Agent 2 - loss: 2.52695e-05 total reward: 16.8686987162\n",
      "Agent 1 - loss: 0.0062784 total reward: 38.5661303997\n",
      "Agent 2 - loss: 2.00952e-05 total reward: 34.9956693649\n",
      "Agent 1 - loss: 0.00577904 total reward: 33.0925900936\n",
      "Agent 2 - loss: 2.03269e-05 total reward: 30.1178029776\n",
      "Agent 1 - loss: 0.000936549 total reward: 55.2689068317\n",
      "Agent 2 - loss: 0.000245943 total reward: 39.8534737825\n",
      "Agent 1 - loss: 0.00483189 total reward: 23.8038550615\n",
      "Agent 2 - loss: 2.66235e-05 total reward: 21.0967662334\n",
      "Agent 1 - loss: 0.00251443 total reward: 49.5614311695\n",
      "Agent 2 - loss: 0.00417184 total reward: 44.862673521\n",
      "Agent 1 - loss: 0.0033343 total reward: 33.3656008244\n",
      "Agent 2 - loss: 0.000472529 total reward: 30.3472465277\n",
      "Agent 1 - loss: 0.00169767 total reward: 19.2906290293\n",
      "Agent 2 - loss: 2.37925e-05 total reward: 16.9836913347\n",
      "Agent 1 - loss: 0.00322729 total reward: 34.449998498\n",
      "Agent 2 - loss: 9.87869e-05 total reward: 30.3217388391\n",
      "Agent 1 - loss: 0.00648564 total reward: 23.7782417536\n",
      "Agent 2 - loss: 2.0302e-05 total reward: 21.1627783775\n",
      "Agent 1 - loss: 0.0211562 total reward: 49.8146145344\n",
      "Agent 2 - loss: 4.05837e-05 total reward: 45.0312619209\n",
      "Agent 1 - loss: 0.00598525 total reward: 50.352797389\n",
      "Agent 2 - loss: 3.92932e-05 total reward: 45.0373820066\n",
      "Agent 1 - loss: 0.00545619 total reward: 38.4088363647\n",
      "Agent 2 - loss: 3.30302e-05 total reward: 35.0407817364\n",
      "Agent 1 - loss: 0.0122565 total reward: 23.495875001\n",
      "Agent 2 - loss: 1.86529e-05 total reward: 21.1026189327\n",
      "Agent 1 - loss: 0.00162223 total reward: 60.643006444\n",
      "Agent 2 - loss: 0.000762513 total reward: 44.9798579216\n",
      "Agent 1 - loss: 0.000381947 total reward: 60.6867706776\n",
      "Agent 2 - loss: 0.000273955 total reward: 45.0463223457\n",
      "Agent 1 - loss: 0.00290976 total reward: 49.3919693232\n",
      "Agent 2 - loss: 2.64988e-05 total reward: 44.9551584721\n",
      "Agent 1 - loss: 0.00275023 total reward: 24.0838158131\n",
      "Agent 2 - loss: 5.29526e-05 total reward: 21.1605744362\n",
      "Agent 1 - loss: 0.113416 total reward: 55.2346704006\n",
      "Agent 2 - loss: 5.09321e-05 total reward: 40.0619517565\n",
      "Agent 1 - loss: 0.0011517 total reward: 19.1842509508\n",
      "Agent 2 - loss: 9.13601e-05 total reward: 16.7655190229\n",
      "Agent 1 - loss: 0.000755745 total reward: 28.3216068745\n",
      "Agent 2 - loss: 7.26882e-05 total reward: 25.6226007938\n",
      "Agent 1 - loss: 0.00150456 total reward: 28.3532067537\n",
      "Agent 2 - loss: 6.038e-05 total reward: 25.6039872169\n",
      "Agent 1 - loss: 0.00182033 total reward: 49.2523099184\n",
      "Agent 2 - loss: 2.34317e-05 total reward: 45.0482180119\n",
      "Agent 1 - loss: 0.0027539 total reward: 33.5887663364\n",
      "Agent 2 - loss: 0.00132095 total reward: 30.1549911499\n",
      "Agent 1 - loss: 0.00287962 total reward: 28.2546051741\n",
      "Agent 2 - loss: 1.62453e-05 total reward: 25.5852755308\n",
      "Agent 1 - loss: 0.0273275 total reward: 39.2786291838\n",
      "Agent 2 - loss: 5.99815e-05 total reward: 35.1061563492\n",
      "Agent 1 - loss: 0.0103361 total reward: 48.9488382339\n",
      "Agent 2 - loss: 1.84949e-05 total reward: 44.9912509918\n",
      "Agent 1 - loss: 0.000658394 total reward: 19.0834615231\n",
      "Agent 2 - loss: 1.40067e-05 total reward: 16.8900893927\n",
      "Agent 1 - loss: 0.0264172 total reward: 44.7769857645\n",
      "Agent 2 - loss: 9.92601e-05 total reward: 30.4087927341\n",
      "Agent 1 - loss: 0.000593289 total reward: 33.0871754885\n",
      "Agent 2 - loss: 0.000314754 total reward: 30.2637583017\n",
      "Agent 1 - loss: 0.00176021 total reward: 24.5106621981\n",
      "Agent 2 - loss: 3.89276e-05 total reward: 21.14912498\n",
      "Agent 1 - loss: 0.0054747 total reward: 43.7571537495\n",
      "Agent 2 - loss: 1.8556e-05 total reward: 39.9780521393\n",
      "Agent 1 - loss: 0.00281548 total reward: 24.1565074921\n",
      "Agent 2 - loss: 4.35648e-05 total reward: 21.3346598148\n",
      "Agent 1 - loss: 0.0038432 total reward: 49.6857391596\n",
      "Agent 2 - loss: 9.14357e-05 total reward: 45.1112095118\n",
      "Agent 1 - loss: 0.00592686 total reward: 23.2880398035\n",
      "Agent 2 - loss: 2.51254e-05 total reward: 21.1152635813\n",
      "Agent 1 - loss: 0.00047142 total reward: 38.6398541927\n",
      "Agent 2 - loss: 2.53547e-05 total reward: 35.2198996544\n",
      "Agent 1 - loss: 0.349204 total reward: 50.0562177896\n",
      "Agent 2 - loss: 0.0661952 total reward: 45.1747648716\n",
      "Agent 1 - loss: 0.0279655 total reward: 23.5705573559\n",
      "Agent 2 - loss: 0.000222445 total reward: 20.9647643566\n",
      "Agent 1 - loss: 0.000730749 total reward: 38.8272393942\n",
      "Agent 2 - loss: 3.1775e-05 total reward: 34.824966073\n",
      "Agent 1 - loss: 0.00153321 total reward: 38.7074406147\n",
      "Agent 2 - loss: 4.31772e-05 total reward: 34.707588315\n",
      "Agent 1 - loss: 0.00173639 total reward: 33.1514956951\n",
      "Agent 2 - loss: 1.5151e-05 total reward: 30.0088099241\n",
      "Agent 1 - loss: 0.00158081 total reward: 28.4220856428\n",
      "Agent 2 - loss: 0.000162808 total reward: 25.3960617781\n",
      "Agent 1 - loss: 0.00142651 total reward: 23.516546607\n",
      "Agent 2 - loss: 3.24478e-05 total reward: 21.0267330408\n",
      "Agent 1 - loss: 0.00187718 total reward: 49.3577034473\n",
      "Agent 2 - loss: 2.31748e-05 total reward: 44.6776081324\n",
      "Agent 1 - loss: 0.00303528 total reward: 23.2797832489\n",
      "Agent 2 - loss: 8.70216e-05 total reward: 20.988394618\n",
      "Agent 1 - loss: 0.0212508 total reward: 30.3178608418\n",
      "Agent 2 - loss: 0.000277109 total reward: 16.7669371367\n",
      "Agent 1 - loss: 0.00282464 total reward: 39.2670966387\n",
      "Agent 2 - loss: 0.000362444 total reward: 34.7606166601\n",
      "Agent 1 - loss: 0.000939848 total reward: 33.6261570454\n",
      "Agent 2 - loss: 9.23862e-05 total reward: 29.9735199213\n",
      "Agent 1 - loss: 0.00206595 total reward: 39.7587826252\n",
      "Agent 2 - loss: 0.00110375 total reward: 25.4390757084\n",
      "Agent 1 - loss: 0.0101164 total reward: 19.4959477186\n",
      "Agent 2 - loss: 0.000128188 total reward: 16.7815444469\n",
      "Agent 1 - loss: 0.280088 total reward: 27.7584476471\n",
      "Agent 2 - loss: 0.0772092 total reward: 25.3503119946\n",
      "Agent 1 - loss: 0.00473812 total reward: 49.3061621189\n",
      "Agent 2 - loss: 3.50301e-05 total reward: 44.6915684938\n",
      "Agent 1 - loss: 0.00255461 total reward: 24.2767364979\n",
      "Agent 2 - loss: 2.31435e-05 total reward: 21.0071358681\n",
      "Agent 1 - loss: 0.0115614 total reward: 44.7137663364\n",
      "Agent 2 - loss: 0.000119395 total reward: 39.7650327682\n",
      "Agent 1 - loss: 0.021749 total reward: 33.0014350414\n",
      "Agent 2 - loss: 0.000561256 total reward: 30.0043250322\n",
      "Agent 1 - loss: 0.015032 total reward: 28.0808856487\n",
      "Agent 2 - loss: 7.10469e-05 total reward: 25.4062030315\n",
      "Agent 1 - loss: 0.00318832 total reward: 49.2759430408\n",
      "Agent 2 - loss: 0.000334256 total reward: 44.6664054394\n",
      "Agent 1 - loss: 0.00105452 total reward: 33.4359018803\n",
      "Agent 2 - loss: 0.000155915 total reward: 30.0296905041\n",
      "Agent 1 - loss: 0.26589 total reward: 39.3372747898\n",
      "Agent 2 - loss: 0.00100128 total reward: 34.905144453\n",
      "Agent 1 - loss: 0.00204018 total reward: 19.1920342445\n",
      "Agent 2 - loss: 9.10732e-05 total reward: 16.8565315008\n",
      "Agent 1 - loss: 0.000366757 total reward: 44.0474853516\n",
      "Agent 2 - loss: 0.000455634 total reward: 39.8014121056\n",
      "Agent 1 - loss: 0.00140716 total reward: 28.505874157\n",
      "Agent 2 - loss: 0.000142465 total reward: 25.4302943945\n",
      "Agent 1 - loss: 0.00264862 total reward: 38.5662732124\n",
      "Agent 2 - loss: 1.98229e-05 total reward: 34.8143564463\n",
      "Agent 1 - loss: 0.000678072 total reward: 34.233206749\n",
      "Agent 2 - loss: 2.87701e-05 total reward: 30.0614352226\n",
      "Agent 1 - loss: 0.0010835 total reward: 29.5480685234\n",
      "Agent 2 - loss: 0.000115428 total reward: 25.4938439131\n",
      "Agent 1 - loss: 0.000259344 total reward: 33.5217308998\n",
      "Agent 2 - loss: 0.000587099 total reward: 30.1189396381\n",
      "Agent 1 - loss: 0.000506224 total reward: 33.3713774681\n",
      "Agent 2 - loss: 2.88706e-05 total reward: 30.1107211113\n",
      "Agent 1 - loss: 0.00813018 total reward: 33.6035876274\n",
      "Agent 2 - loss: 4.34356e-05 total reward: 30.0610663891\n",
      "Agent 1 - loss: 0.0022035 total reward: 18.7974307537\n",
      "Agent 2 - loss: 1.51079e-05 total reward: 16.7500319481\n",
      "Agent 1 - loss: 0.00142539 total reward: 28.56291008\n",
      "Agent 2 - loss: 0.000125395 total reward: 25.4215751886\n",
      "Agent 1 - loss: 0.00138898 total reward: 44.075220108\n",
      "Agent 2 - loss: 1.93906e-05 total reward: 39.6998918056\n",
      "Agent 1 - loss: 0.0114547 total reward: 33.68364048\n",
      "Agent 2 - loss: 7.99044e-05 total reward: 30.0684882402\n",
      "Agent 1 - loss: 0.00277924 total reward: 45.6356582642\n",
      "Agent 2 - loss: 7.34631e-05 total reward: 39.7781770229\n",
      "Agent 1 - loss: 0.00578771 total reward: 49.9472256899\n",
      "Agent 2 - loss: 0.000506645 total reward: 34.7915592194\n",
      "Agent 1 - loss: 0.000372576 total reward: 49.6536947489\n",
      "Agent 2 - loss: 3.11796e-05 total reward: 44.7103888988\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-419-cdf01e16da23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdialog\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcorpus_filtered\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msimulate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdialog\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'turns'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-418-8a3d4b1d36b4>\u001b[0m in \u001b[0;36msimulate\u001b[0;34m(self, inputs, display)\u001b[0m\n\u001b[1;32m    360\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0mag1_previous_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag1_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m             \u001b[0mag1_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_agent1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag2_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag1_previous_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    363\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_append_agent_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'agent_1'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag2_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag1_previous_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag1_output\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_p\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'agent_1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'previous_state'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-418-8a3d4b1d36b4>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, inputs, previous_outputs)\u001b[0m\n\u001b[1;32m    185\u001b[0m         }\n\u001b[1;32m    186\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for dialog in corpus_filtered:\n",
    "    trainer.reset()\n",
    "    trainer.simulate([embed(dialog['turns'][0])], display=False)\n",
    "    trainer.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"i 'm not .\""
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ids, _, _ = trainer._agent1.run([embed('What are you doing?')], [[0]])\n",
    "decode(token_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0641787"
      ]
     },
     "execution_count": 363,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_ps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ True, False, False, False],\n",
       "       [ True,  True, False, False],\n",
       "       [ True,  True,  True,  True]], dtype=bool)"
      ]
     },
     "execution_count": 340,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "ps = tf.constant([[[0.1, 1.1, 2.2, 3.3, 4.4]]*5]*3, tf.float32)\n",
    "\n",
    "chosen_tokens = tf.constant([\n",
    "    [1, 3, 0, 0, 0],\n",
    "    [1, 2, 3, 0, 0],\n",
    "    [2, 4, 1, 2, 0],\n",
    "], tf.int32)\n",
    "\n",
    "batch_size, n_tokens, _ = tf.unstack(tf.shape(ps))\n",
    "\n",
    "token_indices = tf.concat([\n",
    "    tf.tile(tf.reshape(tf.range(batch_size), [batch_size, 1, 1]), [1, n_tokens, 1]),\n",
    "    tf.tile(tf.reshape(tf.range(n_tokens), [1, n_tokens, 1]), [batch_size, 1, 1]),\n",
    "    tf.expand_dims(chosen_tokens, -1)\n",
    "], -1)\n",
    "\n",
    "\n",
    "p = tf.gather_nd(ps, token_indices)\n",
    "\n",
    "\n",
    "sess.run(tf.sequence_mask([1, 2, 4], 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.,  1.,  2.,  3.,  4.],\n",
       "        [ 0.,  1.,  2.,  3.,  4.],\n",
       "        [ 0.,  1.,  2.,  3.,  4.],\n",
       "        [ 0.,  1.,  2.,  3.,  4.],\n",
       "        [ 0.,  1.,  2.,  3.,  4.]],\n",
       "\n",
       "       [[ 0.,  1.,  2.,  3.,  4.],\n",
       "        [ 0.,  1.,  2.,  3.,  4.],\n",
       "        [ 0.,  1.,  2.,  3.,  4.],\n",
       "        [ 0.,  1.,  2.,  3.,  4.],\n",
       "        [ 0.,  1.,  2.,  3.,  4.]],\n",
       "\n",
       "       [[ 0.,  1.,  2.,  3.,  4.],\n",
       "        [ 0.,  1.,  2.,  3.,  4.],\n",
       "        [ 0.,  1.,  2.,  3.,  4.],\n",
       "        [ 0.,  1.,  2.,  3.,  4.],\n",
       "        [ 0.,  1.,  2.,  3.,  4.]]], dtype=float32)"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "token_ps = np.argmax(token_ps, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 True\n",
      "1 True\n",
      "2 True\n",
      "3 True\n",
      "4 True\n",
      "5 True\n"
     ]
    }
   ],
   "source": [
    "for i, response in enumerate(trainer._state['agent_1']['responses']):\n",
    "    print(i, np.all(token_ps[i][:len(response)] == response))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous output: \n",
      "Input: how are you today\n",
      "Response: fine .\n",
      "Reward: 1.20768195391\n",
      "\n",
      "Previous output: fine .\n",
      "Input: no harm has n't missed his books .\n",
      "Response: no . let 's stick up .\n",
      "Reward: 3.23449\n",
      "\n",
      "Previous output: no . let 's stick up .\n",
      "Input: no .\n",
      "Response: \n",
      "Reward: 0.133586\n",
      "\n",
      "Previous output: \n",
      "Input: push her burnt station .\n",
      "Response: they did n't even notice it .\n",
      "Reward: 1.7796\n",
      "\n",
      "Previous output: they did n't even notice it .\n",
      "Input: come on .\n",
      "Response: but you ca n't move out night , <UNK> <UNK> alone in the next day , did n't he ?\n",
      "Reward: 2.54831\n",
      "\n",
      "Previous output: but you ca n't move out night , <UNK> <UNK> alone in the next day , did n't he ?\n",
      "Input: shoot .\n",
      "Response: do n't probably stop one . it 's probably a good story .\n",
      "Reward: 2.19818\n",
      "\n"
     ]
    }
   ],
   "source": [
    "agent = 'agent_1'\n",
    "\n",
    "for i in range(len(trainer._state[agent]['responses'])):\n",
    "    print('Previous output:', decode(trainer._state[agent]['previous_outputs'][i]))\n",
    "    print('Input:', decode(trainer._state[agent]['inputs'][i]))\n",
    "    print('Response:', decode(trainer._state[agent]['responses'][i]))\n",
    "    print('Reward:', trainer._state[agent]['rewards'][i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.35192207,  9.36028142,  7.65724176,  9.40456923,  9.53121285,\n",
       "        8.7286303 ,  8.16306739,  7.44175142,  7.95393311,  8.05174573,\n",
       "        6.67757938,  6.43976879,  6.93575635,  5.41345495,  4.03126113,\n",
       "        2.78128719])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "discount_rewards(ag1_rewards, discount_factor=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.56754887]\n",
      "[ 2.33131266]\n"
     ]
    }
   ],
   "source": [
    "print(trainer._reward([embed('What would you like to do tomorrow?')]))\n",
    "print(trainer._reward([embed('How about dinner?')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer._previous_state = np.asarray([[1, 1, 1, 1]])\n",
    "trainer._information_flow_reward(np.asarray([[1, 1, 1, 1]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  1.31403327e-01,   5.40749334e-10], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p[[0, 1], [2, 8]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "139\n"
     ]
    }
   ],
   "source": [
    "def samples_iterator(data, batch_size=64, step = 2):\n",
    "    def turns_iterator():\n",
    "        for dialog in data:\n",
    "            for turn_idx in range(0, len(dialog)-1, 2):\n",
    "                yield dialog[turn_idx], dialog[turn_idx+1], [0] if turn_idx == 0 else dialog[turn_idx-1]\n",
    "        \n",
    "    batch_inputs, batch_targets, batch_previous_outputs = [], [], []\n",
    "    batch_input_lengths, batch_target_lengths, batch_previous_output_lengths = [], [], []\n",
    "    for (inputs, targets, previous_outputs) in turns_iterator():\n",
    "        batch_inputs.append(inputs)\n",
    "        batch_targets.append(targets)\n",
    "        batch_previous_outputs.append(previous_outputs)\n",
    "        batch_input_lengths.append(len(inputs))\n",
    "        batch_target_lengths.append(len(targets))\n",
    "        batch_previous_output_lengths.append(len(previous_outputs))\n",
    "        \n",
    "        if len(batch_inputs) >= batch_size:\n",
    "            yield {\n",
    "                'inputs': pad_sequences(batch_inputs, min(50, max(batch_input_lengths))),\n",
    "                'inputs_length': np.clip(batch_input_lengths, 0, 50).tolist(),\n",
    "                'previous_outputs': pad_sequences(batch_previous_outputs, min(50, max(batch_previous_output_lengths))),\n",
    "                'previous_outputs_length': np.clip(batch_previous_output_lengths, 0, 50).tolist(),\n",
    "                'targets': pad_sequences(batch_targets, min(50, max(batch_target_lengths))),\n",
    "                'targets_length': np.clip(batch_target_lengths, 0, 50).tolist(),\n",
    "            }\n",
    "            \n",
    "            batch_inputs, batch_targets, batch_previous_outputs = [], [], []\n",
    "            batch_input_lengths, batch_target_lengths, batch_previous_output_lengths = [], [], []\n",
    "\n",
    "a = 0\n",
    "for idx, batch in enumerate(samples_iterator(train_data)):\n",
    "    a += 1\n",
    "    po_len = batch['previous_outputs_length'][1]\n",
    "#     assert np.all(batch['targets'][0][:po_len] == batch['previous_outputs'][1][:po_len])\n",
    "    \n",
    "    if len(batch['targets']) > 64:\n",
    "        print(len(batch['targets']))\n",
    "        raise ValueError\n",
    "#     break\n",
    "\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: [9858, 300]\n",
      "9.19813\n",
      "9.14735\n",
      "9.02763\n",
      "8.8638\n",
      "8.67223\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-91-2b9526f556fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                 \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprevious_outputs_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'previous_outputs_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'targets'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                 \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'targets_length'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m             })\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    trainer = CrossEntropyTrainer([len(embeddings), 300])\n",
    "    embeddings_ph, embeddings_init_op = trainer.embeddings_initializer()\n",
    "    \n",
    "    train_op = tf.train.AdamOptimizer().minimize(trainer.loss)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(embeddings_init_op, feed_dict={embeddings_ph: embeddings})\n",
    "        \n",
    "        for idx, batch in enumerate(samples_iterator(train_data)):\n",
    "            _, loss_val = sess.run([train_op, trainer.loss], feed_dict = {\n",
    "                trainer.inputs: batch['inputs'],\n",
    "                trainer.inputs_length: batch['inputs_length'],\n",
    "                trainer.previous_outputs: batch['previous_outputs'],\n",
    "                trainer.previous_outputs_length: batch['previous_outputs_length'],\n",
    "                trainer.targets: batch['targets'],\n",
    "                trainer.targets_length: batch['targets_length']\n",
    "            })\n",
    "            \n",
    "#             break\n",
    "            \n",
    "            print(loss_val)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inference\n",
      "Embeddings: [9858, 300]\n"
     ]
    }
   ],
   "source": [
    "chatbot_graph = tf.Graph()\n",
    "\n",
    "with chatbot_graph.as_default():\n",
    "    inputs_ph = tf.placeholder(tf.int32, [1, None])\n",
    "    inputs_length_ph = tf.placeholder(tf.int32, [1])\n",
    "    previous_outputs_ph = tf.placeholder(tf.int32, [1, None])\n",
    "    previous_outputs_length_ph = tf.placeholder(tf.int32, [1])\n",
    "    \n",
    "    chatbot = ChatbotAgent(\n",
    "        inputs_ph, inputs_length_ph, previous_outputs_ph, previous_outputs_length_ph,\n",
    "        word_embeddings_shape=[len(embeddings), 300], train=False,\n",
    "        decoder_sampling_p=0.0\n",
    "    )\n",
    "    \n",
    "    saver = tf.train.Saver(max_to_keep=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "def embed(text):\n",
    "    return [word_dict.get(token, 2) for token in nltk.word_tokenize(str(text).lower())]\n",
    "\n",
    "def decode(token_ids):\n",
    "    return ' '.join([word_index[token_id] for token_id in token_ids])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'movie_id': 'm0',\n",
       " 'turns': [\"Listen, I know you hate having to sit home because I'm not Susie High School.\",\n",
       "  'Like you care.',\n",
       "  \"I do care. But I'm a firm believer in doing something for your own reasons, not someone else ' s .\",\n",
       "  \"I wish I had that luxury. I'm the only sophomore that got asked to the prom and I can't go, because you won ' t.\"]}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus_filtered[8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/gcloud_simple_agent_15/model.ckpt-19\n",
      "[44, 36, 110, 23, 994, 39]\n",
      "> Can we make this quick?\n",
      "got ta catch 'em , ray . <EOS>\n",
      "> who?\n",
      "fucker . <EOS>\n",
      "> aha\n",
      "pig days . <EOS>\n",
      "> that suck\n",
      "'cause he 's dead . <EOS>\n",
      "> who killed him?\n",
      "he 's a <UNK> . he 's in a room . he 's a <UNK> . he 's a <UNK> . he 's a <UNK> . he 's a <UNK> . he 's a <UNK> . he 's a <UNK> . he 's a <UNK> . he 's gon na be\n",
      "> okay\n",
      "<UNK> , too . <EOS>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                 \u001b[0mident\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstdin_socket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    720\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/jupyter_client/session.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, socket, mode, content, copy)\u001b[0m\n\u001b[1;32m    777\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 778\u001b[0;31m             \u001b[0mmsg_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_multipart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    779\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mzmq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZMQError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/zmq/sugar/socket.py\u001b[0m in \u001b[0;36mrecv_multipart\u001b[0;34m(self, flags, copy, track)\u001b[0m\n\u001b[1;32m    394\u001b[0m         \"\"\"\n\u001b[0;32m--> 395\u001b[0;31m         \u001b[0mparts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrack\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrack\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m         \u001b[0;31m# have first part already, only loop while more to receive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7683)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket.Socket.recv (zmq/backend/cython/socket.c:7460)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mzmq/backend/cython/socket.pyx\u001b[0m in \u001b[0;36mzmq.backend.cython.socket._recv_copy (zmq/backend/cython/socket.c:2344)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/zmq/backend/cython/checkrc.pxd\u001b[0m in \u001b[0;36mzmq.backend.cython.checkrc._check_rc (zmq/backend/cython/socket.c:9621)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-149-cd4fff4d4c9f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mprevious_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m         \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'> '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m         response_token_ids = sess.run(chatbot.decoder_token_ids, feed_dict={\n\u001b[1;32m     12\u001b[0m             \u001b[0minputs_ph\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    692\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m             \u001b[0mpassword\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    695\u001b[0m         )\n\u001b[1;32m    696\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/marekgalovic/.virtualenvs/data/lib/python3.6/site-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    722\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 724\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=chatbot_graph) as sess:\n",
    "    saver.restore(sess, './models/gcloud_simple_agent_15/model.ckpt-19')\n",
    "    \n",
    "    previous_output = [0]\n",
    "    message = embed('Can we make this quick?')\n",
    "    print(message)\n",
    "\n",
    "    previous_output = [0]\n",
    "    while True:\n",
    "        message = embed(input('> '))\n",
    "        response_token_ids = sess.run(chatbot.decoder_token_ids, feed_dict={\n",
    "            inputs_ph: [message],\n",
    "            inputs_length_ph: [len(message)],\n",
    "            previous_outputs_ph: [previous_output],\n",
    "            previous_outputs_length_ph: [len(previous_output)]\n",
    "        })\n",
    "        previous_output = response_token_ids[0]\n",
    "        print(decode(response_token_ids[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings: [9858, 300]\n",
      "[[0 2 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 2 2 1 0 0 0 0 0 0 0]\n",
      " [0 2 2 2 2 2 2 2 1 0 0 0]\n",
      " [0 2 2 1 0 0 0 0 0 0 0 0]\n",
      " [0 2 2 2 2 2 2 2 2 2 2 1]]\n",
      "[[2 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 2 2 1 0 0 0 0 0 0 0 0]\n",
      " [2 2 2 2 2 2 2 1 0 0 0 0]\n",
      " [2 2 1 0 0 0 0 0 0 0 0 0]\n",
      " [2 2 2 2 2 2 2 2 2 2 1 0]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    with tf.Session() as sess:\n",
    "        trainer = CrossEntropyTrainer([len(embeddings), 300])\n",
    "        \n",
    "        seq_lens = tf.constant([1, 3, 7, 2, 10], tf.int32)\n",
    "        data = tf.sequence_mask(seq_lens, 10, dtype=tf.int32) * 2\n",
    "        \n",
    "        seq_lens, data = sess.run([seq_lens, data])\n",
    "        \n",
    "        targets, loss_targets = sess.run([trainer._padded_targets, trainer.loss_targets], feed_dict={\n",
    "            trainer.targets: data,\n",
    "            trainer.targets_length: seq_lens\n",
    "        })\n",
    "        \n",
    "        print(targets)\n",
    "        print(loss_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "sess = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0],\n",
       "       [1, 1, 1, 1, 1]], dtype=int32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lens = tf.constant([0, 1, 0, 0, 1], tf.int32)\n",
    "\n",
    "data = tf.ones([5, 5], dtype=tf.int32)\n",
    "\n",
    "sess.run(tf.expand_dims(seq_lens, -1) * data)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
